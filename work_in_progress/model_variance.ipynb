{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fed6ef89-bad1-4bc3-ab21-bc40982ded56",
   "metadata": {},
   "source": [
    "# Calculating variance across models\n",
    "\n",
    "This notebook runs through the calculation of multiple types of uncertainty for an example metric, across 3 future scenarios spanning 2015-2100. Here we calculate: \n",
    "- **Internal variability:** the component of future uncertainty attributable to natural variations in the climate, or within-model variation. We calculate this using a large ensemble approach to the variance calculation. \n",
    "- **Model uncertainty:** the component of future uncertainty attributable to differences between models, which, due to their different components and assumptions, may respond differently to identical radiative forcing scenarios. \n",
    "- **Scenario uncertainty:** the component of future uncertainty attributable to differences between Shared Socioeconomic Pathways, or the uncertainty in future emissions, and thus radiative forcing and climate. \n",
    "\n",
    "We will calculate the annual average of maximum daily air temperature, area-averaged across Alameda county as an example metric. Note that other metrics over different areas may have different relative uncertainties.\n",
    "\n",
    "**Intended application**: As a user, I wish to understand projections of air temperature in my region by:\n",
    "- Calculating variance in internal, model, and scenario uncertainties through end of century\n",
    "- Visualizing the variance\n",
    "\n",
    "**Runtime**: With the default settings, this notebook takes approximately **6 minutes** to run from start to finish. Modifications to selections may increase the runtime.\n",
    "\n",
    "## Step 0: Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadf83d8-91a4-42c2-83ad-c59a2d30a431",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import climakitae as ck\n",
    "import climakitaegui as ckg\n",
    "\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7d5ed3-2652-40d4-b3a7-cd6a9f66ece3",
   "metadata": {},
   "source": [
    "## Step 1: Calculate historical baseline\n",
    "\n",
    "If we want to look at anomalies of future projections rather than absolute, first we need to calculate a historical baseline for our metric of interest. From this baseline and the absolute values provided by future projections, we can calculate anomalies. Use the select tool, or bypass and hardcode the selections for historical 1985-2015. \n",
    "\n",
    "### 1a) Select historical data\n",
    "First we need to select the relevant subset of historical data. In this example, we want to select the hybrid-statistically downscaled area average over Alameda County of monthly maximum air temperature over our historical baseline period of 1985-2015. We can either use the selection tool (commented out) to choose this subset of historical data; or we can just hard-coded these selections. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc50c6a7-d2a4-4e32-88ef-84f417d86d0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "selections = ckg.Select()\n",
    "\n",
    "selections.area_average='Yes'\n",
    "selections.area_subset='CA counties'\n",
    "selections.cached_area=['Alameda County']\n",
    "selections.data_type='Gridded'\n",
    "selections.downscaling_method='Statistical'\n",
    "selections.resolution='3 km'\n",
    "selections.scenario_historical=['Historical Climate']\n",
    "selections.time_slice=(1985, 2015)\n",
    "selections.timescale='monthly'\n",
    "selections.units='K'\n",
    "selections.variable='Maximum air temperature at 2m'\n",
    "\n",
    "# selections.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936c41f3-2a0f-4b27-acbd-dfdb106f3930",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 1b) Retrieve and load data\n",
    "Next we retrieve the historical data that we have selected, and load it into memory. The historical data may take a minute to load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde5f5fb-e69d-4da0-a53b-9d2a3f4d6d54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hist_data = selections.retrieve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a080f21c-eb7a-4499-8795-806b4797833b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_data = ck.load(hist_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad54f5e-eba9-4799-98ec-a1f06a5a413d",
   "metadata": {},
   "source": [
    "## Step 2: Calculate weighted temporal mean of historical data\n",
    "\n",
    "From monthly timeseries, calculate annual averages (weighted by the number of days in each month), and then average across years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa85b245-a1f8-4603-ac51-472445e34292",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from climakitae.explore.uncertainty import weighted_temporal_mean\n",
    "\n",
    "# calculate annual average\n",
    "ann_avg_temp_hist = weighted_temporal_mean(hist_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01ab413-0817-4025-add7-65ac1ea566a3",
   "metadata": {},
   "source": [
    "Calculate the 30-year mean:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797352f8-99a6-4bd2-b8b7-1d37da423cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_mean = ann_avg_temp_hist.mean(dim='time')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2e283a-61d7-4b97-bb54-2122d1280b76",
   "metadata": {},
   "source": [
    "## Step 3: Select and retrieve future projections, calculate anomalies \n",
    "\n",
    "Next we need to retrieve and load in our desired future projection data. As with the historical data, we'll calculate annual averages, and then use the historical baseline we calculated in the section above to convert our future projections from absolute values into anomalies relative to that 1985-2015 historical baseline. \n",
    "\n",
    "\n",
    "#### 3a) Select future projections\n",
    "\n",
    "In this example, we want hybrid-statistically downscaled maximum temperature for 2015-2100, area-averaged across Alameda county, and we want to include all available future projections and ensemble members across all scenarios. For now we want these to be area-averaged over Alameda county during retrieval, but later on (for at least some metrics), area-averaging should occur after metric calculation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc760b7-8ac6-4f36-80fc-43f3b7da86ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "selections.area_average='Yes'\n",
    "selections.area_subset='CA counties'\n",
    "selections.cached_area=['Alameda County']\n",
    "selections.data_type='Gridded'\n",
    "selections.downscaling_method='Statistical'\n",
    "selections.resolution='3 km'\n",
    "selections.scenario_historical=[]\n",
    "selections.scenario_ssp=['SSP 3-7.0', 'SSP 2-4.5', 'SSP 5-8.5']\n",
    "selections.time_slice=(2015, 2100)\n",
    "selections.timescale='monthly'\n",
    "selections.units='K'\n",
    "selections.variable='Maximum air temperature at 2m'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3accf2f6-914a-482a-9153-0c78de5d922d",
   "metadata": {},
   "source": [
    "#### 3b) Retrieve and load data\n",
    "\n",
    "Next we retrieve the future projections data that we have selected, and load it into memory. Like the historical data, this may take a few minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350fe341-112c-455b-be05-954cb083ba62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ssp_data = selections.retrieve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439241f1-b415-4037-9f09-12f342a79fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssp_data = ck.load(ssp_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0813e65-f910-4c2c-b888-131b20755c5e",
   "metadata": {},
   "source": [
    "#### 3b) Evaluate for missing values\n",
    "We will next check how many missing values are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8695be24-d939-406a-ae40-4d591d6da038",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check missing values \n",
    "print(\"There are\", np.isnan(ssp_data).sum().values, \"missing values in `ssp_data`, out of\", ssp_data.size, \"expected values.\")\n",
    "print(\"This is about\", round(np.isnan(ssp_data).sum().values / ssp_data.size * 100),\"%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf162a80-693c-4f23-b6d7-4802f0cadf80",
   "metadata": {},
   "source": [
    "#### 3c) Construct annual means \n",
    "\n",
    "From monthly data we calculate the annual average of the projected data next. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebc3346-5224-498e-afb5-49f5c0fce146",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ann_avg_temp = weighted_temporal_mean(ssp_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f1018a-57de-4f87-8a23-4689ec52c115",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 3d) Calculate anomalies\n",
    "\n",
    "Then we subtract off the ensemble member minus specific historical mean from all future projections values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984efd78-170e-463b-b4d1-1e275d3746b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First broadcast `ann_avg_temp` and `hist_mean` to match dimensions in order to calc anomalies: \n",
    "a, b = xr.broadcast(ann_avg_temp, hist_mean.sel(scenario = 'Historical Climate'))\n",
    "\n",
    "# subtract historical mean from future projection values: \n",
    "anoms = ann_avg_temp - b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e76f9a-be4c-4cac-9cfc-702b2af17709",
   "metadata": {},
   "source": [
    "## Step 4: Variance calculations\n",
    "\n",
    "Here we calculate 3 types of uncertainty so we can compare variance: \n",
    "- Internal variability (using a large ensemble approach)\n",
    "- Model uncertainty\n",
    "- Scenario uncertainty\n",
    "\n",
    "For more detailed information on internal variability and model uncertainty, check out the `internal_variability.ipynb` and `model_uncertainty.ipynb` notebooks in the Exploratory notebook folder!\n",
    "\n",
    "#### 4a) Calculate internal variability\n",
    "While there are several methods of calculating internal variability, here we choose to leverage the **Large Ensemble** (the presence of multiple ensemble members per model) to calculate internal variability. In doing so, we will first calculate the decadal running mean of anomalies, and calculate the variance as a function of time across all available ensemble members, and average variance across time. This results in a single time-invariant estimate of internal variability. \n",
    "\n",
    "We first calculate the decadal running mean of anomalies, i.e. the 10-year mean of 2015-2025, the 10-year mean of 2016-2026, and so on. Note that calculating rolling averages contracts the number of years for which we have data (e.g. if looking at 2070-2100, we need 2060-2100 rather than 2070-2100 to get a 30-year timeseries of rolling decadal averages), so we have these rolling 30-year averages for 2025-2100 rather than 2015-2100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b2c933-2eb6-47af-a26d-12e294a3b43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_mean_anom = anoms.rolling(time=10, center=False).mean() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7a17dc-4fa9-423a-bbac-684c7e41cd29",
   "metadata": {},
   "source": [
    "From the decadal averages of anomalies, we calculate the variance across all ensemble members as a function of time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b705f176-8eb0-472d-95eb-b4dcf84fb444",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_foft = dec_mean_anom.var(dim='simulation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac966e3d-88d3-41fc-ad2a-50f885a96a35",
   "metadata": {},
   "source": [
    "For our internal variability calculation we average these variances across time, and then average across the three scenarios. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3306b22-7f71-49ac-a4b6-0fe42df649e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# avg across time, then scenario\n",
    "int_var = var_foft.mean(dim='time').mean() # yields 1 value per scenario, then average across these"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17605826-94e9-434a-a835-3ecdb8cc8aca",
   "metadata": {},
   "source": [
    "#### 4b) Calculate model variability\n",
    "Here we estimate the (unweighted) model variability by calculating model averages across all of each model's ensemble members, calculating the variance across model averages as a function of time, and then averaging across scenarios (yields a time-varying model variability). \n",
    "\n",
    "$M(t) = \\frac{1}{N_s}\\sum_s var^W_m(x_{m,s,t})$ \n",
    "\n",
    "Here, this is variance in decadal average anomaly $x_{m,s,t}$ following the model predictin fits described in [Hawkins and Sutton (2009)](https://journals.ametsoc.org/view/journals/bams/90/8/2009bams2607_1.xml).\n",
    "\n",
    "First, we'll make a key (outside of the data), mapping each simulation to its parent GCM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2cd22d-63e4-457e-9300-9d69855ee252",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_GCM(sim_dim):\n",
    "    \"\"\"Extract GCM from simulation name\n",
    "    Parameters\n",
    "    ----------\n",
    "    sim_dim: dimension of xarray.DataArray\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    to_return: list\n",
    "    \"\"\"\n",
    "\n",
    "    to_return = []\n",
    "    \n",
    "    for one_val in sim_dim:\n",
    "        one_gcm = str(one_val.values).split('_')[1]\n",
    "        to_return.append(one_gcm)\n",
    "    return to_return\n",
    "\n",
    "# gcm/simulation key\n",
    "gcm_df = pd.DataFrame(get_GCM(dec_mean_anom.simulation), index = dec_mean_anom.simulation,\n",
    "                      columns = ['gcm'])\n",
    "\n",
    "# list out unique GCMs \n",
    "gcm_list = list(set(gcm_df['gcm'].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d415dd-992c-45cb-ad77-a9df67fcb596",
   "metadata": {},
   "source": [
    "Iterate through models to calculate ensemble averages for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79406147-8491-425b-bb47-ccea91cb8b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by deleting `mod_df` if it's already in memory: \n",
    "\n",
    "try:\n",
    "    mod_df\n",
    "except NameError:\n",
    "    print(\"not yet defined\")\n",
    "else:\n",
    "    del mod_df\n",
    "\n",
    "# Iterate through models\n",
    "for tgt_mod in gcm_list:\n",
    "    # select subset of simulations in dec_mean_anom from that model \n",
    "    # and average across simulations to get a model average\n",
    "    # (yields 1 model average of decadally-smoothed anomaly per scenario):\n",
    "    m = dec_mean_anom.sel(simulation = gcm_df.loc[gcm_df['gcm'] == tgt_mod].index).mean('simulation')\n",
    "    # add gcm coordinate to concat along\n",
    "    m = m.assign_coords({\"gcm\": tgt_mod})\n",
    "    \n",
    "    # concat if mod_df doesn't exist yet\n",
    "    try:\n",
    "        mod_df\n",
    "    except NameError:\n",
    "        mod_df = m\n",
    "    else:\n",
    "        mod_df = xr.concat([mod_df, m], dim=\"gcm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a89f15-9cd2-405b-b75c-d9a065a55d74",
   "metadata": {},
   "source": [
    "Calculate variance across models for each scenario, and average across scenarios.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0bd5a6-8102-4f74-af61-60678de6cac1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mod_var = mod_df.var(dim = 'gcm').mean(dim = 'scenario')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7891fc4f-f9a0-4e4d-b76a-08b11df7fafd",
   "metadata": {},
   "source": [
    "#### 4c) Calculate scenario variability\n",
    "\n",
    "Calculate scenario variability, similar to how it was calculated in [Hawkins and Sutton (2009)](https://journals.ametsoc.org/view/journals/bams/90/8/2009bams2607_1.xml): estimating from unweighted variance across weighted multimodel means (eq. 6), take the (unweighted) multimodel mean for three scenarios, and calculate the variance across these. \n",
    "\n",
    "$S(t) = var_s(\\sum_m x_{m,s,t})$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b817b3-108c-4adb-9d23-ba249af81a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scen_var = dec_mean_anom.mean(dim = 'simulation').var(dim = 'scenario')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939ebbcc-67c7-492e-99f4-58c69cec72cd",
   "metadata": {},
   "source": [
    "#### 4d) Calculate total variability from internal variability, model uncertainty, and scenario uncertainty\n",
    "\n",
    "First we calculate **total variability**, and model and internal variability as percentages of their combined uncertainty. We'll turn the time-invariant internal variability estimate (`int_var`) into a timeseries (note: forces computation if data not already loaded). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81034120-9653-4f4f-bd66-9ae7751f5643",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_var_rep = np.repeat(int_var.values, mod_var.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e7770b-8cc4-42fb-80dc-4ac2d7810a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total variance\n",
    "tot_var = mod_var.values + int_var_rep + scen_var.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e829580-667b-4631-ad71-a1ad33773565",
   "metadata": {},
   "source": [
    "We next will calculate total uncertainty components from internal variability, model uncertainty, and scenario uncertainty, as well as the **fractional uncertainty components** for each. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f42cb8-e5de-40dc-b23f-ddd8d675fadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Component uncertainty\n",
    "modelComponent = mod_var.values\n",
    "scenarioComponent = scen_var.values\n",
    "internalComponent =  tot_var - mod_var.values - scen_var.values  # replace values of int_var_rep with NaN if NaN in tot_var\n",
    "\n",
    "# Fractional uncertainty\n",
    "fracModel  = mod_var.values / tot_var\n",
    "fracInternal  = internalComponent / tot_var\n",
    "fracScenario = scen_var.values / tot_var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716fcb3e-e558-4616-ad7a-f637bb80916a",
   "metadata": {},
   "source": [
    "#### Step 5: Visualize variance\n",
    "Now, we'll visualize the proportional difference between model variability and internal variability over time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8624b0a-fec8-4e62-bd8f-6273212a2648",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de38d2a-bfbd-4b77-ae46-a36433f1e9f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, (ax0, ax1) = plt.subplots(2, 1, sharex=True, figsize=(6, 6))\n",
    "\n",
    "x = mod_var.time # x-axis (last year of decadal average anomaly) \n",
    "\n",
    "# total variance\n",
    "ax0.plot(x, modelComponent, color='b',linewidth=2)\n",
    "ax0.plot(x, scenarioComponent, color='g',linewidth=2)\n",
    "ax0.plot(x, internalComponent, color='orange',linewidth=2)\n",
    "ax0.set_ylabel('Total variance [0-1]');\n",
    "\n",
    "# fractional variance\n",
    "ax1.fill_between(x, fracModel+fracScenario, 1, color='orange')\n",
    "ax1.fill_between(x, fracModel, fracModel+fracScenario, color = 'green')\n",
    "ax1.fill_between(x, 0, fracModel, color='blue')\n",
    "ax1.set_ylabel('Fractional variance [0-1]');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6258099b-7cfa-453b-9696-aaa22ead0e46",
   "metadata": {},
   "source": [
    "## Discussion: Implications for selecting projections\n",
    "Let's think about this figure in the context of making decisions about what future projections to use.\n",
    "\n",
    "Towards the end of the century, say 2085-2100, model uncertainty (shown in blue) and scenario uncertainty (shown in green) are both much larger than internal variability (shown in orange). So when choosing projections for over this time frame, which models and scenarios you choose will drive differences in your analysis more than which ensemble member from a given model you choose. In other words, if you're trying to select projections and want to see the greatest range of possible outcomes without looking at *all* projections, you should focus your efforts on looking at outputs from different models and scenarios, rather than looking at different ensemble members, because you can expect that the greatest differences over this time period between all of the different future projections available to you will be between models and scenarios, rather than between ensemble members. At 2100, scenario uncertainty contributes the most to total uncertainty, followed by model uncertainty, and with internal variability contributing the least. So you can expect that the largest differences between projections, if you compare them, will be driven by which scenario you're looking at. \n",
    "\n",
    "In the first 20 years however, from 2015 to around 2035, we see that model and scenario uncertainty are quite low, and internal variability is actually the largest percentage of total uncertainty. So when choosing projections for over this time frame, thinking carefully about which ensemble member you're choosing is important, as differences between ensemble members will drive the range of possible outcomes. \n",
    "\n",
    "In the top figure, we observe that total uncertainty increases as time progresses towards 2100. While the proportion of total uncertainty that internal variability contributes to declines towards 2100, its contribution, by this estimate, remains constant. \n",
    "\n",
    "Remember also that we're looking at one specific example metric (annual average maximum surface temperatures) and averaging across one specific geospatial region (Alameda county). The conclusions above about which sources of uncertainty are most important and when will likely change if we choose to look at a different metric or region. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9832be6-de17-4b0e-ba1e-c8a8b7d818f8",
   "metadata": {},
   "source": [
    "## Potential future additions\n",
    "- **Area averaging**: Ideally the area averaging function should be separated from the data selection process, and area-averaging should be implemented after calculating metrics and constructing anomalies.  \n",
    "- **Missing data**: There is quite a bit of missing data -- should look into whether this is due to differences in which models and simulations contribute data to which SSPs, if data don't span certain time periods (would be surprising), or data are lost at some stage in calculation, such as area averaging. \n",
    "- **Data loading**: Xarrays should be loaded into memory right before plot generation, rewriting of code needed to ensure calculations are written such that it is in fact faster to load at that stage rather than at the beginning of the notebook. \n",
    "- **Sensitivity testing**: \n",
    "    - Large ensemble internal variability sensitivity test: restrict LE internal variance calculation to models with multiple ensemble members (e.g. remove TaiESM because only has 1 ensemble member). See what, if any, difference this makes.\n",
    "    - Model variance sensitivity test:  try sampling a single ensemble member from each model to calculate model variance, rather than first averaging across ensemble members. See what, if any, difference it makes. \n",
    "- **Decadal smoothing and 30-yr averages** may not be appropriate for all purposes. A user may wish to fine-tune the time frame over which smoothing occurs for the particular climate variable(s) and metric of interest they wish to work with. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
