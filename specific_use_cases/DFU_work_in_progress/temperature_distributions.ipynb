{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e608c90-0373-4b64-a998-fedb0e0a0260",
   "metadata": {},
   "source": [
    "## Temperature Density Profiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e15e7c-b317-4b93-8868-8c0b4fc8271f",
   "metadata": {},
   "source": [
    "This notebook is an early attempt to replicate the daily minimum and maximum weather distribution profiles provided to us by the DFU.\n",
    "\n",
    "**Try plotting the daily min and max pdf’s for the WRF-BC’d-to-a-station data, on the same plot as the station data we use for bias-correction, to compare with a similar plot that DFU shared an excel workbook for**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc90d21-de7c-4e46-8cf2-655b88ab9492",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import scipy.stats as stats\n",
    "import calendar\n",
    "import climakitae as ck\n",
    "\n",
    "pd.options.plotting.backend = 'holoviews'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b39158e-412e-4923-b537-ccf9547b2025",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = ck.Application()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ca3cbc-6315-4fcc-9199-33d7e30c08ea",
   "metadata": {},
   "source": [
    "### Step 1: Retrieve bias-corrected data for a station \n",
    "\n",
    "First we'll read in some **bias-corrected station data**. For ease of reproducibility, we have pre-loaded data selections for air temperature for the Burbank-Glendale-Pasadena Airport for 1985-2010. However, if you would like to make modifications, or see how the data can be selected, uncomment the line app.select in the cell below to pull up a useful panel that illustrates all of the data options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84435f8-6117-42e5-a601-966a68647b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "## preset location and data selections for ease here\n",
    "\n",
    "app.location.data_type = \"Station\"\n",
    "app.location.station=['Burbank-Glendale-Pasadena Airport']\n",
    "app.selections.variable = \"Air Temperature at 2m\"\n",
    "app.selections.unit = \"degF\" \n",
    "app.selections.resolution = \"3 km\"\n",
    "app.selections.time_slice = (1985, 2010)\n",
    "\n",
    "# app.select()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60226cd-b5b7-4b00-a45b-2415beb096c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = app.retrieve().squeeze() # retrieves the data, and drops any singleton dimensions (scenario, in this case)\n",
    "data = app.load(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f0f27c-25a6-406d-8b50-fb13f738cd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine the dataset for information\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d83fb00-1639-4c4b-82fc-565a4ba10158",
   "metadata": {},
   "source": [
    "### Step 2: Calculate daily min and max temperatures distributions\n",
    "\n",
    "#### Step 2a: Calculate daily min and max temperaturees\n",
    "As the bias corrected data is at an hourly scale, we will need to calculate the daily minimum and maximum values. We do this below using the built-in xarray function `resample` which identifies the maximum/minimum value in each 1 day period, and returns that value for every day as a collapsed daily time-series. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96384bc-afa6-4c7d-ac5d-7499e15ff2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "t2_dailymax = data.resample(time=\"1D\").max() # daily maximum from hourly data\n",
    "t2_dailymin = data.resample(time=\"1D\").min() # daily minimum from hourly data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca4ded5-282b-4ea7-87d0-582f1a153571",
   "metadata": {},
   "source": [
    "#### Step 2b: Calculate the probability distribution function for daily maximum and minimum temperature\n",
    "\n",
    "We'll do this with the scipy library function `stats.norm` with the `pdf` option, this ensures that we are calculating the probability density function. We've created a wrapper function `data_pdf` that does this for all the simulations available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4d12a0-ce06-4a13-bf46-5aa5609253b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_pdf(data, bins, ext):\n",
    "    \"\"\"processes data to produce the pdf arrays\"\"\"\n",
    "    \n",
    "    # determines how many simulations we are working with\n",
    "    num_sim = len(data.simulation.values)\n",
    "    \n",
    "    # set-up for first simulation\n",
    "    data_sim = data.isel(simulation=0) # first simulation\n",
    "    data_sim_arr = data_sim.to_array() # converts to a data-array, as stats can only be calculated on a single array at a time\n",
    "    data_sim_mean, data_sim_std = data_sim_arr.mean(), data_sim_arr.std() # calculates the mean, standard deviation\n",
    "    data_sim_snd = stats.norm(data_sim_mean.values, data_sim_std.values) # calculates normal distribution using mean and std. deviation\n",
    "    data_pdf_arr = data_sim_snd.pdf(bins) # calculates the pdf\n",
    "    \n",
    "    # sets-up dataframe of pdf values, for easy plotting and export\n",
    "    df = pd.DataFrame(data = data_pdf_arr, columns = [str(data_sim.simulation.values) + \"_\" + str(ext)])\n",
    "    \n",
    "    # same process for every other simulation\n",
    "    for sim in range(1, num_sim):\n",
    "        data_sim = data.isel(simulation=sim)\n",
    "        data_sim_arr = data_sim.to_array()\n",
    "        data_sim_i_mean, data_sim_i_std = data_sim_arr.mean(), data_sim_arr.std()\n",
    "        data_sim_i_snd = stats.norm(data_sim_i_mean.values, data_sim_i_std.values) \n",
    "        data_pdf_arr = data_sim_i_snd.pdf(bins)\n",
    "        df[str(data_sim.simulation.values) + '_' + str(ext)] = data_pdf_arr # adds simulation name and max/min extension\n",
    "                \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bfe215-4527-46de-b51c-d90d5b7ae230",
   "metadata": {},
   "source": [
    "Next we set-up the number of bins to calculate the PDF over. We are interested in the range between 20°F and 120°F, at a 1°F interval. In the bins set-up, the high end of the range has a +1 included to ensure that 120 is the maximum here (and not 119). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d18fc79-5c3b-4c45-bca8-7c910e72d846",
   "metadata": {},
   "outputs": [],
   "source": [
    "lowest_temp = 20\n",
    "highest_temp = 120\n",
    "bins = np.arange(lowest_temp, highest_temp+1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90eb921c-4df0-4ad9-b588-33e4fe8709c9",
   "metadata": {},
   "source": [
    "Now, we calculate the PDF for a specific month. First, we need to grab just the data for that month, for which we've set-up the `grab_months` function, for which you can pass the month to, but be sure to pass a number to this function (Jan=1, Dec=12). We use February (month=2) as an example here, but you can modify the month to be any of your choosing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c9c2ec-dae4-4451-933f-f29c52a1c6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_months(data, month):\n",
    "    \"\"\"Grabs the specific month of interest and returns DataSet of all years for that month.\n",
    "    Month must be passed as a number\"\"\"\n",
    "    data_months = data.groupby('time.month').groups\n",
    "    month_idxs = data_months[month]\n",
    "    return data.isel(time=month_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2028c8-a748-407f-9d17-b0723b69bc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "month = 2 # default of February\n",
    "t2_dailymax_monthly = grab_months(t2_dailymax, month=month)\n",
    "t2_dailymin_monthly = grab_months(t2_dailymin, month=month)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326b6e00-8824-475e-96ce-5978f1e41f05",
   "metadata": {},
   "source": [
    "Calculate the daily PDFs for that month below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc7b707-c922-4895-a9aa-e1b2a8946b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxtemp_pdf = data_pdf(t2_dailymax_monthly, bins=bins, ext='max')\n",
    "mintemp_pdf = data_pdf(t2_dailymin_monthly, bins=bins, ext='min')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443766e1-73a7-4456-9ca0-c1ba68a31d41",
   "metadata": {},
   "source": [
    "Combine the dataframes together so that they are all in a single location, and can be easily visualized and exported to a .csv file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea61ef5d-f0b2-4266-8adc-e69f88abf784",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins_df = pd.DataFrame(data=bins, columns=['Temperature'])\n",
    "df_to_plot = pd.concat([bins_df, maxtemp_pdf, mintemp_pdf], axis=1, join=\"inner\")\n",
    "df_to_plot = df_to_plot.set_index('Temperature')\n",
    "df_to_plot.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba75f25-5118-40d8-9b12-f4ebeed7018c",
   "metadata": {},
   "source": [
    "#### Step 2c: Visualize the results\n",
    "Plot distributions of daily maximum and minimum temperature for a selected month over a set of years. Remember, here we are using data from 1985-2010 as our baseline, and are displaying the results for February, but you can choose any month above! Play around with different months to see how the PDF distributions vary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd450a03-11a8-40fa-936c-db03523fecdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_plot.plot(xlabel=\"Temperature (degF)\",\n",
    "                grid=True, # adds gridlines for easier interpretation\n",
    "                title=\"PDFs for \" + str(app.location.station[0]) + \"\\n\" + calendar.month_name[month],\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c1776f-e8cf-4db4-8cc1-7e4bd32985f4",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Step 3: Export PDF values to a .csv file\n",
    "Lastly, we'll export the dataframe of PDF values to a csv file. Included is the temperature bins, and the maximum and minimum PDF distributions per simulation. \n",
    "- **QUESTION**: original spreadsheet has \"grand total\" = sum of each column? do we include this? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2570c02-e6dc-4f8f-b5be-0dbc8bdcaefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"temperature_pdfs_{0}.csv\".format(app.location.station[0].replace(\" \", \"_\")).lower()\n",
    "df_to_plot.to_csv(filename, index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
