{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc908fc1-557b-49ad-b515-70e91ed630c8",
   "metadata": {},
   "source": [
    "# Using the Analytics Engine (AE) to reproduce annual consumption model\n",
    "This notebook is an early draft attempt to reproduce the workflow CEC's Demand Forecast Unit takes to generate weather and climate information for the annual consumption model. Here the existing workflow is replicated, but connecting with new data from California's Fifth Climate Change Assessment.\n",
    "\n",
    "To execute a given 'cell' of this notebook, place the cursor in the cell and press the 'play' icon, or simply press shift+enter together. Some cells will take longer to run, and you will see a [$\\ast$] to the left of the cell while AE is still working."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ab2392-ae18-4ce8-baff-3aa87c58dd96",
   "metadata": {},
   "source": [
    "## Step 0: Setup\n",
    "First, we'll import any general python libraries required to run the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8703a0d0-82dc-41d1-8482-e0c7e824f5f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import panel as pn\n",
    "pn.extension()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8accb6-def7-40be-bdf7-722026fa8db0",
   "metadata": {},
   "source": [
    "Next, we'll import the python library [climakitae](https://github.com/cal-adapt/climakitae), our AE toolkit for climate data analysis, along with this specific functions from that library that we'll use in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b1980c-c7f7-48f0-91cd-388e6aa46793",
   "metadata": {},
   "outputs": [],
   "source": [
    "from climakitae.utils import get_closest_gridcell\n",
    "from climakitae.selectors import Boundaries\n",
    "from climakitae.derive_variables import compute_hdd_cdd\n",
    "from climakitae.cluster import Cluster\n",
    "import climakitae as ck"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474e2e5c-77a7-4de0-a6c2-75f9e0259575",
   "metadata": {},
   "source": [
    "Because we have two separate notebooks covering this same topic, we've put shared functions in a utils module, named `utils_notebook_1.py`. We'll import in the functions from that file next. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16db3e99-ffdd-49db-9ee9-7291a1db812a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_notebook_1 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a04e169-a8d0-4a63-9938-451989a4016f",
   "metadata": {},
   "source": [
    "Additionally, get set up to make the computing go faster by executing the following cell. It will likely take several minutes to spin up! Learn more about dask and see some common [troubleshooting tips on our FAQ page](https://analytics.cal-adapt.org/docs/faq/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c5da5d-b377-4b82-bcaf-3a6b569b5041",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = Cluster()\n",
    "cluster.adapt(minimum=0, maximum=8)\n",
    "client = cluster.get_client()\n",
    "cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d7b471-cf37-4f74-8d50-b29fd6f5bbc1",
   "metadata": {},
   "source": [
    "To use climakitae, load a new application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afd4668-4e0e-4386-9b91-39feb323d866",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = ck.Application()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9016fdc-a478-47bf-9fdd-1edbfc8761f2",
   "metadata": {},
   "source": [
    "## Step 1: Retrieve the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7c8f9c-685e-4405-bf6b-2ddab808f748",
   "metadata": {},
   "source": [
    "### 1a) Read in the data \n",
    "To allow for better reproducibility of this notebook, we have put all data and location selections into a csv file, which should be located in the same folder as this notebook. We'll read this into our notebook by using the climakitae helper function `app.retrieve()`, providing the local filepath to the csv file as an argument to the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7767adb-bc32-4c12-8dc3-c58a34f8aa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data = app.retrieve(\"data/config_hourly_data.csv\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfbdb20-e168-4239-977a-59c11750a415",
   "metadata": {},
   "source": [
    "### 1b) Preview the data\n",
    "The function above returns an xarray Dataset object, with the three variables we want to load in-- air temperature, relative humidity, and dew point temperature-- as separate elements of a single python object. Since all the variables have the same spatial and temporal dimensions, they can be stored in the same object. To access an individual variable (for example, Air Temperature at 2m), you can simple type `data[\"Air Temperature at 2m\"]` to get just data for that variable.<br><br>\n",
    "Understanding the dimensionality of this object is not a prerequisite to understanding the concepts in this notebook. However, if you'd like to learn more about this data type, [xarray's documentation](https://docs.xarray.dev/en/stable/generated/xarray.Dataset.html) gives an excellent description.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c171d4d-ddfc-493d-85ce-8f0ce733009d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24feece0-2be8-4528-9ffa-d6f2ba8c7ee7",
   "metadata": {},
   "source": [
    "### 1c) Preview the data using app.select()\n",
    "This panel will display the data and location settings of the final row in the csv file fed to `app.retrieve()`. We'll display it in the notebook to give you a better sense of the data, as well as the other data options in the Analytics Engine catalog. <br><br>Although we won't do this in this particular notebook, you can also use this panel to directly modify the data and location selections, and then read in the data afterward by calling `app.retrieve()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d38787-c2a0-4a43-945e-ad4d36b718b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "app.select()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79d4034-9ea2-4af7-8410-6612c46d82fe",
   "metadata": {},
   "source": [
    "## Step 2: Get data from the closest grid cell to the weather station. \n",
    "As an example - to replicate the historical observations at Sacramento Executive Airport, grab the grid cell from the model nearest to the airport."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1900ddce-eed5-4ed4-83a0-63964df4b0ac",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2a) Read in a csv file of the station coordinates \n",
    "Make sure the filepath to the csv file matches the correct location on your computer. This file will be read into the notebook as a pandas DataFrame object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786c2ff2-b12e-4b31-af79-4648cd348e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_df = pd.read_csv(\"data/CEC_Forecast_Weather Stations_California.csv\", index_col=\"STATION\")\n",
    "stations_df.head(5) # Display the first 5 rows "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4cd9b4-6df8-499a-b7d9-c45c782b3b87",
   "metadata": {},
   "source": [
    "### 2b) Grab the closest grid cell to the weather station\n",
    "To demonstrate this process, we'll use the Sacramento Executive Airport weather station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9486019c-c1f7-4ac9-b63e-e2234de5dc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_name = \"SACRAMENTO EXECUTIVE AIRPORT\"\n",
    "one_station = stations_df.loc[station_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16bc16c-d454-47f8-81d7-1f360e6c46df",
   "metadata": {},
   "source": [
    "Next, we need to convert the lat/lon coordinate pair to the model's projection coordinates. We can easily do this using the built in helper function in climakitae: `get_closest_gridcell`. For more information on this function, you can call `help(get_closest_gridcell)` or look in the climakitae.utils module for the actual code that performs the computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2351746-9ef3-4525-aaa9-317d0c703c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_closest_gridcell = get_closest_gridcell(\n",
    "    data=data,\n",
    "    lat=one_station.LAT_Y,\n",
    "    lon=one_station.LON_X, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ccd297-ac6a-40a5-a544-83c0a2f0025a",
   "metadata": {},
   "source": [
    "### 2c) Load the data into memory \n",
    "This may take some time, because the data has to be loaded into memory and then subsetted to get the closest grid cell. All computations we've done before this step are actually computed in this step; before, we just see a preview of the data. **Because of this, we recommend running this notebook in the Analytics Engine's Jupyter Hub, which provides additional computational resources that greatly speed up this step.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb37f64-2547-4ccf-b2e6-190020c09382",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data_closest_gridcell = app.load(data_closest_gridcell)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23fcd87-c6ba-4e90-abdf-2bc3343c1266",
   "metadata": {},
   "source": [
    "### 2d) Output final data product as a csv file\n",
    "We'll drop all unneeded coordinates and convert our xarray Dataset to a pandas Dataframe, allowing us to easily output the final data product to a csv file. In the output table, the first column is the time in units of UTC, and the second column are the various global climate models (which can be filtered in excel or in python code in the notebook). The other columns are the variables selected at the beginning of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007ad4ef-a2c9-446c-8399-6163bc09f1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_closest_gridcell_df = data_closest_gridcell.isel(scenario=0).drop(\n",
    "    [\"x\",\"y\",\"landmask\",\"lakemask\",\"lat\",\"lon\",\"Lambert_Conformal\",\"scenario\"]\n",
    ").to_dataframe()\n",
    "data_closest_gridcell_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ada176-ef6d-40cd-a334-48b089f98e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"hourly_data_closest_gridcell_{0}.csv\".format(station_name.replace(\" \", \"_\")).lower()\n",
    "data_closest_gridcell_df.to_csv(filename, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769cdbba-bc22-46ec-b610-e07aed14e3ec",
   "metadata": {},
   "source": [
    "## Step 3: Compute the median value of the grid cells in station's corresponding forecast zone\n",
    "As an alternative to a single point, we can instead consider weather conditions across an entire forecast zone. In this example we calculate the median of all conditions across the Sacramento Municipal Utility District."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece01bbc-f33f-4474-b502-06c016837f50",
   "metadata": {},
   "source": [
    "### 3a) Read in the shapefiles of the demand forecast zones \n",
    "We'll use this to find the demand forecast zone that contains the weather station, then find the overlapping grid cells over which to compute the median value. The geometries of each demand forecast zone is available in our data catalog. You can grab the data as a pandas DataFrame object using the code provided below, or subset by forecast zones easily in `app.select` in the location subsetting tab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae93ca47-4c98-4867-a39c-22e9f6e264d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dfzs_df = Boundaries()._ca_forecast_zones # Load geometries from catalog\n",
    "dfzs_df.head() # Display the first few rows  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8907cd0d-db63-4ae3-98a2-abd794f32c28",
   "metadata": {},
   "source": [
    "### 3b) Crop the data to the corresponding forecast zone\n",
    "We'll use [geopanda's `.contains` function](https://geopandas.org/en/stable/docs/reference/api/geopandas.GeoSeries.contains.html) to find the demand forecast zone where the weather station is located, and print the result. Then, we'll use [rioxarray](https://corteva.github.io/rioxarray/stable/rioxarray.html#rioxarray.raster_array.RasterArray.clip) to clip the data to the geometry that defines the forecast zone. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bc2023-4e3b-4ec8-9533-a1e3cc349cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dfz = clip_data_to_dfz(\n",
    "    gridded_data=data, \n",
    "    dfzs_df=dfzs_df, \n",
    "    station_lat=one_station.LAT_Y,\n",
    "    station_lon=one_station.LON_X\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bffd2d1-104a-4dbd-a308-1ed4efdee246",
   "metadata": {},
   "source": [
    "### 3c) Visualize both the Demand Forecast Zone and the weather station on the same map "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47d48af-1060-40c1-a273-03b56b0de5d1",
   "metadata": {},
   "source": [
    "For simplicity's sake, we'll show just one variable and only two weeks of data. In the outputted map, you can see that our data contains multiple simulation options as well, which you can toggle between in the map's dropdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc711be3-937b-4832-a4d5-b56696c55eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to add weather station as star to map \n",
    "point_df = pd.DataFrame({\n",
    "    \"longitude (degrees_east)\":[one_station.LON_X],\n",
    "    \"latitude (degrees_north)\":[one_station.LAT_Y],\n",
    "    \"weather station\": station_name\n",
    "})\n",
    "\n",
    "# Grab subset of data and load into memory \n",
    "to_plot = data_dfz[\"Air Temperature at 2m\"].isel(time = np.arange(0,13))\n",
    "to_plot = app.load(to_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f142cf6-c160-495b-b701-9c3e17b45302",
   "metadata": {},
   "outputs": [],
   "source": [
    "app.view(to_plot) * point_df.hvplot.points(\n",
    "    hover_cols = [\"weather station\"], \n",
    "    marker = \"star\", size = 300, color = \"black\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d59289-4f6b-4683-8c56-9a4b25c3e672",
   "metadata": {},
   "source": [
    "### 3d) Agreggate values across grid cells in the forecast zone \n",
    "**Chose your aggregation: median, mean, min, or max.** All can be easily computed with just one line of code, thanks to xarray. You could also write your own code to compute a weighted mean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e862e2d1-42a8-445f-b549-1234b1d3dd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dfz_aggregated = data_dfz.median(dim=[\"x\",\"y\"])\n",
    "#data_dfz_aggregated = data_dfz.mean(dim=[\"x\",\"y\"])\n",
    "#data_dfz_aggregated = data_dfz.min(dim=[\"x\",\"y\"])\n",
    "#data_dfz_aggregated = data_dfz.max(dim=[\"x\",\"y\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b626b3b2-7c30-4b50-9b99-255eef08764e",
   "metadata": {},
   "source": [
    "Finally, let's load this final data product into memory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9550fc9-94f7-41ed-b46a-cdfb32f391b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data_dfz_aggregated = app.load(data_dfz_aggregated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba95c67-2065-4b23-ba77-b7fea0833e7b",
   "metadata": {},
   "source": [
    "### 3e) Output final data product as a csv file\n",
    "We'll drop all unneeded coordinates and convert our xarray Dataset to a pandas Dataframe, allowing us to easily output the final data product to a csv file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054fccf6-d33f-422c-a133-c300716d55ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfz_aggregated_df = data_dfz_aggregated.isel(scenario=0).drop(\n",
    "    [\"scenario\",\"Lambert_Conformal\"]).to_dataframe()\n",
    "dfz_aggregated_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aeecce2-6a4e-49df-9ce5-a8cae910a1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"dfz_aggregated_{0}.csv\".format(station_name.replace(\" \", \"_\").lower())\n",
    "dfz_aggregated_df.to_csv(filename, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36d58c7-5f96-4749-add4-f09d0558d107",
   "metadata": {},
   "source": [
    "## Step 4: Compute heating degree days and cooling degree days\n",
    "Here, a heating degree day (HDD) is calculated by computing how many degrees Farenheit **colder** the daily temperature is from a standard temperature of 65 degrees Farenheit. A cooling degree day (CDD) is calulcated by computing how many degrees **warmer** the daily temperature is from the same standard temperature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f1f8c9-8fc2-41aa-8d9d-0ec6614920d2",
   "metadata": {},
   "source": [
    "### 4a) Decide which input data you want to use \n",
    "You can use the closest grid cell to the weather station, which we computed in step 3. Or, you can use the data agreggated over the demand forecast zone, which we computed in step 4. Just comment out whichever you don't want to use. We've chosen to show the analysis with the agreggated DFZ data, but if you want to use the closest grid cell data, just comment out the DFZ cells and uncomment out the closest grid cells.<br><br>Depending on the input data, we will also set a new variable defining the number of grid cells. This will of course be just 1 for the closest grid cell method; for the agreggated DFZ data, however, this value will change depending on the size of the DFZ. This information is used to compute the annual agreggate HDD and CDD in step 5c."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b4413e-e085-409c-a603-7232a3bd5781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLOSEST GRID CELL \n",
    "# data_to_use = data_closest_gridcell\n",
    "# num_grid_cells = 1\n",
    "\n",
    "# AGGREGATED CELLS IN DFZ \n",
    "data_to_use = data_dfz_aggregated \n",
    "num_grid_cells = data_dfz.x.size * data_dfz.y.size # Number of grid cells within the demand forecast region"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410b69ba-7697-483f-828d-634bb6b5200f",
   "metadata": {},
   "source": [
    "### 4b) Compute HDD and CDD \n",
    "We'll use the climakitae helper function `compute_hdd_cdd` to perform the computation, which uses a default standard temperature of 65 degrees F. You can change this default using the function argument `standard_temp`. The function performs the following calculations:<br><br>\n",
    "**HDD = 65 - temperature<br>\n",
    "CDD = (-1)\\*(65 - temperature)**<br><br>\n",
    "For HDD, we can just subtract the 2m temperature from 65 degrees Farenheight, then set any negative to 0. For CDD, we will do the same, but will then multiply by -1 to turn negative values to positive, then set negative values to 0. We need to multiply by -1 for CDD to avoid having all negative values; for example, on a day of 80F, CDD = 65 - 80 = -15, but the CDD value is +15. Multiplying -15 by -1 will give us the true value of 15. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed2a93f-38d2-48eb-a9ac-1517a4fb19ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(compute_hdd_cdd) # See information about the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef4cb23-0d5a-437c-8037-79e07d5d802e",
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = data_to_use[\"Air Temperature at 2m\"]\n",
    "hdd, cdd = compute_hdd_cdd(t2, standard_temp=65)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269feeb8-5af2-43cf-9da3-e2233f967d54",
   "metadata": {},
   "source": [
    "### 4c) Aggregate annually to find HDD and CDD per year\n",
    "To do this, we will first group the data by year and compute a sum across space and time. Then, we will divide the annual aggregated data by the number of grid cells over which the sum was computed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d1af18-bb1f-43d7-9143-97c13d0eb383",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdd_annual = compute_annual_aggreggate(\n",
    "    data=hdd, \n",
    "    name=\"Annual Heating Degree Days (HDD)\", \n",
    "    num_grid_cells=num_grid_cells\n",
    ")\n",
    "cdd_annual = compute_annual_aggreggate(\n",
    "    data=cdd, \n",
    "    name=\"Annual Cooling Degree Days (CDD)\", \n",
    "    num_grid_cells=num_grid_cells\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71416d6-d849-4f61-8908-fda24f8a4da5",
   "metadata": {},
   "source": [
    "### 4d) Compute the multimodel mean, min, and max. \n",
    "We'll add these statistics to our main datasets, `hdd_annual` and `cdd_annual`, so they can be easily accessed for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e136ea87-b766-4b47-9783-db9668d618e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdd_annual, cdd_annual = compute_multimodel_stats(hdd_annual, cdd_annual)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ded86c-3e65-4bf2-a724-f807e8afbf86",
   "metadata": {},
   "source": [
    "### 4e) Compute a trendline using the mean of all simulations\n",
    "We'll find the coefficients for a first degree (linear) polynomial using [numpy's `polyfit` function](https://numpy.org/doc/stable/reference/generated/numpy.polyfit.html). The returned coefficients (**m** and **b** in the code below) will allow us to compute the trendline using the linear polynomial y = mx + b, where **y** is the trendline and **x** is the years. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e81e5d-1277-4218-8e95-7e138c1793fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdd_trendline = trendline(hdd_annual) \n",
    "cdd_trendline = trendline(cdd_annual) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267d8368-c7ee-4a12-bef8-54c84138d25e",
   "metadata": {},
   "source": [
    "### 4f) Visualize the results\n",
    "Using the python package *hvplot*, we can easily make a line plot of the annual aggregated data. To do this, we'll plot the annual HDD, then add the trendline on top. The code to generate the plot is contained in a function `hdd_cdd_lineplot`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e094aa-b9b7-4cd1-9f1a-8f2ac7adea4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdd_cdd_lineplot(\n",
    "    annual_data = hdd_annual, \n",
    "    trendline = hdd_trendline, \n",
    "    title = \"Annual Aggregate Heating Degree Days\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dab899-3053-4e62-8d54-59ec867ab30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdd_cdd_lineplot(\n",
    "    annual_data = cdd_annual, \n",
    "    trendline = cdd_trendline, \n",
    "    title = \"Annual Aggregate Cooling Degree Days\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fbb5b8-8d4c-4a7c-9eb9-a0d51a4e1daa",
   "metadata": {},
   "source": [
    "### 4g) Output data as csv files\n",
    "We'll drop all unneeded coordinates and convert our xarray Dataset to a pandas Dataframe, allowing us to easily output the final data product to a csv file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf13ea2a-fb98-4438-ba98-fb7b1039d579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge and simplify data \n",
    "hdd_cdd_combined = xr.merge([hdd_annual, cdd_annual]).drop([\"Lambert_Conformal\",\"scenario\"])\n",
    "hdd_cdd_combined = app.load(hdd_cdd_combined) \n",
    "\n",
    "# Convert to pandas dataframe \n",
    "hdd_cdd_df = hdd_cdd_combined.to_dataframe()\n",
    "hdd_cdd_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e3937b-1e36-4164-b96a-55b097d56c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"annual_hdd_cdd_{0}.csv\".format(station_name.replace(\" \", \"_\").lower())\n",
    "hdd_cdd_df.to_csv(filename, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce481fe-175d-4261-84ec-584c08045747",
   "metadata": {},
   "source": [
    "## Step 5: Close the compute cluster\n",
    "Lastly, when you are done, close your cluster resources to free them up for the next time you work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1133cf-8686-4e6e-b7a3-6103ca79a8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
