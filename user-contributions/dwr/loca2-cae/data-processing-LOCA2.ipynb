{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cce06bd7-61f8-4c22-894f-ad9941064f9f",
   "metadata": {},
   "source": [
    "## Cal-Adapt Analytics Engine Data processing script for DWR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ee17fd-e9a4-4b1d-9165-c4cd089fbfde",
   "metadata": {},
   "source": [
    "This notebook downloads climate data for pr, tasmax, and tasmin, processes the data to monthly time step, weights each sub-basin of interest by Area and Flow and determines the average monthly value for the entire watershed. The average weighted results for Area and Flow are then processed into a monthly rolling average series for pr, tasmax, tasmin, and tasave.\n",
    "\n",
    "The output format of processed climate data are listed below:\n",
    "* Each subasin is stored stored to exported to CSV without the weights with file name format {source_id}_{experiment_id}_{member_id}_{basin_id}_19.csv.  \n",
    "* The Area and Flow weighted value are exported to CSV files in seperate directories with the file name format {source_id}_{experiment_id}_{member_id}_19{weight_type}Weighted.csv.\n",
    "* The monthly rolling average results are exported to CSV files in seperate diretories based on inputs weighting.  The file format is {source_id}_{member_id}_{experiment_id}_30yrAve.csv.\n",
    "\n",
    "\n",
    "Note: All SSPs/realization(member_1d) must have the corrisponding historical/realization(member_1d) otherwise the 30YrAve post-proccesing will throw an error. \n",
    "\n",
    "Inputs listed below:\n",
    "* data/Basin_Weights.csv: Constains Basin the data for the basin weighting.\n",
    "* data/GCM_Run_List_#-#.csv: Constains a run list for all GCMs of interest.  Note: Each SSP must contain corrisponding historical for 30 year rolling average calc.\n",
    "\n",
    "Known Issues:  \n",
    "* When running a large run list, if you screen is locked or browser focus is lost the kernal will lose connection with Dask array.  To avoid this, you can either convert to a script and run in a terminal window like `tmux` or `screen`, or simply shorten the run list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb58d327-b995-4189-b98b-ce8794c0546a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from io import StringIO\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "import pandas as pd\n",
    "import intake\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import dask\n",
    "import panel as pn\n",
    "from dask.distributed import get_client\n",
    "\n",
    "pn.extension()\n",
    "xr.set_options(keep_attrs=True)\n",
    "dask.config.set({\"array.slicing.split_large_chunks\": True})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e40c6c-9aaa-4365-ab1d-3bf8267c42e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VARIABLES\n",
    "#Use these cordinates to clip around the watershed of interest.\n",
    "bbox = {\n",
    "    \"maxy\": 42.432494,\n",
    "    \"miny\": 34.775317,\n",
    "    \"minx\": -123.097421,\n",
    "    \"maxx\": -117.980799,\n",
    "}\n",
    "\n",
    "#run_list_path = \"data/GCM_Run_List_1-43.csv\"\n",
    "#file_zip = \"GCM_1-43.zip\"\n",
    "\n",
    "run_list_path = \"data/GCM_Run_List_1-19.csv\"\n",
    "file_zip = \"GCM_1-19.zip\"\n",
    "\n",
    "run_list_path = \"data/GCM_Run_List_20-43.csv\"\n",
    "file_zip = \"GCM_20-43.zip\"\n",
    "\n",
    "run_list_path = \"data/GCM_Run_List_44-82.csv\"\n",
    "file_zip = \"GCM_44-82.zip\"\n",
    "\n",
    "run_list_path = \"data/GCM_Run_List_83-100.csv\"\n",
    "file_zip = \"GCM_83-100.zip\"\n",
    "\n",
    "run_list_path = \"data/GCM_Run_List_Add_1-22.csv\"\n",
    "file_zip = \"GCM_Add_1-22.zip\"\n",
    "\n",
    "run_list_path = \"data/GCM_Run_List_Add_23-47.csv\"\n",
    "file_zip = \"GCM_Add_23-47.zip\"\n",
    "\n",
    "run_list_path = \"data/GCM_Run_List_Add_48-75.csv\"\n",
    "file_zip = \"GCM_Add_48-75.zip\"\n",
    "\n",
    "#run_list_path = \"data/GCM_Run_List_Test_Rolling.csv\"\n",
    "#file_zip = \"GCM_Rolling_Test.zip\"\n",
    "\n",
    "basin_weights_csv = \"data/Basin_Weights.csv\"\n",
    "esm_datastore = \"https://cadcat.s3.amazonaws.com/cae-collection.json\"\n",
    "output_folder = \"outputs\"\n",
    "mask_path = \"mask/mask.npy\"\n",
    "dir_area_weighted = 'AREA_WEIGHTED_CENTRALVALLEY'\n",
    "dir_flow_weighted = 'FLOW_WEIGHTED_CENTRALVALLEY'\n",
    "dir_non_weighted = 'NON_WEIGHTED_CENTRALVALLEY'\n",
    "dir_individual = 'INDIVIDUAL_BASIN_LOCA2'\n",
    "dir_area_weighted_rolling = 'AREA_WEIGHTED_30_YEAR_ROLLING_AVE_CENTRALVALLEY'\n",
    "dir_flow_weighted_rolling = 'FLOW_WEIGHTED_30_YEAR_ROLLING_AVE_CENTRALVALLEY'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef6091f-e369-4137-9551-6b661cea5a57",
   "metadata": {},
   "source": [
    "This loads the dask client for faster processing.  If client is running, do not run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264d2e29-399c-427f-9efd-7e59edf7c611",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load dask Area for faster computing. \n",
    "#Note, this will take some time but in long run processing should be faster when compute is called.\n",
    "# Check if there is already a dask client running\n",
    "try:\n",
    "    client = get_client()\n",
    "    print(\"Using existing dask client\")\n",
    "except ValueError:\n",
    "    # No client found, create a new one\n",
    "    cluster = dask.distributed.LocalCluster(\n",
    "        n_workers=16,  # Adjust the number of workers as needed\n",
    "        threads_per_worker=1,  # Use one thread per worker\n",
    "        memory_limit='2GB'  # Adjust memory limit per worker as needed\n",
    "    )\n",
    "    cluster.adapt(minimum=0, maximum=16)\n",
    "    client = cluster.get_client()\n",
    "    print(\"Created new dask client\")\n",
    "\n",
    "# Get client link\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2649b7a-d8d1-44a3-9860-cf2715649e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_region_dict() -> dict:\n",
    "    \"\"\"\n",
    "    Gets flow and area weights for each region from the basin weights CSV file.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary with region ID as key and dictionary containing region_name,\n",
    "        flow_ratio, and area_ratio as values.\n",
    "    \"\"\"\n",
    "    region_dict = {}\n",
    "    with open(basin_weights_csv, \"r\") as csv_file:\n",
    "        reader = csv.DictReader(csv_file)\n",
    "        for row in reader:\n",
    "            region_dict[int(row[\"ID\"])] = {\n",
    "                                \"region_name\": row[\"Regions\"],\n",
    "                                \"flow_ratio\": float(row[\"Flow Ratio\"]),\n",
    "                                \"area_ratio\": float(row[\"Area Ratio\"]),\n",
    "                            }\n",
    "    return region_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5e0ccc-88e4-488c-94fa-8b62c0b7bdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_params(run_list_path: str) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Read each set of model parameters into dictionary from csv file.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    run_list_path : str\n",
    "        Path to CSV file containing model parameters with columns for\n",
    "        activity_id, institution_id, source_id, experiment_id, table_id,\n",
    "        grid_label, member_id, start_year, and end_year.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list[dict]\n",
    "        List of dictionaries where each dictionary contains model parameters\n",
    "        for a single model run with keys corresponding to CSV column headers.\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> params = get_model_params(\"data/GCM_Run_List_1-19.csv\")\n",
    "    >>> len(params)\n",
    "    19\n",
    "    >>> params[0].keys()\n",
    "    dict_keys(['activity_id', 'institution_id', 'source_id', ...])\n",
    "    \"\"\"\n",
    "    model_params = []\n",
    "    with open(run_list_path, \"r\") as src:\n",
    "        d = csv.DictReader(src)\n",
    "        for row in d:\n",
    "            model_params.append(row)\n",
    "    return model_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91935c5-dff2-48ae-8858-111927c9cf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_by_time_years_dataset(ds: xr.Dataset, startyear: str, endyear: str) -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Slice the dataset to years of interest.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds : xarray.Dataset\n",
    "        Input dataset containing time dimension to be sliced\n",
    "    startyear : str\n",
    "        Start year for slicing in string format (e.g., '1950')\n",
    "    endyear : str\n",
    "        End year for slicing in string format (e.g., '2014')\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    xarray.Dataset\n",
    "        Dataset sliced to the specified time range\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> ds = xr.Dataset({'temp': (['time'], np.random.rand(100))})\n",
    "    >>> sliced_ds = slice_by_time_years_dataset(ds, '1950', '2014')\n",
    "    >>> # Returns dataset with time dimension sliced from 1950 to 2014\n",
    "    \"\"\"\n",
    "    ds = ds.sel(\n",
    "        time=slice(str(startyear), str(endyear))\n",
    "        )\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8381dd-f218-4b17-a1fd-0d22e562b9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_daily_to_monthly_dataset(ds: xr.Dataset) -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Convert daily climate data to monthly aggregated values.\n",
    "    \n",
    "    Converts daily precipitation from kg/m2/s to monthly accumulated mm,\n",
    "    and daily temperature from Kelvin to monthly average Celsius.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds : xarray.Dataset\n",
    "        Input dataset containing daily climate data with variables:\n",
    "        - pr: precipitation in kg/m2/s\n",
    "        - tasmin: minimum temperature in Kelvin\n",
    "        - tasmax: maximum temperature in Kelvin\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    xarray.Dataset\n",
    "        Dataset with monthly aggregated climate data:\n",
    "        - pr: monthly accumulated precipitation in mm/mon\n",
    "        - tasmin: monthly average minimum temperature in degC\n",
    "        - tasmax: monthly average maximum temperature in degC\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    Precipitation conversion: kg/m2/s * 86400 s/day = mm/day\n",
    "    Temperature conversion: K - 273.15 = degC\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> daily_ds = xr.Dataset({\n",
    "    ...     'pr': (['time', 'lat', 'lon'], np.random.rand(365, 10, 10)),\n",
    "    ...     'tasmin': (['time', 'lat', 'lon'], np.random.rand(365, 10, 10) + 273.15),\n",
    "    ...     'tasmax': (['time', 'lat', 'lon'], np.random.rand(365, 10, 10) + 283.15)\n",
    "    ... })\n",
    "    >>> monthly_ds = convert_daily_to_monthly_dataset(daily_ds)\n",
    "    >>> monthly_ds.pr.attrs['units']\n",
    "    'mm/mon'\n",
    "    >>> monthly_ds.tasmin.attrs['units']\n",
    "    'degC'\n",
    "    \"\"\"\n",
    "    # Convert daily precipitation from kg/m2/s to mm/day\n",
    "    ds['pr'] = ds.pr * 86400\n",
    "    ds.pr.attrs[\"units\"] = 'mm/day' \n",
    "    \n",
    "    # Resample precipitation to monthly sum (accumulated)\n",
    "    ds_precip = ds['pr'].resample(time=\"M\").sum()\n",
    "    ds_precip.attrs[\"units\"] = 'mm/mon' \n",
    "    \n",
    "    # Resample temperature to monthly mean (average)\n",
    "    ds_temp = ds[['tasmin','tasmax']].resample(time=\"M\").mean()\n",
    "    \n",
    "    # Convert temperature from Kelvin to Celsius\n",
    "    ds_temp = ds_temp[['tasmin','tasmax']] - 273.15\n",
    "    ds_temp.tasmin.attrs[\"units\"] = 'degC'\n",
    "    ds_temp.tasmax.attrs[\"units\"] = 'degC'\n",
    "        \n",
    "    # Merge precipitation and temperature datasets\n",
    "    ds = xr.merge([ds_precip, ds_temp])\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203b349d-4c0d-408d-bd45-4b7a2f5d9938",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(esm_datastore: str, model_params: dict) -> xr.Dataset:\n",
    "  \"\"\"\n",
    "  Return xarray.DataSet from model parameters.\n",
    "  \n",
    "  Parameters\n",
    "  ----------\n",
    "  esm_datastore : str\n",
    "    URL or path to the ESM datastore catalog\n",
    "  model_params : dict\n",
    "    Dictionary containing model parameters with keys:\n",
    "    activity_id, institution_id, table_id, experiment_id, \n",
    "    grid_label, member_id, source_id, start_year, end_year\n",
    "  \n",
    "  Returns\n",
    "  -------\n",
    "  xarray.Dataset\n",
    "    Dataset containing precipitation (pr), maximum temperature (tasmax),\n",
    "    and minimum temperature (tasmin) data converted to monthly time step\n",
    "    and sliced to the specified time window\n",
    "  \n",
    "  Examples\n",
    "  --------\n",
    "  >>> params = {\n",
    "  ...     'activity_id': 'CMIP',\n",
    "  ...     'institution_id': 'NCAR',\n",
    "  ...     'table_id': 'day',\n",
    "  ...     'experiment_id': 'historical',\n",
    "  ...     'grid_label': 'gn',\n",
    "  ...     'member_id': 'r1i1p1f1',\n",
    "  ...     'source_id': 'CESM2',\n",
    "  ...     'start_year': '1950',\n",
    "  ...     'end_year': '2014'\n",
    "  ... }\n",
    "  >>> ds = get_dataset(\"https://cadcat.s3.amazonaws.com/cae-collection.json\", params)\n",
    "  >>> list(ds.data_vars)\n",
    "  ['pr', 'tasmax', 'tasmin']\n",
    "  \"\"\"\n",
    "  # Open catalog of available data sets using intake-esm package\n",
    "  cat = intake.open_esm_datastore(esm_datastore)\n",
    "  cat_item = cat.search(\n",
    "    activity_id=model_params[\"activity_id\"],\n",
    "    institution_id=model_params[\"institution_id\"],\n",
    "    table_id=model_params[\"table_id\"], \n",
    "    variable_id=['pr','tasmax','tasmin'],\n",
    "    experiment_id=model_params[\"experiment_id\"],\n",
    "    grid_label=model_params[\"grid_label\"],\n",
    "    member_id=model_params[\"member_id\"],\n",
    "    source_id=model_params[\"source_id\"],  \n",
    "  )\n",
    "  \n",
    "  # Add catalog item to dataset dict\n",
    "  data_dict = cat_item.to_dataset_dict(\n",
    "    #  xarray_open_kwargs={'consolidated': True},\n",
    "    storage_options={'anon': True}\n",
    "  )\n",
    "  \n",
    "  # Construct dataset key to retrieve from the dictionary\n",
    "  key = \"{}.{}.{}.{}.{}.{}\".format(\n",
    "      model_params['activity_id'],\n",
    "      model_params['institution_id'],\n",
    "      model_params['source_id'],\n",
    "      model_params['experiment_id'],\n",
    "      model_params['table_id'],\n",
    "      model_params['grid_label'],)\n",
    "  \n",
    "  # Slice the dataset to the input time window.\n",
    "  ds = slice_by_time_years_dataset(data_dict[key],model_params['start_year'],model_params['end_year'])\n",
    "  ds = convert_daily_to_monthly_dataset(ds)\n",
    "  return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d990d3-0d54-470d-be7a-aa828ef0da5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_mask_to_dataset(mask_path: str, ds: xr.Dataset) -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Attach the mask to input dataset ds.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    mask_path : str\n",
    "        Path to the numpy mask file (.npy format)\n",
    "    ds : xarray.Dataset\n",
    "        Input dataset to which the mask will be attached\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    xarray.Dataset\n",
    "        Dataset with mask coordinate added\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> ds = xr.Dataset({'temp': (['lat', 'lon'], np.random.rand(10, 10))})\n",
    "    >>> masked_ds = add_mask_to_dataset('mask/mask.npy', ds)\n",
    "    >>> 'mask' in masked_ds.coords\n",
    "    True\n",
    "    \"\"\"\n",
    "    with open(mask_path, 'rb') as f:\n",
    "        mask = np.load(f, allow_pickle=True)\n",
    "    ds.coords['mask'] = (('lat', 'lon'), mask)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3901ea78-2ae9-4335-b386-91c0124bbefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_dataset_to_bbox(ds: xr.Dataset, bbox: dict) -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Clip the dataset to a bounding box.\n",
    "    \n",
    "    This function sets the spatial dimensions and coordinate reference system\n",
    "    for the dataset, then clips it to the specified bounding box coordinates.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ds : xr.Dataset\n",
    "        Input xarray dataset containing spatial data with 'lon' and 'lat' dimensions\n",
    "    bbox : dict\n",
    "        Dictionary containing bounding box coordinates with keys:\n",
    "        - 'minx': minimum longitude\n",
    "        - 'miny': minimum latitude  \n",
    "        - 'maxx': maximum longitude\n",
    "        - 'maxy': maximum latitude\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    xr.Dataset\n",
    "        Dataset clipped to the specified bounding box\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> bbox = {\n",
    "    ...     \"maxy\": 42.432494,\n",
    "    ...     \"miny\": 34.775317,\n",
    "    ...     \"minx\": -123.097421,\n",
    "    ...     \"maxx\": -117.980799,\n",
    "    ... }\n",
    "    >>> clipped_ds = trim_dataset_to_bbox(ds, bbox)\n",
    "    >>> # Returns dataset clipped to the watershed of interest\n",
    "    \"\"\"\n",
    "    # This needs to be done for the clipping.\n",
    "    ds.rio.set_spatial_dims(x_dim=\"lon\", y_dim=\"lat\", inplace=True)\n",
    "    ds.rio.write_crs(\"EPSG:4326\", inplace=True)\n",
    "\n",
    "    # Get the subset of data for watershed.\n",
    "    ds = ds.rio.clip_box(\n",
    "        minx=bbox[\"minx\"],\n",
    "        miny=bbox[\"miny\"],\n",
    "        maxx=bbox[\"maxx\"],\n",
    "        maxy=bbox[\"maxy\"],\n",
    "    )\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405fe5e9-361c-4b9f-8561-1a4d7478a1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_file_name_monthly(model_params: dict, end_part: str) -> str:\n",
    "    \"\"\"\n",
    "    Format the output file name from model parameters.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model_params : dict\n",
    "        Dictionary containing model parameters with keys:\n",
    "        - source_id : str\n",
    "            Source identifier for the model\n",
    "        - experiment_id : str\n",
    "            Experiment identifier (e.g., 'historical', 'ssp245')\n",
    "        - member_id : str\n",
    "            Member identifier (e.g., 'r1i1p1f1')\n",
    "    end_part : str\n",
    "        String to append at the end of the filename before the .csv extension\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Formatted filename in the format: {source_id}_{experiment_id}_{member_id}_{end_part}.csv\n",
    "        \n",
    "    Examples\n",
    "    --------\n",
    "    >>> params = {\n",
    "    ...     'source_id': 'CESM2',\n",
    "    ...     'experiment_id': 'historical',\n",
    "    ...     'member_id': 'r1i1p1f1'\n",
    "    ... }\n",
    "    >>> get_output_file_name_monthly(params, '19FlowWeighted')\n",
    "    'CESM2_historical_r1i1p1f1_19FlowWeighted.csv'\n",
    "    \"\"\"\n",
    "    return '%s_%s_%s_%s.csv'%(model_params['source_id'],model_params['experiment_id'],model_params['member_id'],end_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf74e9e-2b0d-4344-af9f-c8bafc6859a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_with_mask(esm_datastore_in: str, model_params_in: dict, mask_path: str, bbox: dict) -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Load dataset, add mask, and trim to bounding box around area of interest.\n",
    "    \n",
    "    This function combines the dataset loading, mask addition, and spatial trimming\n",
    "    operations into a single convenience function for processing climate data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    esm_datastore_in : str\n",
    "        URL or path to the ESM datastore catalog\n",
    "    model_params_in : dict\n",
    "        Dictionary containing model parameters with keys:\n",
    "        activity_id, institution_id, table_id, experiment_id, \n",
    "        grid_label, member_id, source_id, start_year, end_year\n",
    "    mask_path : str\n",
    "        Path to the numpy mask file (.npy format) containing regional identifiers\n",
    "    bbox : dict\n",
    "        Dictionary containing bounding box coordinates with keys:\n",
    "        - 'minx': minimum longitude\n",
    "        - 'miny': minimum latitude  \n",
    "        - 'maxx': maximum longitude\n",
    "        - 'maxy': maximum latitude\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    xr.Dataset\n",
    "        Processed dataset with mask coordinate added and spatially trimmed to\n",
    "        the specified bounding box, containing monthly climate data for\n",
    "        precipitation (pr), maximum temperature (tasmax), and minimum \n",
    "        temperature (tasmin)\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> params = {\n",
    "    ...     'activity_id': 'CMIP',\n",
    "    ...     'source_id': 'CESM2',\n",
    "    ...     'experiment_id': 'historical',\n",
    "    ...     'start_year': '1950',\n",
    "    ...     'end_year': '2014'\n",
    "    ... }\n",
    "    >>> bbox = {\n",
    "    ...     \"minx\": -123.097421,\n",
    "    ...     \"miny\": 34.775317,\n",
    "    ...     \"maxx\": -117.980799,\n",
    "    ...     \"maxy\": 42.432494\n",
    "    ... }\n",
    "    >>> ds = load_dataset_with_mask(esm_datastore, params, 'mask/mask.npy', bbox)\n",
    "    >>> 'mask' in ds.coords\n",
    "    True\n",
    "    \"\"\"\n",
    "    ds = get_dataset(esm_datastore_in, model_params_in)\n",
    "    ds = add_mask_to_dataset(mask_path, ds)\n",
    "    ds = trim_dataset_to_bbox(ds, bbox)\n",
    "    return ds\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0d9d89-1c25-48e6-a52e-2942252a9644",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_map_mask(id_region: int, ds: xr.Dataset, use_full_mask: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns a dataframe for the specified region ID from the masked dataset.\n",
    "    \n",
    "    This function extracts climate data for a specific region based on the mask values,\n",
    "    computes spatial averages, and formats the results into a pandas DataFrame with\n",
    "    monthly climate statistics.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    id_region : int\n",
    "        Region identifier to extract from the mask. Set to -1 and use_full_mask=True\n",
    "        to get the entire domain excluding the specified region.\n",
    "    ds : xarray.Dataset\n",
    "        Input dataset containing climate variables (pr, tasmin, tasmax) and a mask\n",
    "        coordinate with regional identifiers.\n",
    "    use_full_mask : bool, optional\n",
    "        If True, extracts data where mask != id_region (excludes the region).\n",
    "        If False, extracts data where mask == id_region (includes only the region).\n",
    "        Default is False.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        DataFrame containing monthly climate data with columns:\n",
    "        - time: datetime index reset as column\n",
    "        - Year: year extracted from time index\n",
    "        - Month: month number (1-12)\n",
    "        - Pr (mm): monthly precipitation in mm/mon\n",
    "        - Tasmax (degC): monthly maximum temperature in degC\n",
    "        - Tasmin (degC): monthly minimum temperature in degC\n",
    "        - Tave (degC): monthly average temperature in degC\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    The function computes spatial averages over latitude and longitude dimensions\n",
    "    for each time step, skipping NaN values. Temperature average is calculated\n",
    "    as the mean of tasmax and tasmin.\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> # Extract data for region 5\n",
    "    >>> df = get_df_map_mask(5, ds)\n",
    "    >>> df.columns.tolist()\n",
    "    ['time', 'Year', 'Month', 'Pr (mm)', 'Tasmax (degC)', 'Tasmin (degC)', 'Tave (degC)']\n",
    "    \n",
    "    >>> # Extract data for entire domain excluding region 5\n",
    "    >>> df_full = get_df_map_mask(5, ds, use_full_mask=True)\n",
    "    \"\"\"\n",
    "    if use_full_mask:\n",
    "        map_data = ds.where(ds.mask != id_region)\n",
    "    else:\n",
    "        map_data = ds.where(ds.mask == id_region)\n",
    "\n",
    "    results_precip = map_data.pr.mean(['lat','lon'],skipna=True)\n",
    "    results_precip.attrs[\"units\"]  = 'mm/mon'\n",
    "\n",
    "    results_tasmin = map_data.tasmin.mean(['lat','lon'],skipna=True)\n",
    "    results_tasmin.attrs[\"units\"]  = 'degC'\n",
    "\n",
    "    results_tasmax = map_data.tasmax.mean(['lat','lon'],skipna=True)\n",
    "    results_tasmax.attrs[\"units\"]  = 'degC'\n",
    "\n",
    "    ds_all= xr.merge([results_precip,results_tasmax,results_tasmin])\n",
    "    df = ds_all.to_pandas()\n",
    "\n",
    "    df.drop('spatial_ref',axis=1, inplace=True)\n",
    "\n",
    "    df['Year'] = df.index.strftime('%Y')\n",
    "    df['Month'] = df.index.month\n",
    "    df['Tave (degC)'] = df[['tasmax','tasmin']].mean(axis=1)\n",
    "    df.rename({'pr': 'Pr (mm)','tasmax': 'Tasmax (degC)','tasmin' : 'Tasmin (degC)'}, axis=1,inplace=True)\n",
    "\n",
    "    df_r = df.iloc[:,[3,4,0,1,2,5]]\n",
    "    df_n = df_r.reset_index()\n",
    "    return df_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02c5ffe-4985-4495-8f97-932c877ef003",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weighted_dataframe(df_in: pd.DataFrame, weighting_factor: float) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Computes the weighted dataframe from df_in and returns the resulting dataframe.\n",
    "    \n",
    "    This function applies a weighting factor to climate variables in the input\n",
    "    dataframe by multiplying precipitation and temperature values by the specified\n",
    "    weighting factor.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_in : pd.DataFrame\n",
    "        Input dataframe containing climate data with columns:\n",
    "        - 'Pr (mm)': precipitation in mm/mon\n",
    "        - 'Tasmax (degC)': maximum temperature in degC\n",
    "        - 'Tasmin (degC)': minimum temperature in degC\n",
    "        - 'Tave (degC)': average temperature in degC\n",
    "    weighting_factor : float\n",
    "        Multiplicative factor to apply to climate variables. Typically represents\n",
    "        area ratio or flow ratio for basin weighting calculations.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Dataframe with weighted climate values. All climate variables are\n",
    "        multiplied by the weighting factor while preserving other columns.\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    The function modifies the input dataframe in-place and returns it.\n",
    "    For area-weighted calculations, weighting_factor represents the area ratio\n",
    "    of each sub-basin. For flow-weighted calculations, it represents the flow ratio.\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> df = pd.DataFrame({\n",
    "    ...     'Pr (mm)': [100, 150, 200],\n",
    "    ...     'Tasmax (degC)': [25, 30, 35],\n",
    "    ...     'Tasmin (degC)': [10, 15, 20],\n",
    "    ...     'Tave (degC)': [17.5, 22.5, 27.5]\n",
    "    ... })\n",
    "    >>> weighted_df = get_weighted_dataframe(df, 0.3)\n",
    "    >>> weighted_df['Pr (mm)'].tolist()\n",
    "    [30.0, 45.0, 60.0]\n",
    "    \"\"\"\n",
    "    df_in['Pr (mm)'] = df_in['Pr (mm)'] * weighting_factor\n",
    "    df_in['Tasmax (degC)'] = df_in['Tasmax (degC)'] * weighting_factor\n",
    "    df_in['Tasmin (degC)'] = df_in['Tasmin (degC)'] * weighting_factor\n",
    "    df_in['Tave (degC)'] = df_in['Tave (degC)'] * weighting_factor\n",
    "    return df_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fb4db7-db40-4955-9cd6-2a9270e98526",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sum_dataframes(df_in: pd.DataFrame, df_to_add: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds the weighted results from df_to_add to the df_in and returns the resulting dataframe.\n",
    "    \n",
    "    This function performs element-wise addition of climate variables between two dataframes,\n",
    "    filling missing values with 0. It's used to aggregate weighted climate data across\n",
    "    multiple regions or time periods.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_in : pd.DataFrame\n",
    "        Base dataframe containing climate data with columns:\n",
    "        - 'Pr (mm)': precipitation in mm/mon\n",
    "        - 'Tasmax (degC)': maximum temperature in degC\n",
    "        - 'Tasmin (degC)': minimum temperature in degC\n",
    "        - 'Tave (degC)': average temperature in degC\n",
    "    df_to_add : pd.DataFrame\n",
    "        Dataframe to add to df_in, containing the same climate columns\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Dataframe with summed climate values. The function modifies df_in in-place\n",
    "        and returns it with climate variables added from df_to_add.\n",
    "        \n",
    "    Notes\n",
    "    -----\n",
    "    The function uses pandas.Series.add() with fill_value=0 to handle missing values\n",
    "    during the addition operation. This ensures that NaN values are treated as 0\n",
    "    in the summation.\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> df1 = pd.DataFrame({\n",
    "    ...     'Pr (mm)': [100, 150, 200],\n",
    "    ...     'Tasmax (degC)': [25, 30, 35],\n",
    "    ...     'Tasmin (degC)': [10, 15, 20],\n",
    "    ...     'Tave (degC)': [17.5, 22.5, 27.5]\n",
    "    ... })\n",
    "    >>> df2 = pd.DataFrame({\n",
    "    ...     'Pr (mm)': [50, 75, 100],\n",
    "    ...     'Tasmax (degC)': [5, 7, 10],\n",
    "    ...     'Tasmin (degC)': [2, 3, 5],\n",
    "    ...     'Tave (degC)': [3.5, 5, 7.5]\n",
    "    ... })\n",
    "    >>> result = get_sum_dataframes(df1, df2)\n",
    "    >>> result['Pr (mm)'].tolist()\n",
    "    [150.0, 225.0, 300.0]\n",
    "    \"\"\"\n",
    "    df_in['Pr (mm)'] = df_in['Pr (mm)'].add(df_to_add['Pr (mm)'], fill_value=0)\n",
    "    df_in['Tasmax (degC)'] = df_in['Tasmax (degC)'].add(df_to_add['Tasmax (degC)'], fill_value=0)\n",
    "    df_in['Tasmin (degC)'] = df_in['Tasmin (degC)'].add(df_to_add['Tasmin (degC)'], fill_value=0)\n",
    "    df_in['Tave (degC)'] = df_in['Tave (degC)'].add(df_to_add['Tave (degC)'], fill_value=0)\n",
    "    return df_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dee56f-bbfd-407d-af98-23b70feccd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_monthly_rolling_ave(dict_df_weighted_all: dict, average_over_years: int, append_name: str) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate rolling average for each SSP (Shared Socioeconomic Pathway) scenario.\n",
    "    \n",
    "    This function processes historical and SSP climate data to compute rolling averages\n",
    "    over a specified number of years. It matches historical data with corresponding\n",
    "    SSP scenarios and calculates monthly averages for each rolling window.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dict_df_weighted_all : dict\n",
    "        Dictionary with filename keys and weighted dataframe values containing\n",
    "        climate data. Filenames should follow format: \n",
    "        {source_id}_{experiment_id}_{member_id}_{suffix}.csv\n",
    "    average_over_years : int\n",
    "        Number of years to average over for rolling calculation (typically 30)\n",
    "    append_name : str\n",
    "        String to append to output filenames (e.g., '30yrAve')\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary with filename keys and rolling average dataframe values.\n",
    "        Output filenames follow format: {source_id}_{member_id}_{ssp_experiment}_{append_name}.csv\n",
    "        Each dataframe contains columns:\n",
    "        - 'Year Range': string showing start-end years of rolling window\n",
    "        - 'Year (30y start)': start year of rolling window\n",
    "        - 'Month': month number (1-12)\n",
    "        - 'Pr (mm)': monthly average precipitation\n",
    "        - 'Tasmax (degC)': monthly average maximum temperature\n",
    "        - 'Tasmin (degC)': monthly average minimum temperature\n",
    "        - 'Tave (degC)': monthly average temperature\n",
    "        \n",
    "    Notes\n",
    "    -----\n",
    "    The function expects historical data to be paired with SSP scenarios for the\n",
    "    same GCM and realization. Rolling averages are calculated from 1950 to 2072\n",
    "    using the specified averaging window.\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> weighted_data = {\n",
    "    ...     'CESM2_historical_r1i1p1f1_19FlowWeighted.csv': df_hist,\n",
    "    ...     'CESM2_ssp245_r1i1p1f1_19FlowWeighted.csv': df_ssp\n",
    "    ... }\n",
    "    >>> rolling_dict = get_monthly_rolling_ave(weighted_data, 30, '30yrAve')\n",
    "    >>> list(rolling_dict.keys())\n",
    "    ['CESM2_r1i1p1f1_ssp245_30yrAve.csv']\n",
    "    \"\"\"\n",
    "    dict_fil_rolling_df = {}  # Dict with filename keys and rolling average dataframe as value.\n",
    "    dict_gcm_hist_realization = {} # Dict with Historical dataframes.\n",
    "    dict_gcm_other_realization = {} # Dict with SSP dataframes.\n",
    "    \n",
    "    #Key is the formated file name and value is dataframe.\n",
    "    for key_file, value_df in dict_df_weighted_all.items():\n",
    "        lst_file_parts = key_file.split('_')\n",
    "        if 'historical' in key_file:\n",
    "            if not lst_file_parts[0] in dict_gcm_hist_realization:\n",
    "                dict_gcm_hist_realization[lst_file_parts[0]] = {}\n",
    "            if not lst_file_parts[1] in dict_gcm_hist_realization[lst_file_parts[0]]:\n",
    "                dict_gcm_hist_realization[lst_file_parts[0]][lst_file_parts[1]] = {}\n",
    "            dict_gcm_hist_realization[lst_file_parts[0]][lst_file_parts[1]][lst_file_parts[2]] = value_df\n",
    "        else:\n",
    "            if not lst_file_parts[0] in dict_gcm_other_realization:\n",
    "                dict_gcm_other_realization[lst_file_parts[0]] = {}\n",
    "            if not lst_file_parts[1] in dict_gcm_other_realization[lst_file_parts[0]]:\n",
    "                dict_gcm_other_realization[lst_file_parts[0]][lst_file_parts[1]] = {}\n",
    "            dict_gcm_other_realization[lst_file_parts[0]][lst_file_parts[1]][lst_file_parts[2]]=value_df      \n",
    "    \n",
    "    # Do rolling average and output with dictionary key as filename and values as rolling average dataframe.\n",
    "    df_rolling = pd.DataFrame\n",
    "    start_year = 1950\n",
    "    end_year = 2072\n",
    "    for key_gcm in dict_gcm_hist_realization:\n",
    "        for key_ssp in dict_gcm_hist_realization[key_gcm]:\n",
    "            for key_relization in dict_gcm_hist_realization[key_gcm][key_ssp]:\n",
    "                df_history = dict_gcm_hist_realization[key_gcm][key_ssp][key_relization]\n",
    "                for key_ssp_other in dict_gcm_other_realization[key_gcm]:\n",
    "                    if not key_relization in dict_gcm_other_realization[key_gcm][key_ssp_other]: continue\n",
    "                    df_ssp = dict_gcm_other_realization[key_gcm][key_ssp_other][key_relization]\n",
    "                    df_out = None\n",
    "                    df_rolling = pd.concat([df_history,df_ssp], axis=0)\n",
    "                    df_rolling.drop('time' , axis=1, inplace=True)  \n",
    "                    for currentYear in range(start_year,end_year):\n",
    "                        year_30 = currentYear + average_over_years\n",
    "                        df30year = df_rolling[(df_rolling['Year'].astype(int) >= currentYear) & (df_rolling['Year'].astype(int) < year_30)]\n",
    "                        dftemp = df30year.groupby(df30year.Month, as_index=False, sort=True)[['Pr (mm)','Tasmax (degC)','Tasmin (degC)','Tave (degC)']].mean().reset_index()\n",
    "                        dftemp.insert(loc=0,column=\"Year (30y start)\",value=currentYear)\n",
    "                        dftemp.insert(loc=0,column=\"Year Range\",value='%s-%s'%(currentYear,year_30-1))\n",
    "                        if df_out is None:\n",
    "                            df_out = dftemp.copy(deep=True)\n",
    "                        else:\n",
    "                            df_out = pd.concat([df_out,dftemp], axis=0)\n",
    "                    df_out.drop('index', axis=1, inplace=True) \n",
    "                    file_out = '%s_%s_%s_%s.csv'%(key_gcm,key_relization,key_ssp_other,append_name)\n",
    "                    dict_fil_rolling_df[file_out] = df_out\n",
    "    return dict_fil_rolling_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501cd359-2f63-4723-ada5-9a759182adb2",
   "metadata": {},
   "source": [
    "The loop goes through all GCMs and writes the individual subbasin, the area weighted, and the flow weighted to results dictionaries with filname as the key and value equal to result dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0af3b2e-5cb6-495c-a786-7964323026bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "region_dict = get_region_dict()\n",
    "# main loop\n",
    "all_model_params = get_model_params(run_list_path)\n",
    "\n",
    "#Defile output dicts.\n",
    "results_dict = {}\n",
    "flow_weighted_results_dict = {}\n",
    "area_weighted_basin_results_dict = {}\n",
    "for model_params in all_model_params:\n",
    "    #Add masking to the dataset.\n",
    "    ds = load_dataset_with_mask(esm_datastore, model_params, mask_path,bbox)\n",
    "   \n",
    "    key = \"{}.{}.{}.{}.{}.{}\".format(\n",
    "        model_params['activity_id'],\n",
    "        model_params['institution_id'],\n",
    "        model_params['source_id'],\n",
    "        model_params['experiment_id'],\n",
    "        model_params['table_id'],\n",
    "        model_params['grid_label'],)\n",
    "    #Force load the dataset.\n",
    "    print('Loading: %s'%key)\n",
    "    ds = ds.compute()\n",
    "        \n",
    "    df_w = None\n",
    "    df_a = None\n",
    "    df_nw = None\n",
    "    for id_region, v in region_dict.items():     \n",
    "        # Get this regions results\n",
    "        df_n = get_df_map_mask(id_region,ds)\n",
    "        output_filename = get_output_file_name_monthly(model_params, '%s-19'%'{:02d}'.format(id_region))\n",
    "\n",
    "        df_out = df_n.drop('time' , axis=1)\n",
    "        results_dict[output_filename] = df_out\n",
    "                \n",
    "        # Get Area Weighted dataframe\n",
    "        weighting_factor = v['area_ratio']\n",
    "        df_weighted_a = get_weighted_dataframe(df_n.copy(deep=True),weighting_factor)\n",
    "        if df_a is None:\n",
    "            df_a = df_weighted_a.copy(deep=True)\n",
    "        else:\n",
    "            df_a = get_sum_dataframes(df_a,df_weighted_a)\n",
    "            \n",
    "        # Get Flow Weighted results \n",
    "        weighting_factor = v['flow_ratio']\n",
    "        df_weighted = get_weighted_dataframe(df_n.copy(deep=True),weighting_factor)\n",
    "        if df_w is None:\n",
    "            df_w = df_weighted.copy(deep=True)\n",
    "        else:\n",
    "            df_w = get_sum_dataframes(df_w,df_weighted)\n",
    "    print('Processed %s...'%get_output_file_name_monthly(model_params,'').replace('.csv',''))       \n",
    "    \n",
    "    #Add weighted dataframes to output.\n",
    "    output_filename = get_output_file_name_monthly(model_params, \"19FlowWeighted\")\n",
    "    flow_weighted_results_dict[output_filename] = df_w\n",
    "    output_filename = get_output_file_name_monthly(model_params, \"19AreaWeighted\")\n",
    "    area_weighted_basin_results_dict[output_filename] = df_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a128fe-3ead-4a52-a601-e6e637c5b557",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write to ouput.\n",
    "zip_path = os.path.join(output_folder, file_zip)\n",
    "\n",
    "dict_rolling_flow_weighted = get_monthly_rolling_ave(flow_weighted_results_dict,30,'30yrAve')\n",
    "dict_rolling_area_weighted = get_monthly_rolling_ave(area_weighted_basin_results_dict,30,'30yrAve')\n",
    "\n",
    "with zipfile.ZipFile(zip_path, mode=\"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
    "    for k, v in results_dict.items():\n",
    "        text_stream = StringIO();\n",
    "        v.to_csv(text_stream, index=False)\n",
    "        fileout = dir_individual + '/' + k\n",
    "        zf.writestr(fileout, text_stream.getvalue())\n",
    "\n",
    "    for k, v in flow_weighted_results_dict.items():\n",
    "        fileout = dir_flow_weighted + '/' + k\n",
    "        fileout_raw = dir_flow_weighted + '/Raw/' + k\n",
    "        text_stream = StringIO();\n",
    "        v.to_csv(text_stream, index=False)\n",
    "        zf.writestr(fileout_raw, text_stream.getvalue())\n",
    "        v.drop('time' , axis=1, inplace=True)\n",
    "        text_stream = StringIO();\n",
    "        v.to_csv(text_stream, index=False)\n",
    "        zf.writestr(fileout, text_stream.getvalue())\n",
    "\n",
    "    for k, v in area_weighted_basin_results_dict.items():\n",
    "        fileout = dir_area_weighted + '/' + k\n",
    "        fileout_raw = dir_area_weighted + '/Raw/' + k\n",
    "        text_stream = StringIO();\n",
    "        v.to_csv(text_stream, index=False)\n",
    "        zf.writestr(fileout_raw, text_stream.getvalue())\n",
    "        v.drop('time' , axis=1, inplace=True)\n",
    "        text_stream = StringIO();\n",
    "        v.to_csv(text_stream, index=False)\n",
    "        zf.writestr(fileout, text_stream.getvalue())\n",
    "\n",
    "    for k, v in dict_rolling_area_weighted.items():\n",
    "        fileout = dir_area_weighted_rolling + '/' + k\n",
    "        text_stream = StringIO();\n",
    "        v.to_csv(text_stream, index=False)\n",
    "        zf.writestr(fileout, text_stream.getvalue())\n",
    "        \n",
    "    for k, v in dict_rolling_flow_weighted.items():\n",
    "        fileout = dir_flow_weighted_rolling + '/' + k\n",
    "        text_stream = StringIO();\n",
    "        v.to_csv(text_stream, index=False)\n",
    "        zf.writestr(fileout, text_stream.getvalue())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee98484-2c9e-4768-ad92-c1dd039787d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
