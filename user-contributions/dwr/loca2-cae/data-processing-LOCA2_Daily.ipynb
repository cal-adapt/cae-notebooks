{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cce06bd7-61f8-4c22-894f-ad9941064f9f",
   "metadata": {},
   "source": [
    "## Cal-Adapt Analytics Engine Data processing script for DWR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "90ee17fd-e9a4-4b1d-9165-c4cd089fbfde",
   "metadata": {},
   "source": [
    "This script downloads climate data for pr, tasmax, and tasmin, processes the data to daily time step, weights each sub-basin of interest by Area and Flow and determines the average monthly value for the entire watershed.   \n",
    "\n",
    "The output format of processed climate data are listed below:\n",
    "* Each subasin is stored stored to exported to CSV without the weights with file name format {source_id}_{experiment_id}_{member_id}_{basin_id}_{year_start}_{year_end}_19.csv.  \n",
    "* The Area and Flow weighted value are exported to CSV files in seperate directories with the file name format {source_id}_{experiment_id}_{member_id}_{year_start}_{year_end}_19{weight_type}Weighted.csv.\n",
    "\n",
    "Inputs listed below:\n",
    "* data/Basin_Weights.csv: Constains Basin the data for the basin weighting.\n",
    "* data/GCM_Run_List_#-#.csv: Constains a run list for all GCMs of interest.  Note: Each SSP must contain corrisponding historical for 30 year rolling average calc.\n",
    "\n",
    "Known Issues:  \n",
    "* When running a large run list, if you screen is locked or browser focus is lost the kernal will lose connection with Dask array.  To avoid this, you can either convert to a script and run in a terminal window like `tmux` or `screen`, or simply shorten the run list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb58d327-b995-4189-b98b-ce8794c0546a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from io import StringIO\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "import pandas as pd\n",
    "import intake\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import dask\n",
    "import panel as pn\n",
    "\n",
    "pn.extension()\n",
    "xr.set_options(keep_attrs=True)\n",
    "dask.config.set({\"array.slicing.split_large_chunks\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e40c6c-9aaa-4365-ab1d-3bf8267c42e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VARIABLES\n",
    "#Use these cordinates to clip around the watershed of interest.\n",
    "# latitude = [34.775317,42.432494]\n",
    "# longitude = [-123.097421,-117.980799]\n",
    "bbox = {\n",
    "    \"maxy\": 42.432494,\n",
    "    \"miny\": 34.775317,\n",
    "    \"minx\": -123.097421,\n",
    "    \"maxx\": -117.980799,\n",
    "}\n",
    "\n",
    "#run_list_path = \"data/GCM_Run_List_1-43.csv\"\n",
    "#file_zip = \"GCM_1-43.zip\"\n",
    "\n",
    "# run_list_path = \"data/GCM_Run_List_1_Test.csv\"\n",
    "# file_zip = \"GCM_1.zip\"\n",
    "\n",
    "run_list_path = \"data/GCM_Run_List_All.csv\"\n",
    "file_zip = \"GCM_All.zip\"\n",
    "\n",
    "#run_list_path = \"data/GCM_Run_List_20-43.csv\"\n",
    "#file_zip = \"GCM_20-43.zip\"\n",
    "\n",
    "#run_list_path = \"data/GCM_Run_List_44-82.csv\"\n",
    "#file_zip = \"GCM_44-82.zip\"\n",
    "\n",
    "#run_list_path = \"data/GCM_Run_List_83-100.csv\"\n",
    "#file_zip = \"GCM_83-100.zip\"\n",
    "\n",
    "#run_list_path = \"data/GCM_Run_List_Add_1-22.csv\"\n",
    "#file_zip = \"GCM_Add_1-22.zip\"\n",
    "\n",
    "#run_list_path = \"data/GCM_Run_List_Add_23-47.csv\"\n",
    "#file_zip = \"GCM_Add_23-47.zip\"\n",
    "\n",
    "#run_list_path = \"data/GCM_Run_List_Add_48-75.csv\"\n",
    "#file_zip = \"GCM_Add_48-75.zip\"\n",
    "\n",
    "#run_list_path = \"data/GCM_Run_List_Test_Rolling.csv\"\n",
    "#file_zip = \"GCM_Rolling_Test.zip\"\n",
    "\n",
    "basin_weights_csv = \"data/Basin_Weights.csv\"\n",
    "esm_datastore = \"https://cadcat.s3.amazonaws.com/cae-collection.json\"\n",
    "output_folder = \"outputs\"\n",
    "mask_path = \"mask/mask.npy\"\n",
    "dir_area_weighted = 'AREA_WEIGHTED_CENTRALVALLEY'\n",
    "dir_flow_weighted = 'FLOW_WEIGHTED_CENTRALVALLEY'\n",
    "dir_non_weighted = 'NON_WEIGHTED_CENTRALVALLEY'\n",
    "dir_individual = 'INDIVIDUAL_BASIN_LOCA2'\n",
    "dir_area_weighted_rolling = 'AREA_WEIGHTED_30_YEAR_ROLLING_AVE_CENTRALVALLEY'\n",
    "dir_flow_weighted_rolling = 'FLOW_WEIGHTED_30_YEAR_ROLLING_AVE_CENTRALVALLEY'\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ef6091f-e369-4137-9551-6b661cea5a57",
   "metadata": {},
   "source": [
    "This loads the dask client for faster processing.  If client is running, do not run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264d2e29-399c-427f-9efd-7e59edf7c611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Load dask Area for faster computing. \n",
    "# #Note, this will take awhile but in long run processing should be faster when compute is called.\n",
    "# cluster = Cluster()\n",
    "# cluster.adapt(minimum=0, maximum=16)\n",
    "# #cluster.adapt(minimum=0, maximum=30)\n",
    "# client = cluster.get_client()\n",
    "\n",
    "# #Get client link.\n",
    "# client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2649b7a-d8d1-44a3-9860-cf2715649e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_region_dict():\n",
    "    \"\"\" Gets flow and area weights for each region \"\"\"\n",
    "    region_dict = {}\n",
    "    with open(basin_weights_csv, \"r\") as csv_file:\n",
    "        reader = csv.DictReader(csv_file)\n",
    "        for row in reader:\n",
    "            region_dict[int(row[\"ID\"])] = {\n",
    "                                \"region_name\": row[\"Regions\"],\n",
    "                                \"flow_ratio\": float(row[\"Flow Ratio\"]),\n",
    "                                \"area_ratio\": float(row[\"Area Ratio\"]),\n",
    "                            }\n",
    "    return region_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5e0ccc-88e4-488c-94fa-8b62c0b7bdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_params(run_list_path):\n",
    "    \"\"\" Read each set of model parameters into dictionary from csv file.\n",
    "        Return list of dictionaries.\n",
    "    \"\"\"\n",
    "    model_params = []\n",
    "    with open(run_list_path, \"r\") as src:\n",
    "        d = csv.DictReader(src)\n",
    "        for row in d:\n",
    "            model_params.append(row)\n",
    "    return model_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203b349d-4c0d-408d-bd45-4b7a2f5d9938",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(esm_datastore, model_params):\n",
    "    \"\"\"Return xarray.DataSet from model parameters.\"\"\"\n",
    "    # Open catalog of available data sets using intake-esm package\n",
    "    cat = intake.open_esm_datastore(esm_datastore)\n",
    "    cat_item = cat.search(\n",
    "        activity_id=model_params[\"activity_id\"],\n",
    "        institution_id=model_params[\"institution_id\"],\n",
    "        table_id=model_params[\"table_id\"], \n",
    "        variable_id=['pr','tasmax','tasmin'],\n",
    "        experiment_id=model_params[\"experiment_id\"],\n",
    "        grid_label=model_params[\"grid_label\"],\n",
    "        member_id=model_params[\"member_id\"],\n",
    "        source_id=model_params[\"source_id\"],  \n",
    "    )\n",
    "    \n",
    "    # Add catalog item to dataset dict\n",
    "    data_dict = cat_item.to_dataset_dict(\n",
    "      #  xarray_open_kwargs={'consolidated': True},\n",
    "        storage_options={'anon': True}\n",
    "    )\n",
    "    \n",
    "    # Construct dataset key to retrieve from the dictionary\n",
    "    key = \"{}.{}.{}.{}.{}.{}\".format(\n",
    "            model_params['activity_id'],\n",
    "            model_params['institution_id'],\n",
    "            model_params['source_id'],\n",
    "            model_params['experiment_id'],\n",
    "            model_params['table_id'],\n",
    "            model_params['grid_label'],)\n",
    "    \n",
    "    # Slice the dataset to the input time window.\n",
    "    ds = slice_by_time_years_dataset(data_dict[key],model_params['start_year'],model_params['end_year'])\n",
    "    ds = convert_units_only_dataset(ds)  #convert_daily_to_monthly_dataset(ds)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d990d3-0d54-470d-be7a-aa828ef0da5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_mask_to_dataset(mask_path, ds):\n",
    "    \"\"\" Attach the mask input dataset ds. \"\"\"\n",
    "    with open(mask_path, 'rb') as f:\n",
    "        mask = np.load(f, allow_pickle=True)\n",
    "    ds.coords['mask'] = (('lat', 'lon'), mask)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8381dd-f218-4b17-a1fd-0d22e562b9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_daily_to_monthly_dataset(ds):\n",
    "    \"\"\" Convert our daily values to monthly.  Precip is the accumulated and temperature is the average.\n",
    "        Precip is converted to mm from kg/m2/s: 86400 x kg/m2/s = daily value (mm).\n",
    "        Temperature is converts to C.  C = K - 273.15\n",
    "    \"\"\"\n",
    "    #Convert our daily values to monthly.  Precip is the accumulated and temperature is the average.\n",
    "    # 86400 x kg/m2/s = daily value (mm)\n",
    "    ds['pr'] = ds.pr * 86400\n",
    "    ds.pr.attrs[\"units\"] = 'mm/day' \n",
    "    ds_precip = ds['pr'].resample(time=\"M\").sum()\n",
    "    ds_precip.attrs[\"units\"] = 'mm/mon' \n",
    "    ds_temp = ds[['tasmin','tasmax']].resample(time=\"M\").mean()\n",
    "    \n",
    "    #Change the temp to C\n",
    "    ds_temp = ds_temp[['tasmin','tasmax']] - 273.15\n",
    "    ds_temp.tasmin.attrs[\"units\"]  = 'degC'\n",
    "    ds_temp.tasmax.attrs[\"units\"]  = 'degC'\n",
    "        \n",
    "    #Merge the dataset back into on dataset.\n",
    "    ds= xr.merge([ds_precip,ds_temp])\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0914aec6-0a4b-4b9f-9a0c-8adcf945892c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_units_only_dataset(ds):\n",
    "    \"\"\" Convert our daily values.  Precip is the accumulated and temperature is the average.\n",
    "        Precip is converted to mm from kg/m2/s: 86400 x kg/m2/s = daily value (mm).\n",
    "        Temperature is converts to C.  C = K - 273.15\n",
    "    \"\"\"\n",
    "    #Convert our daily values to monthly.  Precip is the accumulated and temperature is the average.\n",
    "    # 86400 x kg/m2/s = daily value (mm)\n",
    "    ds_precip = ds.pr * 86400\n",
    "    ds_precip.attrs[\"units\"] = 'mm/day' \n",
    "    \n",
    "    #Change the temp to C\n",
    "    ds_temp = ds[['tasmin','tasmax']] - 273.15\n",
    "    ds_temp.tasmin.attrs[\"units\"]  = 'degC'\n",
    "    ds_temp.tasmax.attrs[\"units\"]  = 'degC'\n",
    "    \n",
    "    \n",
    "    #Merge the dataset back into on dataset.\n",
    "    ds= xr.merge([ds_precip,ds_temp])\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91935c5-dff2-48ae-8858-111927c9cf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_by_time_years_dataset(ds,startyear,endyear):\n",
    "    \"\"\" Slice the dataset to years of interest. \"\"\"\n",
    "    ds = ds.sel(\n",
    "        time=slice(str(startyear), str(endyear))\n",
    "        )\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3901ea78-2ae9-4335-b386-91c0124bbefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_dataset_to_bbox(ds, bbox):\n",
    "    \"\"\" Clip the dataset to a box. \"\"\"\n",
    "    #This needs to be done for the cliping.\n",
    "    ds.rio.set_spatial_dims(x_dim=\"lon\", y_dim=\"lat\", inplace=True)\n",
    "    ds.rio.write_crs(\"EPSG:4326\", inplace=True)\n",
    "\n",
    "    #Get the subset of data for watershed.\n",
    "    ds = ds.rio.clip_box(\n",
    "        minx=bbox[\"minx\"],\n",
    "        miny=bbox[\"miny\"],\n",
    "        maxx=bbox[\"maxx\"],\n",
    "        maxy=bbox[\"maxy\"],\n",
    "    )\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405fe5e9-361c-4b9f-8561-1a4d7478a1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_file_name_monthly(model_params,end_part):\n",
    "    \"\"\" Format the output file name from model_params \"\"\"\n",
    "    return '%s_%s_%s_%s_%s_%s.csv'%(model_params['source_id'],model_params['experiment_id'],model_params['member_id'],model_params['start_year'],model_params['end_year'],end_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf74e9e-2b0d-4344-af9f-c8bafc6859a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_with_mask(esm_datastore_in, model_params_in, mask_path,bbox):\n",
    "    \"\"\" Loads the dataset, adds mask, and trims dataset to box arround area of interest\"\"\"\n",
    "    ds = get_dataset(esm_datastore, model_params)\n",
    "    ds = add_mask_to_dataset(mask_path, ds)\n",
    "    ds = trim_dataset_to_bbox(ds, bbox)\n",
    "    return ds\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0d9d89-1c25-48e6-a52e-2942252a9644",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_map_mask(id_region,ds,use_full_mask = False):\n",
    "    \"\"\" Returns a dataframe for the id_region. Set id_region = -1 and use_full_mask to get the entire domain. \"\"\" \n",
    "    if use_full_mask:\n",
    "        map_data = ds.where(ds.mask != id_region)\n",
    "    else:\n",
    "        map_data = ds.where(ds.mask == id_region)\n",
    "\n",
    "    results_precip = map_data.pr.mean(['lat','lon'],skipna=True)\n",
    "    results_precip.attrs[\"units\"]  = 'mm/day'\n",
    "\n",
    "    results_tasmin = map_data.tasmin.mean(['lat','lon'],skipna=True)\n",
    "    results_tasmin.attrs[\"units\"]  = 'degC'\n",
    "\n",
    "    results_tasmax = map_data.tasmax.mean(['lat','lon'],skipna=True)\n",
    "    results_tasmax.attrs[\"units\"]  = 'degC'\n",
    "\n",
    "    ds_all= xr.merge([results_precip,results_tasmax,results_tasmin])\n",
    "    df = ds_all.to_dataframe()\n",
    "    \n",
    "    df= df.reset_index()\n",
    "\n",
    "    \n",
    "    df.drop('spatial_ref',axis=1, inplace=True)\n",
    "\n",
    "    df['Year'] = pd.to_datetime(df['time']).dt.strftime('%Y')\n",
    "    df['Month'] = pd.to_datetime(df['time']).dt.strftime('%m') #df['time'].month #strftime('%M')\n",
    "    df['Day'] = pd.to_datetime(df['time']).dt.strftime('%d')\n",
    "    df['Tave (degC)'] = df[['tasmax','tasmin']].mean(axis=1)\n",
    "    df.rename({'pr': 'Pr (mm)','tasmax': 'Tasmax (degC)','tasmin' : 'Tasmin (degC)'}, axis=1,inplace=True)\n",
    "    #print(df.head())\n",
    "    df_r = df.iloc[:,[0,1,5,6,7,2,3,4,5]]\n",
    "    df_n = df_r.reset_index()\n",
    "    #print(df_n.head())\n",
    "    return df_n\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02c5ffe-4985-4495-8f97-932c877ef003",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weighted_dataframe(df_in, weighting_factor):\n",
    "    \"\"\" Computes the weighted from from df_in and returns the resulting dataframe. \"\"\"\n",
    "    df_in['Pr (mm)'] = df_in['Pr (mm)'] * weighting_factor\n",
    "    df_in['Tasmax (degC)'] = df_in['Tasmax (degC)'] * weighting_factor\n",
    "    df_in['Tasmin (degC)'] = df_in['Tasmin (degC)'] * weighting_factor\n",
    "#    df_in['Tave (degC)'] = df_in['Tave (degC)'] * weighting_factor\n",
    "    return df_in\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fb4db7-db40-4955-9cd6-2a9270e98526",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sum_dataframes(df_in, df_to_add):\n",
    "    \"\"\" Adds the weighted results from df_to_add to the df_in and returns the resulting dataframe. \"\"\"\n",
    "    df_in['Pr (mm)'] = df_in['Pr (mm)'].add(df_to_add['Pr (mm)'], fill_value=0)\n",
    "    df_in['Tasmax (degC)'] = df_in['Tasmax (degC)'].add(df_to_add['Tasmax (degC)'], fill_value=0)\n",
    "    df_in['Tasmin (degC)'] = df_in['Tasmin (degC)'].add(df_to_add['Tasmin (degC)'], fill_value=0)\n",
    "    #df_in['Tave (degC)'] = df_in['Tave (degC)'].add(df_to_add['Tave (degC)'], fill_value=0)\n",
    "    return df_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dee56f-bbfd-407d-af98-23b70feccd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_monthly_rolling_ave(dict_df_weighted_all,average_over_years,append_name):\n",
    "    \"\"\"Gets rolling average for each SSP\"\"\"\n",
    "    dict_fil_rolling_df = {}  # Dict with filename keys and rolling average dataframe as value.\n",
    "    dict_gcm_hist_realization = {} # Dict with Historical dataframes.\n",
    "    dict_gcm_other_realization = {} # Dict with SSP dataframes.\n",
    "    #Key is the formated file name and value is dataframe.\n",
    "    for key_file, value_df in dict_df_weighted_all.items():\n",
    "        lst_file_parts = key_file.split('_')\n",
    "        if 'historical' in key_file:\n",
    "            if not lst_file_parts[0] in dict_gcm_hist_realization:\n",
    "                dict_gcm_hist_realization[lst_file_parts[0]] = {}\n",
    "            if not lst_file_parts[1] in dict_gcm_hist_realization[lst_file_parts[0]]:\n",
    "                dict_gcm_hist_realization[lst_file_parts[0]][lst_file_parts[1]] = {}\n",
    "            dict_gcm_hist_realization[lst_file_parts[0]][lst_file_parts[1]][lst_file_parts[2]] = value_df\n",
    "        else:\n",
    "            if not lst_file_parts[0] in dict_gcm_other_realization:\n",
    "                dict_gcm_other_realization[lst_file_parts[0]] = {}\n",
    "            if not lst_file_parts[1] in dict_gcm_other_realization[lst_file_parts[0]]:\n",
    "                dict_gcm_other_realization[lst_file_parts[0]][lst_file_parts[1]] = {}\n",
    "            dict_gcm_other_realization[lst_file_parts[0]][lst_file_parts[1]][lst_file_parts[2]]=value_df      \n",
    "    \n",
    "    # Do rolling average and output with dictionary key as filename and values as rolling average dataframe.\n",
    "    df_rolling = pd.DataFrame\n",
    "    start_year = 1950\n",
    "    end_year = 2072\n",
    "    for key_gcm in dict_gcm_hist_realization:\n",
    "        for key_ssp in dict_gcm_hist_realization[key_gcm]:\n",
    "            for key_relization in dict_gcm_hist_realization[key_gcm][key_ssp]:\n",
    "                df_history = dict_gcm_hist_realization[key_gcm][key_ssp][key_relization]\n",
    "                for key_ssp_other in dict_gcm_other_realization[key_gcm]:\n",
    "                    if not key_relization in dict_gcm_other_realization[key_gcm][key_ssp_other]: continue\n",
    "                    df_ssp = dict_gcm_other_realization[key_gcm][key_ssp_other][key_relization]\n",
    "                    df_out = None\n",
    "                    df_rolling = pd.concat([df_history,df_ssp], axis=0)\n",
    "                    df_rolling.drop('time' , axis=1, inplace=True)  \n",
    "                    for currentYear in range(start_year,end_year):\n",
    "                        year_30 = currentYear + average_over_years\n",
    "                        df30year = df_rolling[(df_rolling['Year'].astype(int) >= currentYear) & (df_rolling['Year'].astype(int) < year_30)]\n",
    "                        dftemp = df30year.groupby(df30year.Month, as_index=False, sort=True)[['Pr (mm)','Tasmax (degC)','Tasmin (degC)','Tave (degC)']].mean().reset_index()\n",
    "                        dftemp.insert(loc=0,column=\"Year (30y start)\",value=currentYear)\n",
    "                        dftemp.insert(loc=0,column=\"Year Range\",value='%s-%s'%(currentYear,year_30-1))\n",
    "                        if df_out is None:\n",
    "                            df_out = dftemp.copy(deep=True)\n",
    "                        else:\n",
    "                            df_out = pd.concat([df_out,dftemp], axis=0)\n",
    "                    df_out.drop('index', axis=1, inplace=True) \n",
    "                    file_out = '%s_%s_%s_%s.csv'%(key_gcm,key_relization,key_ssp_other,append_name)\n",
    "                    dict_fil_rolling_df[file_out] = df_out\n",
    "    return dict_fil_rolling_df\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "501cd359-2f63-4723-ada5-9a759182adb2",
   "metadata": {},
   "source": [
    "The loop goes through all GCMs and writes the individual subbasin, the area weighted, and the flow weighted to results dictionaries with filname as the key and value equal to result dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0af3b2e-5cb6-495c-a786-7964323026bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "region_dict = get_region_dict()\n",
    "# main loop\n",
    "all_model_params = get_model_params(run_list_path)\n",
    "\n",
    "#Defile output dicts.\n",
    "results_dict = {}\n",
    "flow_weighted_results_dict = {}\n",
    "area_weighted_basin_results_dict = {}\n",
    "for model_params in all_model_params:\n",
    "    #Add masking to the dataset.\n",
    "    ds = load_dataset_with_mask(esm_datastore, model_params, mask_path,bbox)\n",
    "   \n",
    "    key = \"{}.{}.{}.{}.{}.{}\".format(\n",
    "        model_params['activity_id'],\n",
    "        model_params['institution_id'],\n",
    "        model_params['source_id'],\n",
    "        model_params['experiment_id'],\n",
    "        model_params['table_id'],\n",
    "        model_params['grid_label'],)\n",
    "    #Force load the dataset.\n",
    "    print('Loading: %s'%key)\n",
    "    ds = ds.compute()\n",
    " \n",
    "    df_w = None\n",
    "    df_a = None\n",
    "    df_nw = None\n",
    "    for id_region, v in region_dict.items():     \n",
    "        # Get this regions results\n",
    "        df_n = get_df_map_mask(id_region,ds)\n",
    "        output_filename = get_output_file_name_monthly(model_params, '%s-19'%'{:02d}'.format(id_region))\n",
    "\n",
    "        df_out = df_n.drop('time' , axis=1)\n",
    "        results_dict[output_filename] = df_out\n",
    "                \n",
    "        # Get Area Weighted dataframe\n",
    "        weighting_factor = v['area_ratio']\n",
    "        df_weighted_a = get_weighted_dataframe(df_n.copy(deep=True),weighting_factor)\n",
    "        if df_a is None:\n",
    "            df_a = df_weighted_a.copy(deep=True)\n",
    "        else:\n",
    "            df_a = get_sum_dataframes(df_a,df_weighted_a)\n",
    "            \n",
    "        # Get Flow Weighted results \n",
    "        weighting_factor = v['flow_ratio']\n",
    "        df_weighted = get_weighted_dataframe(df_n.copy(deep=True),weighting_factor)\n",
    "        if df_w is None:\n",
    "            df_w = df_weighted.copy(deep=True)\n",
    "        else:\n",
    "            df_w = get_sum_dataframes(df_w,df_weighted)\n",
    "    print('Processed %s...'%get_output_file_name_monthly(model_params,'').replace('.csv',''))       \n",
    "    \n",
    "    #Add weighted dataframes to output.\n",
    "    output_filename = get_output_file_name_monthly(model_params, \"19FlowWeighted\")\n",
    "    flow_weighted_results_dict[output_filename] = df_w\n",
    "    output_filename = get_output_file_name_monthly(model_params, \"19AreaWeighted\")\n",
    "    area_weighted_basin_results_dict[output_filename] = df_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a128fe-3ead-4a52-a601-e6e637c5b557",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write to ouput.\n",
    "zip_path = os.path.join(output_folder, file_zip)\n",
    "\n",
    "#dict_rolling_flow_weighted = get_monthly_rolling_ave(flow_weighted_results_dict,30,'30yrAve')\n",
    "#dict_rolling_area_weighted = get_monthly_rolling_ave(area_weighted_basin_results_dict,30,'30yrAve')\n",
    "\n",
    "with zipfile.ZipFile(zip_path, mode=\"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
    "    for k, v in results_dict.items():\n",
    "        text_stream = StringIO();\n",
    "        v.to_csv(text_stream, index=False)\n",
    "        fileout = dir_individual + '/' + k\n",
    "        zf.writestr(fileout, text_stream.getvalue())\n",
    "    for k, v in flow_weighted_results_dict.items():\n",
    "        fileout = dir_flow_weighted + '/' + k\n",
    "        fileout_raw = dir_flow_weighted + '/Raw/' + k\n",
    "        text_stream = StringIO();\n",
    "        v.to_csv(text_stream, index=False)\n",
    "        zf.writestr(fileout_raw, text_stream.getvalue())\n",
    "        v.drop('time' , axis=1, inplace=True)\n",
    "        text_stream = StringIO();\n",
    "        v.to_csv(text_stream, index=False)\n",
    "        zf.writestr(fileout, text_stream.getvalue())\n",
    "    for k, v in area_weighted_basin_results_dict.items():\n",
    "        fileout = dir_area_weighted + '/' + k\n",
    "        fileout_raw = dir_area_weighted + '/Raw/' + k\n",
    "        text_stream = StringIO();\n",
    "        v.to_csv(text_stream, index=False)\n",
    "        zf.writestr(fileout_raw, text_stream.getvalue())\n",
    "        v.drop('time' , axis=1, inplace=True)\n",
    "        text_stream = StringIO();\n",
    "        v.to_csv(text_stream, index=False)\n",
    "        zf.writestr(fileout, text_stream.getvalue())\n",
    "#    for k, v in dict_rolling_area_weighted.items():\n",
    "#        fileout = dir_area_weighted_rolling + '/' + k\n",
    "#        text_stream = StringIO();\n",
    "#        v.to_csv(text_stream, index=False)\n",
    "#        zf.writestr(fileout, text_stream.getvalue())\n",
    "#    for k, v in dict_rolling_flow_weighted.items():\n",
    "#        fileout = dir_flow_weighted_rolling + '/' + k\n",
    "#        text_stream = StringIO();\n",
    "#        v.to_csv(text_stream, index=False)\n",
    "#        zf.writestr(fileout, text_stream.getvalue())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee98484-2c9e-4768-ad92-c1dd039787d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835d5f10-0c92-4468-b2fa-ae68445dbb91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
