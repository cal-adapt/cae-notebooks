{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1955a258-7843-41bd-8659-8043b35c65cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from io import StringIO\n",
    "import json\n",
    "import os\n",
    "import tempfile\n",
    "import zipfile\n",
    "\n",
    "import pandas as pd\n",
    "import intake\n",
    "from shapely.geometry import Point, Polygon\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import dask\n",
    "import panel as pn\n",
    "from dask.distributed import progress\n",
    "from dask.distributed import Client\n",
    "from climakitae.cluster import Cluster\n",
    "\n",
    "pn.extension()\n",
    "\n",
    "xr.set_options(keep_attrs=True)\n",
    "dask.config.set({\"array.slicing.split_large_chunks\": True})\n",
    "\n",
    "\n",
    "#Load dask Area for faster computing. \n",
    "#Note, this will take awhile but in long run processing should be faster when compute is called.\n",
    "cluster = Cluster()\n",
    "#cluster.adapt(minimum=0, maximum=16)\n",
    "cluster.adapt(minimum=0, maximum=30)\n",
    "client = cluster.get_client()\n",
    "\n",
    "#Get client link.\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7da6ff9-7c5b-43ce-b8f6-eda1df54a71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VARIABLES\n",
    "#Use these cordinates to clip around the watershed of interest.\n",
    "# latitude = [34.775317,42.432494]\n",
    "# longitude = [-123.097421,-117.980799]\n",
    "bbox = {\n",
    "    \"maxy\": 42.432494,\n",
    "    \"miny\": 34.775317,\n",
    "    \"minx\": -123.097421,\n",
    "    \"maxx\": -117.980799,\n",
    "}\n",
    "\n",
    "\n",
    "# run_list_path = \"data/GCM_Run_List_All.csv\"\n",
    "# file_zip = \"GCM_All.zip\"\n",
    "\n",
    "#run_list_path = \"data/GCM_Run_List_ACCESS-CM2_4.csv\"\n",
    "#file_zip = \"GCM_Test_4.zip\"\n",
    "\n",
    "run_list_path = \"data/GCM_Run_List_1-43.csv\"\n",
    "file_zip = \"GCM_1-43.zip\"\n",
    "\n",
    "run_list_path = \"data/GCM_Run_List_44-82.csv\"\n",
    "file_zip = \"GCM_44-82.zip\"\n",
    "\n",
    "run_list_path = \"data/GCM_Run_List_83-117.csv\"\n",
    "file_zip = \"GCM_83-117.zip\"\n",
    "\n",
    "\n",
    "basin_weights_csv = \"data/Basin_Weights.csv\"\n",
    "esm_datastore = \"https://cadcat.s3.amazonaws.com/cae-collection.json\"\n",
    "output_folder = \"outputs\"\n",
    "mask_path = \"mask/mask.npy\"\n",
    "dir_area_weighted = 'AREA_WEIGHTED_CENTRALVALLEY'\n",
    "dir_flow_weighted = 'FLOW_WEIGHTED_CENTRALVALLEY'\n",
    "dir_individual = 'INDIVIDUAL_BASIN_LOCA2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020916c8-c6a3-4b25-90b9-91add7b78e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "region_dict = {}\n",
    "with open(basin_weights_csv, \"r\") as csv_file:\n",
    "    reader = csv.DictReader(csv_file)\n",
    "    for row in reader:\n",
    "        region_dict[int(row[\"ID\"])] = {\n",
    "                            \"region_name\": row[\"Regions\"],\n",
    "                            \"flow_ratio\": float(row[\"Flow Ratio\"]),\n",
    "                            \"area_ratio\": float(row[\"Area Ratio\"]),\n",
    "                        }\n",
    "for k, v in region_dict.items():\n",
    "    print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bfb833-a0ed-4d94-ac16-a92df5614c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removed 99: {'region_name': 'Diversion from Echo Lake', 'weighting_factor': 0.0},\n",
    "# DEPRECATED\n",
    "# region_dict = {\n",
    "#     0:  {'region_name': 'Goose Lake', 'weighting_factor': 0.0},\n",
    "#     11: {'region_name': 'Westside Streams of SJR', 'weighting_factor': 0.002758980030193925},\n",
    "#     16: {'region_name': 'Other Rim Inflows of Sac', 'weighting_factor': 0.06520559638738632},\n",
    "#     9:  {'region_name': 'Eastside Streams of Delta', 'weighting_factor': 0.047054700553417206},\n",
    "#     17: {'region_name': 'Other Rim Inflows of SJR', 'weighting_factor': 0.008974390104413033},\n",
    "#     5:  {'region_name': 'Upper Stanislaus River', 'weighting_factor': 0.03673909977078438},\n",
    "#     8:  {'region_name': 'Lake Millerton', 'weighting_factor': 0.05571430176496506},\n",
    "#     10: {'region_name': 'Westside Streams of Sac', 'weighting_factor': 0.0789882019162178},\n",
    "#     12: {'region_name': 'Valley Floor of Sac', 'weighting_factor': 0.06745839864015579},\n",
    "#     18: {'region_name': 'Lower Yuba-Bear Rim Inflow', 'weighting_factor': 0.018660200759768486},\n",
    "#     14: {'region_name': 'Tulare Basin', 'weighting_factor': 0.0},\n",
    "#     1:  {'region_name': 'Lake Shasta', 'weighting_factor': 0.1778690069913864},\n",
    "#     15: {'region_name': 'Lake Trinity', 'weighting_factor': 0.04051230102777481},\n",
    "#     2:  {'region_name': 'Upper Feather River', 'weighting_factor': 0.13809999823570251},\n",
    "#     13: {'region_name': 'Valley Floor of SJR', 'weighting_factor': 0.008356500416994095},\n",
    "#     3:  {'region_name': 'Upper Yuba River', 'weighting_factor': 0.07005230337381363},\n",
    "#     19: {'region_name': 'Delta', 'weighting_factor': 0.026663200929760933},\n",
    "#     4:  {'region_name': 'Upper American River', 'weighting_factor': 0.08627369999885559},\n",
    "#     6:  {'region_name': 'Upper Tuolumne River', 'weighting_factor': 0.05876690149307251},\n",
    "#     7:  {'region_name': 'Upper Merced River', 'weighting_factor': 0.030512800440192223}\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5e0ccc-88e4-488c-94fa-8b62c0b7bdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_params(run_list_path):\n",
    "    \"\"\"Read each set of model parameters into dictionary from csv file.\n",
    "    \n",
    "    Return list of dictionaries.\n",
    "    \"\"\"\n",
    "    model_params = []\n",
    "    with open(run_list_path, \"r\") as src:\n",
    "        d = csv.DictReader(src)\n",
    "        for row in d:\n",
    "            model_params.append(row)\n",
    "    return model_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203b349d-4c0d-408d-bd45-4b7a2f5d9938",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(esm_datastore, model_params):\n",
    "    \"\"\"Return xarray.DataSet from model parameters.\"\"\"\n",
    "    # Open catalog of available data sets using intake-esm package\n",
    "    cat = intake.open_esm_datastore(esm_datastore)\n",
    "    cat_item = cat.search(\n",
    "        activity_id=model_params[\"activity_id\"],\n",
    "        institution_id=model_params[\"institution_id\"],\n",
    "        table_id=model_params[\"table_id\"], \n",
    "        variable_id=['pr','tasmax','tasmin'],\n",
    "        experiment_id=model_params[\"experiment_id\"],\n",
    "        grid_label=model_params[\"grid_label\"],\n",
    "        member_id=model_params[\"member_id\"],\n",
    "        source_id=model_params[\"source_id\"],  \n",
    "    )\n",
    "    \n",
    "    # Add catalog item to dataset dict\n",
    "    data_dict = cat_item.to_dataset_dict(\n",
    "      #  xarray_open_kwargs={'consolidated': True},\n",
    "        storage_options={'anon': True}\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Construct dataset key to retrieve from the dictionary\n",
    "    key = \"{}.{}.{}.{}.{}.{}\".format(\n",
    "            model_params['activity_id'],\n",
    "            model_params['institution_id'],\n",
    "            model_params['source_id'],\n",
    "            model_params['experiment_id'],\n",
    "            model_params['table_id'],\n",
    "            model_params['grid_label'],)\n",
    "    \n",
    "    # Slice the dataset to the input time window.\n",
    "    \n",
    "    ds = slice_by_time_years_dataset(data_dict[key],model_params['start_year'],model_params['end_year'])\n",
    "    ds = convert_daily_to_monthly_dataset(ds)\n",
    "    # Trim trim down to cordinates.\n",
    "    #ds = trim_to_lat_lon_dataset(ds)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d990d3-0d54-470d-be7a-aa828ef0da5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_mask_to_dataset(mask_path, ds):\n",
    "    # attach the mask\n",
    "    with open(mask_path, 'rb') as f:\n",
    "        mask = np.load(f, allow_pickle=True)\n",
    "    ds.coords['mask'] = (('lat', 'lon'), mask)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8381dd-f218-4b17-a1fd-0d22e562b9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_daily_to_monthly_dataset(ds):\n",
    "    #Convert our daily values to monthly.  Precip is the accumulated and temperature is the average.\n",
    "     #86400 x kg/m2/s = daily value (mm)  Check this!!!!\n",
    "    ds['pr'] = ds.pr * 86400\n",
    "    ds.pr.attrs[\"units\"] = 'mm/day' \n",
    "    ds_precip = ds['pr'].resample(time=\"M\").sum()\n",
    "    ds_precip.attrs[\"units\"] = 'mm/mon' \n",
    "    ds_temp = ds[['tasmin','tasmax']].resample(time=\"M\").mean()\n",
    "    \n",
    "    #Change the temp to C\n",
    "    ds_temp = ds_temp[['tasmin','tasmax']] - 273.15\n",
    "    ds_temp.tasmin.attrs[\"units\"]  = 'degC'\n",
    "    ds_temp.tasmax.attrs[\"units\"]  = 'degC'\n",
    "    \n",
    "    \n",
    "    #Merge the dataset back into on dataset.\n",
    "    ds= xr.merge([ds_precip,ds_temp])\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91935c5-dff2-48ae-8858-111927c9cf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_by_time_years_dataset(ds,startyear,endyear):\n",
    "    # Time slice\n",
    "    ds = ds.sel(\n",
    "        time=slice(str(startyear), str(endyear))\n",
    "        )\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3901ea78-2ae9-4335-b386-91c0124bbefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_dataset_to_bbox(ds, bbox):\n",
    "    #This needs to be done for the cliping.\n",
    "    ds.rio.set_spatial_dims(x_dim=\"lon\", y_dim=\"lat\", inplace=True)\n",
    "    ds.rio.write_crs(\"EPSG:4326\", inplace=True)\n",
    "\n",
    "    #Get the subset of data for watershed.\n",
    "    ds = ds.rio.clip_box(\n",
    "        minx=bbox[\"minx\"],\n",
    "        miny=bbox[\"miny\"],\n",
    "        maxx=bbox[\"maxx\"],\n",
    "        maxy=bbox[\"maxy\"],\n",
    "    )\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405fe5e9-361c-4b9f-8561-1a4d7478a1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_file_name_monthly(model_params,end_part):\n",
    "    return '%s_%s_%s_%s.csv'%(model_params['source_id'],model_params['experiment_id'],model_params['member_id'],end_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf74e9e-2b0d-4344-af9f-c8bafc6859a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_with_mask(esm_datastore_in, model_params_in, mask_path,bbox):\n",
    "    #Loads current dataset\n",
    "    ds = get_dataset(esm_datastore, model_params)\n",
    "    ds = add_mask_to_dataset(mask_path, ds)\n",
    "    ds = trim_dataset_to_bbox(ds, bbox)\n",
    "    return ds\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7126ba9-9e9e-4f6b-9b95-38e264e2631a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_one(esm_datastore_in, model_params_in):\n",
    "    ds = load_dataset_with_mask(esm_datastore, model_params)\n",
    "    #for each subasin:\n",
    "        #Process outputs without wieghts and save to CSV.\n",
    "        #Process 30 year rolling average and save to CSV.\n",
    "    #Process with weights. This should be 1 value for all subasins.\n",
    "    #Process 30 year rolling average. \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0d9d89-1c25-48e6-a52e-2942252a9644",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_map_mask(id_region,ds,use_full_mask = False):\n",
    "    # Returns a dataframe for the id_region. Set id_region = -1 and use_full_mask to get the entire domain. \n",
    "       # print(\"\\tSpatial mask...\")\n",
    "        if use_full_mask:\n",
    "            map_data = ds.where(ds.mask != id_region)\n",
    "        else:\n",
    "            map_data = ds.where(ds.mask == id_region)\n",
    "        \n",
    "        #print(\"\\tCalculating precip...\")\n",
    "        results_precip = map_data.pr.mean(['lat','lon'],skipna=True)\n",
    "        results_precip.attrs[\"units\"]  = 'mm/mon'\n",
    "        \n",
    "        #print(\"\\tCalculating tasmin...\")\n",
    "        results_tasmin = map_data.tasmin.mean(['lat','lon'])\n",
    "        results_tasmin.attrs[\"units\"]  = 'degC'\n",
    "        \n",
    "        #print(\"\\tCalculating tasmax...\")\n",
    "        results_tasmax = map_data.tasmax.mean(['lat','lon'])\n",
    "        results_tasmax.attrs[\"units\"]  = 'degC'\n",
    "        \n",
    "        #print(\"\\tMerging...\")\n",
    "        ds_all= xr.merge([results_precip,results_tasmax,results_tasmin])\n",
    "        \n",
    "        #print(\"\\tConverting to pandas dataframe...\")\n",
    "        df = ds_all.to_pandas()\n",
    "\n",
    "        df.drop('spatial_ref',axis=1, inplace=True)\n",
    "\n",
    "        df['Year'] = df.index.strftime('%Y')\n",
    "        df['Month'] = df.index.month\n",
    "        df['Tave (degC)'] = df[['tasmax','tasmin']].mean(axis=1)\n",
    "        df.rename({'pr': 'Pr (mm)','tasmax':'Tasmax (degC)','tasmin' : 'Tasmin (degC)'}, axis=1,inplace=True)\n",
    "        \n",
    "        df_r = df.iloc[:,[3,4,0,1,2,5]]\n",
    "        df_n = df_r.reset_index()\n",
    "        # don't drop the time to writing the output df.  We need this for the rolling average.\n",
    "        #df_n.drop('time' , axis=1, inplace=True)\n",
    "        return df_n\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02c5ffe-4985-4495-8f97-932c877ef003",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weighted_dataframe(df_in, weight):\n",
    "    # Computes the weighted from from df_in and returns the resulting dataframe. \n",
    "    df_in['Pr (mm) Weighted'] = df_in['Pr (mm)'] * weighting_factor\n",
    "    df_in['Tasmax (degC) Weighted'] = df_in['Tasmax (degC)'] * weighting_factor\n",
    "    df_in['Tasmin (degC) Weighted'] = df_in['Tasmin (degC)'] * weighting_factor\n",
    "    df_in['Tave (degC) Weighted'] = df_in['Tave (degC)'] * weighting_factor\n",
    "    df_in = df_in.drop(['Pr (mm)', 'Tasmax (degC)', 'Tasmin (degC)', 'Tave (degC)'], axis=1)\n",
    "    return df_in\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fb4db7-db40-4955-9cd6-2a9270e98526",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sum_dataframes(df_in, df_to_add):\n",
    "    #Adds the weighted results from df_to_add to the df_in and returns the resulting dataframe.\n",
    "    df_in['Pr (mm) Weighted'] = df_in['Pr (mm) Weighted'].add(df_to_add['Pr (mm) Weighted'], fill_value=0)\n",
    "    df_in['Tasmax (degC) Weighted'] =  df_in['Tasmax (degC) Weighted'].add(df_to_add['Tasmax (degC) Weighted'], fill_value=0)\n",
    "    df_in['Tasmin (degC) Weighted'] =  df_in['Tasmin (degC) Weighted'].add(df_to_add['Tasmin (degC) Weighted'], fill_value=0)\n",
    "    df_in['Tave (degC) Weighted'] =  df_in['Tave (degC) Weighted'].add(df_to_add['Tave (degC) Weighted'], fill_value=0)\n",
    "    return df_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dee56f-bbfd-407d-af98-23b70feccd6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "501cd359-2f63-4723-ada5-9a759182adb2",
   "metadata": {},
   "source": [
    "The loop goes through all GCMs and writes the individual subbasin, the area weighted, and the flow weighted to results dictionaries with filname as the key and value equal to result dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0af3b2e-5cb6-495c-a786-7964323026bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# main loop\n",
    "all_model_params = get_model_params(run_list_path)\n",
    "\n",
    "results_dict = {}\n",
    "weighted_results_dict = {}\n",
    "non_weighted_basin_results_dict = {}\n",
    "for model_params in all_model_params:\n",
    "    #Add masking to the dataset.\n",
    "    ds = load_dataset_with_mask(esm_datastore, model_params, mask_path,bbox)\n",
    "   \n",
    "    #Force load the dataset.\n",
    "    ds = ds.compute()\n",
    "        \n",
    "    df_w = None\n",
    "    df_a = None\n",
    "    for id_region, v in region_dict.items():\n",
    "      \n",
    "        # Get this regions results\n",
    "        df_n = get_df_map_mask(id_region,ds)\n",
    "        output_filename = get_output_file_name_monthly(model_params, '%s-19'%'{:02d}'.format(id_region))\n",
    "        #print('Adding.. %s'%output_filename)\n",
    "        df_out = df_n.drop('time' , axis=1)\n",
    "        results_dict[output_filename] = df_out\n",
    "        \n",
    "        # Get Area Weighted dataframe\n",
    "        weighting_factor = v['area_ratio']\n",
    "        df_weighted_a = get_weighted_dataframe(df_n.copy(deep=True),weighting_factor)\n",
    "        if df_a is None:\n",
    "            df_a = df_weighted_a.copy(deep=True)\n",
    "        else:\n",
    "            df_a = get_sum_dataframes(df_a,df_weighted_a)\n",
    "            \n",
    "        # Get Flow Weighted results \n",
    "        weighting_factor = v['flow_ratio']\n",
    "        df_weighted = get_weighted_dataframe(df_n.copy(deep=True),weighting_factor)\n",
    "        if df_w is None:\n",
    "            df_w = df_weighted.copy(deep=True)\n",
    "        else:\n",
    "            df_w = get_sum_dataframes(df_w,df_weighted)\n",
    "    print('Processed %s...'%get_output_file_name_monthly(model_params,'').replace('.csv',''))       \n",
    "    #Add weighted dataframes to output.\n",
    "    output_filename = get_output_file_name_monthly(model_params, \"19FlowWeighted\")\n",
    "    weighted_results_dict[output_filename] = df_w\n",
    "    output_filename = get_output_file_name_monthly(model_params, \"19AreaWeighted\")\n",
    "    non_weighted_basin_results_dict[output_filename] = df_a\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d576b63b-1e7b-4c13-b7a3-8ba6c7f31dd3",
   "metadata": {},
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "weighted_results_dict\n",
    "\n",
    "dftest = weighted_results_dict['ACCESS-CM2_historical_r1i1p1f1_19FlowWeighted.csv'].copy(deep=True)\n",
    "dftest['Rolling_pr'] = dftest['Pr (mm) Weighted'].rolling(360).mean()\n",
    "\n",
    "weighted_results_dict['ACCESS-CM2_historical_r1i1p1f1_19FlowWeighted.csv']  = dftest\n",
    "plt.plot(dftest['time'], dftest['Pr (mm) Weighted'])\n",
    "plt.plot(dftest['time'], dftest['Rolling_pr'])\n",
    "plt.title('Data With Rolling Average')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a128fe-3ead-4a52-a601-e6e637c5b557",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write to ouput.\n",
    "zip_path = os.path.join(output_folder, file_zip)\n",
    "\n",
    "with zipfile.ZipFile(zip_path, mode=\"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
    "    for k, v in results_dict.items():\n",
    "        text_stream = StringIO();\n",
    "        v.to_csv(text_stream, index=False)\n",
    "        fileout = dir_individual + '/' + k\n",
    "        zf.writestr(fileout, text_stream.getvalue())\n",
    "    for k, v in weighted_results_dict.items():\n",
    "        fileout = dir_flow_weighted + '/' + k\n",
    "        fileout_raw = dir_flow_weighted + '/Raw/' + k\n",
    "        text_stream = StringIO();\n",
    "        v.to_csv(text_stream, index=False)\n",
    "        zf.writestr(fileout_raw, text_stream.getvalue())\n",
    "        v.drop('time' , axis=1, inplace=True)\n",
    "        text_stream = StringIO();\n",
    "        v.to_csv(text_stream, index=False)\n",
    "        zf.writestr(fileout, text_stream.getvalue())\n",
    "    for k, v in non_weighted_basin_results_dict.items():\n",
    "        fileout = dir_area_weighted + '/' + k\n",
    "        fileout_raw = dir_area_weighted + '/Raw/' + k\n",
    "        text_stream = StringIO();\n",
    "        v.to_csv(text_stream, index=False)\n",
    "        zf.writestr(fileout_raw, text_stream.getvalue())\n",
    "        v.drop('time' , axis=1, inplace=True)\n",
    "        text_stream = StringIO();\n",
    "        v.to_csv(text_stream, index=False)\n",
    "        zf.writestr(fileout, text_stream.getvalue())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee98484-2c9e-4768-ad92-c1dd039787d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8bb42a-5fef-45a5-9d0e-ecdfec9b856f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = results_dict['ACCESS-CM2_historical_r1i1p1f1_1.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc8cf49-ccb9-4484-9763-93ac9b2ba416",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100a0510-5b3d-4e78-beb4-a51b654d4e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(weighted_results_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ba615d-79ed-4029-960e-c8fb7c6f1021",
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_df_list = []\n",
    "for k, v in weighted_results_dict.items():\n",
    "    weighted_df_list.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd82d2a6-fee4-452a-97a1-a1b4144f3769",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd89ea4-22de-4130-aac2-7ae9a5784fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_result_df = reduce(lambda a, b: a.add(b, fill_value=0), weighted_df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fec14d-ff8d-4018-a78b-85c76e51a36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815edd0e-5dfe-40fb-9f0f-d311c6ba857c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in results_dict.items():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6252042-7a24-4722-af7f-53d919cc4ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write to ouput.\n",
    "zip_path = os.path.join(output_folder, \"test5.zip\")\n",
    "\n",
    "#output_filename = get_output_file_name_monthly(model_params,id_subbasin)\n",
    "\n",
    "with zipfile.ZipFile(zip_path, mode=\"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
    "    for k, v in results_dict.items():\n",
    "        text_stream = StringIO();\n",
    "        v.to_csv(text_stream, index=False)\n",
    "        \n",
    "        zf.writestr(k, text_stream.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fbb926-e23d-45b9-934c-5e552af9eda6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
