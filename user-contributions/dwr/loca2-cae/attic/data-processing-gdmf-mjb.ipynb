{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1955a258-7843-41bd-8659-8043b35c65cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import datetime\n",
    "from io import StringIO\n",
    "import json\n",
    "import os\n",
    "import tempfile\n",
    "import zipfile\n",
    "\n",
    "import pandas as pd\n",
    "import intake\n",
    "from shapely.geometry import Point, Polygon\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import dask\n",
    "import panel as pn\n",
    "pn.extension()\n",
    "\n",
    "xr.set_options(keep_attrs=True)\n",
    "dask.config.set({\"array.slicing.split_large_chunks\": True})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a525725-21cb-4604-905a-cd78bdfa5f57",
   "metadata": {},
   "source": [
    "Load dask Area for faster computing.  Note, this will take awhile but in long run processing should be faster when compute is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738d23d7-ef5e-4b8f-a634-6f719917a13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import progress\n",
    "from dask.distributed import Client\n",
    "from climakitae.cluster import Cluster\n",
    "cluster = Cluster()\n",
    "cluster.adapt(minimum=0, maximum=16)\n",
    "client = cluster.get_client()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4730f13c-d06d-4813-bb91-18bcca403be3",
   "metadata": {},
   "source": [
    "Get client link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b2cccf-c225-4f18-a7de-b0c185effb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7da6ff9-7c5b-43ce-b8f6-eda1df54a71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VARIABLES\n",
    "#Use these cordinates to clip around the watershed of interest.\n",
    "# latitude = [34.775317,42.432494]\n",
    "# longitude = [-123.097421,-117.980799]\n",
    "bbox = {\n",
    "    \"maxy\": 42.432494,\n",
    "    \"miny\": 34.775317,\n",
    "    \"minx\": -123.097421,\n",
    "    \"maxx\": -117.980799,\n",
    "}\n",
    "run_list_path = \"data/GCM_Run_List_All.csv\"\n",
    "esm_datastore = \"https://cadcat.s3.amazonaws.com/cae-collection.json\"\n",
    "output_folder = \"outputs\"\n",
    "mask_path = \"mask/mask.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bfb833-a0ed-4d94-ac16-a92df5614c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "region_dict = {\n",
    "    0:  {'region_name': 'Goose Lake', 'weighting_factor': 0.0},\n",
    "    11: {'region_name': 'Westside Streams of SJR', 'weighting_factor': 0.002758980030193925},\n",
    "    16: {'region_name': 'Other Rim Inflows of Sac', 'weighting_factor': 0.06520559638738632},\n",
    "    9:  {'region_name': 'Eastside Streams of Delta', 'weighting_factor': 0.047054700553417206},\n",
    "    17: {'region_name': 'Other Rim Inflows of SJR', 'weighting_factor': 0.008974390104413033},\n",
    "    5:  {'region_name': 'Upper Stanislaus River', 'weighting_factor': 0.03673909977078438},\n",
    "    8:  {'region_name': 'Lake Millerton', 'weighting_factor': 0.05571430176496506},\n",
    "    10: {'region_name': 'Westside Streams of Sac', 'weighting_factor': 0.0789882019162178},\n",
    "    12: {'region_name': 'Valley Floor of Sac', 'weighting_factor': 0.06745839864015579},\n",
    "    18: {'region_name': 'Lower Yuba-Bear Rim Inflow', 'weighting_factor': 0.018660200759768486},\n",
    "    14: {'region_name': 'Tulare Basin', 'weighting_factor': 0.0},\n",
    "    1:  {'region_name': 'Lake Shasta', 'weighting_factor': 0.1778690069913864},\n",
    "    15: {'region_name': 'Lake Trinity', 'weighting_factor': 0.04051230102777481},\n",
    "    2:  {'region_name': 'Upper Feather River', 'weighting_factor': 0.13809999823570251},\n",
    "    13: {'region_name': 'Valley Floor of SJR', 'weighting_factor': 0.008356500416994095},\n",
    "    3:  {'region_name': 'Upper Yuba River', 'weighting_factor': 0.07005230337381363},\n",
    "    19: {'region_name': 'Delta', 'weighting_factor': 0.026663200929760933},\n",
    "    4:  {'region_name': 'Upper American River', 'weighting_factor': 0.08627369999885559},\n",
    "    # 99: {'region_name': 'Diversion from Echo Lake', 'weighting_factor': 0.0},\n",
    "    6:  {'region_name': 'Upper Tuolumne River', 'weighting_factor': 0.05876690149307251},\n",
    "    7:  {'region_name': 'Upper Merced River', 'weighting_factor': 0.030512800440192223}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5e0ccc-88e4-488c-94fa-8b62c0b7bdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_params(run_list_path):\n",
    "    \"\"\"Read each set of model parameters into dictionary from csv file.\n",
    "    \n",
    "    Return list of dictionaries.\n",
    "    \"\"\"\n",
    "    model_params = []\n",
    "    with open(run_list_path, \"r\") as src:\n",
    "        d = csv.DictReader(src)\n",
    "        for row in d:\n",
    "            model_params.append(row)\n",
    "    return model_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203b349d-4c0d-408d-bd45-4b7a2f5d9938",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(esm_datastore, model_params):\n",
    "    \"\"\"Return xarray.DataSet from model parameters.\"\"\"\n",
    "    # Open catalog of available data sets using intake-esm package\n",
    "    cat = intake.open_esm_datastore(esm_datastore)\n",
    "    cat_item = cat.search(\n",
    "        activity_id=model_params[\"activity_id\"],\n",
    "        institution_id=model_params[\"institution_id\"],\n",
    "        table_id=model_params[\"table_id\"], \n",
    "        variable_id=['pr','tasmax','tasmin'],\n",
    "        experiment_id=model_params[\"experiment_id\"],\n",
    "        grid_label=model_params[\"grid_label\"],\n",
    "        member_id=model_params[\"member_id\"],\n",
    "        source_id=model_params[\"source_id\"],  \n",
    "    )\n",
    "    # Add catalog item to dataset dict\n",
    "    data_dict = cat_item.to_dataset_dict(\n",
    "        xarray_open_kwargs={'consolidated': True},\n",
    "        storage_options={'anon': True}\n",
    "    )\n",
    "    # Construct dataset key to retrieve from the dictionary\n",
    "    key = \"{}.{}.{}.{}.{}.{}\".format(\n",
    "            model_params['activity_id'],\n",
    "            model_params['institution_id'],\n",
    "            model_params['source_id'],\n",
    "            model_params['experiment_id'],\n",
    "            model_params['table_id'],\n",
    "            model_params['grid_label'],)\n",
    "    \n",
    "    # Slice the dataset to the input time window.\n",
    "    \n",
    "    ds = slice_by_time_years_dataset(data_dict[key],model_params['start_year'],model_params['end_year'])\n",
    "    ds = convert_daily_to_monthly_dataset(ds)\n",
    "    # Trim trim down to cordinates.\n",
    "    #ds = trim_to_lat_lon_dataset(ds)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d990d3-0d54-470d-be7a-aa828ef0da5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_mask_to_dataset(mask_path, ds):\n",
    "    # attach the mask\n",
    "    with open(mask_path, 'rb') as f:\n",
    "        mask = np.load(f, allow_pickle=True)\n",
    "    ds.coords['mask'] = (('lat', 'lon'), mask)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8381dd-f218-4b17-a1fd-0d22e562b9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_daily_to_monthly_dataset(ds):\n",
    "    #Convert our daily values to monthly.  Precip is the accumulated and temperature is the average.\n",
    "     #86400 x kg/m2/s = daily value (mm)  Check this!!!!\n",
    "    ds['pr'] = ds.pr * 86400\n",
    "    ds.pr.attrs[\"units\"] = 'mm/day' \n",
    "    ds_precip = ds['pr'].resample(time=\"M\").sum()\n",
    "    ds_precip.attrs[\"units\"] = 'mm/mon' \n",
    "    ds_temp = ds[['tasmin','tasmax']].resample(time=\"M\").mean()\n",
    "    \n",
    "    #Change the temp to C\n",
    "    ds_temp = ds_temp[['tasmin','tasmax']] - 273.15\n",
    "    ds_temp.tasmin.attrs[\"units\"]  = 'degC'\n",
    "    ds_temp.tasmax.attrs[\"units\"]  = 'degC'\n",
    "    \n",
    "    \n",
    "    #Merge the dataset back into on dataset.\n",
    "    ds= xr.merge([ds_precip,ds_temp])\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91935c5-dff2-48ae-8858-111927c9cf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_by_time_years_dataset(ds,startyear,endyear):\n",
    "    # Time slice\n",
    "    ds = ds.sel(\n",
    "        time=slice(str(startyear), str(endyear))\n",
    "        )\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3901ea78-2ae9-4335-b386-91c0124bbefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_dataset_to_bbox(ds, bbox):\n",
    "    #This needs to be done for the cliping.\n",
    "    ds.rio.set_spatial_dims(x_dim=\"lon\", y_dim=\"lat\", inplace=True)\n",
    "    ds.rio.write_crs(\"EPSG:4326\", inplace=True)\n",
    "    \n",
    "    #Get the subset of data for watershed.\n",
    "    ds = ds.rio.clip_box(\n",
    "        minx=bbox[\"minx\"],\n",
    "        miny=bbox[\"miny\"],\n",
    "        maxx=bbox[\"maxx\"],\n",
    "        maxy=bbox[\"maxy\"],\n",
    "        crs=4326,\n",
    "    )\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405fe5e9-361c-4b9f-8561-1a4d7478a1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_file_name_monthly(model_params,end_part):\n",
    "    return '%s_%s_%s_%s.csv'%(model_params['source_id'],model_params['experiment_id'],model_params['member_id'],end_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf74e9e-2b0d-4344-af9f-c8bafc6859a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_with_mask(esm_datastore_in, model_params_in):\n",
    "    #Loads current dataset\n",
    "    ds = get_dataset(esm_datastore_in, model_params_in)\n",
    "    ds = add_mask_to_dataset(ds)\n",
    "    ds = trim_dataset_to_bbox(ds, bbox)\n",
    "    #ds = ds.compute()\n",
    "    return ds\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09434bfd-997f-456a-af2d-50353c169f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5b6160-547b-4f07-8a57-4d2fd182221c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main loop\n",
    "all_model_params = get_model_params(run_list_path)\n",
    "\n",
    "for model_params in all_model_params:\n",
    "    print(datetime.datetime.now())\n",
    "\n",
    "    ds = get_dataset(esm_datastore, model_params)\n",
    "    ds = add_mask_to_dataset(mask_path, ds)\n",
    "    ds = trim_dataset_to_bbox(ds, bbox)\n",
    "    ds = ds.compute()\n",
    "    \n",
    "    results_dict = {}\n",
    "    weighted_results_dict = {}\n",
    "    \n",
    "    df_w = None\n",
    "    \n",
    "    for id_region, v in region_dict.items():\n",
    "        print(id_region)\n",
    "        \n",
    "        # print(\"\\tSpatial mask...\")\n",
    "        map_data = ds.where(ds.mask == id_region)\n",
    "        \n",
    "        # print(\"\\tCalculating precip...\")\n",
    "        results_precip = map_data.pr.mean(['lat','lon'])\n",
    "        results_precip.attrs[\"units\"]  = 'mm/mon'\n",
    "        \n",
    "        # print(\"\\tCalculating tasmin...\")\n",
    "        results_tasmin = map_data.tasmin.mean(['lat','lon'])\n",
    "        results_tasmin.attrs[\"units\"]  = 'degC'\n",
    "        \n",
    "        # print(\"\\tCalculating tasmax...\")\n",
    "        results_tasmax = map_data.tasmax.mean(['lat','lon'])\n",
    "        results_tasmax.attrs[\"units\"]  = 'degC'\n",
    "        \n",
    "        # print(\"\\tMerging...\")\n",
    "        ds_all= xr.merge([results_precip,results_tasmax,results_tasmin])\n",
    "        \n",
    "        # print(\"\\tConverting to pandas dataframe...\")\n",
    "        df = ds_all.to_pandas()\n",
    "\n",
    "        df.drop('spatial_ref',axis=1, inplace=True)\n",
    "\n",
    "        df['Year'] = df.index.strftime('%Y')\n",
    "        df['Month'] = df.index.strftime('%b')\n",
    "        df['Tave (degC)'] = df[['tasmax','tasmin']].mean(axis=1)\n",
    "        df.rename({'pr': 'Pr (mm)','tasmax':'Tasmax (degC)','tasmin' : 'Tasmin (degC)'}, axis=1,inplace=True)\n",
    "        \n",
    "        df_r = df.iloc[:,[3,4,0,1,2,5]]\n",
    "        df_n = df_r.reset_index()\n",
    "        df_n.drop('time' , axis=1, inplace=True)\n",
    "        \n",
    "        output_filename = get_output_file_name_monthly(model_params, id_region)\n",
    "        results_dict[output_filename] = df_n\n",
    "        \n",
    "        # Weighted results\n",
    "        weighting_factor = v['weighting_factor']\n",
    "        \n",
    "        df_weighted = df_n.copy(deep=True)\n",
    "        df_weighted['Pr (mm) Weighted'] = df_weighted['Pr (mm)'] * weighting_factor\n",
    "        df_weighted['Tasmax (degC) Weighted'] = df_weighted['Tasmax (degC)'] * weighting_factor\n",
    "        df_weighted['Tasmin (degC) Weighted'] = df_weighted['Tasmin (degC)'] * weighting_factor\n",
    "        df_weighted['Tave (degC) Weighted'] = df_weighted['Tave (degC)'] * weighting_factor\n",
    "        \n",
    "        if \"dates\" not in weighted_results_dict:\n",
    "            df_dates = df_weighted[['Year', 'Month']]\n",
    "            weighted_results_dict[\"dates\"] = df_dates\n",
    "        \n",
    "        df_weighted = df_weighted.drop(['Year', 'Month', 'Pr (mm)', 'Tasmax (degC)', 'Tasmin (degC)', 'Tave (degC)'], axis=1)\n",
    "        weighted_results_dict[id_region]=df_weighted\n",
    "        \n",
    "    break\n",
    "print(datetime.datetime.now())    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5373c4-f29b-46f9-95cf-968a784df056",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = results_dict['ACCESS-CM2_historical_r1i1p1f1_1.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc8cf49-ccb9-4484-9763-93ac9b2ba416",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100a0510-5b3d-4e78-beb4-a51b654d4e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(weighted_results_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ba615d-79ed-4029-960e-c8fb7c6f1021",
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_df_list = []\n",
    "for k, v in weighted_results_dict.items():\n",
    "    weighted_df_list.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd82d2a6-fee4-452a-97a1-a1b4144f3769",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd89ea4-22de-4130-aac2-7ae9a5784fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_result_df = reduce(lambda a, b: a.add(b, fill_value=0), weighted_df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fec14d-ff8d-4018-a78b-85c76e51a36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815edd0e-5dfe-40fb-9f0f-d311c6ba857c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in results_dict.items():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6252042-7a24-4722-af7f-53d919cc4ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This works\n",
    "# text_stream = StringIO();\n",
    "# df_n.to_csv(text_stream, index=False)\n",
    "\n",
    "zip_path = os.path.join(output_folder, \"test4.zip\")\n",
    "\n",
    "#output_filename = get_output_file_name_monthly(model_params,id_subbasin)\n",
    "\n",
    "with zipfile.ZipFile(zip_path, mode=\"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
    "    for k, v in results_dict.items():\n",
    "        text_stream = StringIO();\n",
    "        v.to_csv(text_stream, index=False)\n",
    "        \n",
    "        zf.writestr(k, text_stream.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fbb926-e23d-45b9-934c-5e552af9eda6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
