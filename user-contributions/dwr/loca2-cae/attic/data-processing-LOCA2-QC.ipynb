{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d462e91-fd8f-4920-8b46-cf951501bc1a",
   "metadata": {},
   "source": [
    "Script description goes here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24222f6-cc79-4f55-ae6d-18fb87808478",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from io import StringIO\n",
    "import json\n",
    "import os\n",
    "import tempfile\n",
    "import zipfile\n",
    "\n",
    "import pandas as pd\n",
    "import intake\n",
    "from shapely.geometry import Point, Polygon\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import dask\n",
    "import panel as pn\n",
    "from dask.distributed import progress\n",
    "from dask.distributed import Client\n",
    "from climakitae.cluster import Cluster\n",
    "\n",
    "# VARIABLES\n",
    "#Use these cordinates to clip around the watershed of interest.\n",
    "# latitude = [34.775317,42.432494]\n",
    "# longitude = [-123.097421,-117.980799]\n",
    "bbox = {\n",
    "    \"maxy\": 42.432494,\n",
    "    \"miny\": 34.775317,\n",
    "    \"minx\": -123.097421,\n",
    "    \"maxx\": -117.980799,\n",
    "}\n",
    "\n",
    "\n",
    "# run_list_path = \"data/GCM_Run_List_All.csv\"\n",
    "# file_zip = \"GCM_All.zip\"\n",
    "\n",
    "#run_list_path = \"data/GCM_Run_List_ACCESS-CM2_4.csv\"\n",
    "#file_zip = \"GCM_Test_4.zip\"\n",
    "\n",
    "\n",
    "run_list_path = \"data/GCM_Run_List_1-43.csv\"\n",
    "file_zip = \"GCM_1-43.zip\"\n",
    "\n",
    "run_list_path = \"data/GCM_Run_List_44-82.csv\"\n",
    "file_zip = \"GCM_44-82.zip\"\n",
    "\n",
    "run_list_path = \"data/GCM_Run_List_83-117.csv\"\n",
    "file_zip = \"GCM_83-117.zip\"\n",
    "\n",
    "run_list_path = \"data/GCM_Run_List_QC_1Year.csv\"\n",
    "file_zip = \"GCM_QC.zip\"\n",
    "\n",
    "basin_weights_csv = \"data/Basin_Weights.csv\"\n",
    "esm_datastore = \"https://cadcat.s3.amazonaws.com/cae-collection.json\"\n",
    "output_folder = \"outputs\"\n",
    "mask_path = \"mask/mask.npy\"\n",
    "dir_area_weighted = 'AREA_WEIGHTED_CENTRALVALLEY'\n",
    "dir_flow_weighted = 'FLOW_WEIGHTED_CENTRALVALLEY'\n",
    "dir_individual = 'INDIVIDUAL_BASIN_LOCA2'\n",
    "dir_area_weighted_rolling = 'AREA_WEIGHTED_30_YEAR_ROLLING_AVE_CENTRALVALLEY'\n",
    "dir_flow_weighted_rolling = 'FLOW_WEIGHTED_30_YEAR_ROLLING_AVE_CENTRALVALLEY'\n",
    "\n",
    "pn.extension()\n",
    "xr.set_options(keep_attrs=True)\n",
    "dask.config.set({\"array.slicing.split_large_chunks\": True})\n",
    "#Load dask Area for faster computing. \n",
    "#Note, this will take awhile but in long run processing should be faster when compute is called.\n",
    "cluster = Cluster()\n",
    "#cluster.adapt(minimum=0, maximum=16)\n",
    "cluster.adapt(minimum=0, maximum=16)\n",
    "client = cluster.get_client()\n",
    "\n",
    "#Get client link.\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020916c8-c6a3-4b25-90b9-91add7b78e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "region_dict = {}\n",
    "with open(basin_weights_csv, \"r\") as csv_file:\n",
    "    reader = csv.DictReader(csv_file)\n",
    "    for row in reader:\n",
    "        region_dict[int(row[\"ID\"])] = {\n",
    "                            \"region_name\": row[\"Regions\"],\n",
    "                            \"flow_ratio\": float(row[\"Flow Ratio\"]),\n",
    "                            \"area_ratio\": float(row[\"Area Ratio\"]),\n",
    "                        }\n",
    "#for k, v in region_dict.items():\n",
    "#    print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5e0ccc-88e4-488c-94fa-8b62c0b7bdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_params(run_list_path):\n",
    "    \"\"\"Read each set of model parameters into dictionary from csv file.\n",
    "    \n",
    "    Return list of dictionaries.\n",
    "    \"\"\"\n",
    "    model_params = []\n",
    "    with open(run_list_path, \"r\") as src:\n",
    "        d = csv.DictReader(src)\n",
    "        for row in d:\n",
    "            model_params.append(row)\n",
    "    return model_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203b349d-4c0d-408d-bd45-4b7a2f5d9938",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(esm_datastore, model_params):\n",
    "    \"\"\"Return xarray.DataSet from model parameters.\"\"\"\n",
    "    # Open catalog of available data sets using intake-esm package\n",
    "    cat = intake.open_esm_datastore(esm_datastore)\n",
    "    cat_item = cat.search(\n",
    "        activity_id=model_params[\"activity_id\"],\n",
    "        institution_id=model_params[\"institution_id\"],\n",
    "        table_id=model_params[\"table_id\"], \n",
    "        variable_id=['pr','tasmax','tasmin'],\n",
    "        experiment_id=model_params[\"experiment_id\"],\n",
    "        grid_label=model_params[\"grid_label\"],\n",
    "        member_id=model_params[\"member_id\"],\n",
    "        source_id=model_params[\"source_id\"],  \n",
    "    )\n",
    "    \n",
    "    # Add catalog item to dataset dict\n",
    "    data_dict = cat_item.to_dataset_dict(\n",
    "      #  xarray_open_kwargs={'consolidated': True},\n",
    "        storage_options={'anon': True}\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Construct dataset key to retrieve from the dictionary\n",
    "    key = \"{}.{}.{}.{}.{}.{}\".format(\n",
    "            model_params['activity_id'],\n",
    "            model_params['institution_id'],\n",
    "            model_params['source_id'],\n",
    "            model_params['experiment_id'],\n",
    "            model_params['table_id'],\n",
    "            model_params['grid_label'],)\n",
    "    \n",
    "    # Slice the dataset to the input time window.\n",
    "    ds = slice_by_time_years_dataset(data_dict[key],model_params['start_year'],model_params['end_year'])\n",
    "    ds = convert_units_only_dataset(ds)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d990d3-0d54-470d-be7a-aa828ef0da5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_mask_to_dataset(mask_path, ds):\n",
    "    \"\"\" Attach the mask input dataset ds. \"\"\"\n",
    "    with open(mask_path, 'rb') as f:\n",
    "        mask = np.load(f, allow_pickle=True)\n",
    "    ds.coords['mask'] = (('lat', 'lon'), mask)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdf3e95-c8ad-4a3a-a2c1-062676bf940c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_units_only_dataset(ds):\n",
    "    \"\"\" Convert our daily values.  Precip is the accumulated and temperature is the average.\n",
    "        Precip is converted to mm from kg/m2/s: 86400 x kg/m2/s = daily value (mm).\n",
    "        Temperature is converts to C.  C = K - 273.15\n",
    "    \"\"\"\n",
    "    #Convert our daily values to monthly.  Precip is the accumulated and temperature is the average.\n",
    "    # 86400 x kg/m2/s = daily value (mm)\n",
    "    ds_precip = ds.pr * 86400\n",
    "    ds_precip.attrs[\"units\"] = 'mm/day' \n",
    "    \n",
    "    #Change the temp to C\n",
    "    ds_temp = ds[['tasmin','tasmax']] - 273.15\n",
    "    ds_temp.tasmin.attrs[\"units\"]  = 'degC'\n",
    "    ds_temp.tasmax.attrs[\"units\"]  = 'degC'\n",
    "    \n",
    "    \n",
    "    #Merge the dataset back into on dataset.\n",
    "    ds= xr.merge([ds_precip,ds_temp])\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a4127b-48d1-4858-b6a2-43f2e41a658b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_daily_to_monthly_dataset(ds):\n",
    "    \"\"\" Convert our daily values to monthly.  Precip is the accumulated and temperature is the average.\n",
    "        Precip is converted to mm from kg/m2/s: 86400 x kg/m2/s = daily value (mm).\n",
    "        Temperature is converts to C.  C = K - 273.15\n",
    "    \"\"\"\n",
    "    #Convert our daily values to monthly.  Precip is the accumulated and temperature is the average.\n",
    "    # 86400 x kg/m2/s = daily value (mm)\n",
    "    ds['pr'] = ds.pr * 86400\n",
    "    ds.pr.attrs[\"units\"] = 'mm/day' \n",
    "    ds_precip = ds['pr'].resample(time=\"M\").sum()\n",
    "    ds_precip.attrs[\"units\"] = 'mm/mon' \n",
    "    ds_temp = ds[['tasmin','tasmax']].resample(time=\"M\").mean()\n",
    "    \n",
    "    #Change the temp to C\n",
    "    ds_temp = ds_temp[['tasmin','tasmax']] - 273.15\n",
    "    ds_temp.tasmin.attrs[\"units\"]  = 'degC'\n",
    "    ds_temp.tasmax.attrs[\"units\"]  = 'degC'\n",
    "    \n",
    "    \n",
    "    #Merge the dataset back into on dataset.\n",
    "    ds= xr.merge([ds_precip,ds_temp])\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91935c5-dff2-48ae-8858-111927c9cf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_by_time_years_dataset(ds,startyear,endyear):\n",
    "    \"\"\" Slice the dataset to years of interest. \"\"\"\n",
    "    ds = ds.sel(\n",
    "        time=slice(str(startyear), str(endyear))\n",
    "        )\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3901ea78-2ae9-4335-b386-91c0124bbefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_dataset_to_bbox(ds, bbox):\n",
    "    \"\"\" Clip the dataset to a box. \"\"\"\n",
    "    \n",
    "    #This needs to be done for the cliping.\n",
    "    ds.rio.set_spatial_dims(x_dim=\"lon\", y_dim=\"lat\", inplace=True)\n",
    "    ds.rio.write_crs(\"EPSG:4326\", inplace=True)\n",
    "\n",
    "    #Get the subset of data for watershed.\n",
    "    ds = ds.rio.clip_box(\n",
    "        minx=bbox[\"minx\"],\n",
    "        miny=bbox[\"miny\"],\n",
    "        maxx=bbox[\"maxx\"],\n",
    "        maxy=bbox[\"maxy\"],\n",
    "    )\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405fe5e9-361c-4b9f-8561-1a4d7478a1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_file_name_monthly(model_params,end_part):\n",
    "    \"\"\" Format the output file name from model_params \"\"\"\n",
    "    return '%s_%s_%s_%s.csv'%(model_params['source_id'],model_params['experiment_id'],model_params['member_id'],end_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf74e9e-2b0d-4344-af9f-c8bafc6859a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_with_mask(esm_datastore_in, model_params_in, mask_path,bbox):\n",
    "    \"\"\" Loads the dataset, adds mask, and trims dataset to box arround area of interest\"\"\"\n",
    "    ds = get_dataset(esm_datastore, model_params)\n",
    "    ds = add_mask_to_dataset(mask_path, ds)\n",
    "    ds = trim_dataset_to_bbox(ds, bbox)\n",
    "    return ds\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0d9d89-1c25-48e6-a52e-2942252a9644",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_map_mask(id_region,ds,use_full_mask = False):\n",
    "    \"\"\" Returns a dataframe for the id_region. Set id_region = -1 and use_full_mask to get the entire domain. \"\"\" \n",
    "   # print(\"\\tSpatial mask...\")\n",
    "    if use_full_mask:\n",
    "        map_data = ds.where(ds.mask != id_region)\n",
    "    else:\n",
    "        map_data = ds.where(ds.mask == id_region)\n",
    "\n",
    "    #print(\"\\tCalculating precip...\")\n",
    "    results_precip = map_data.pr.mean(['lat','lon'],skipna=True)\n",
    "    results_precip.attrs[\"units\"]  = 'mm/day'\n",
    "\n",
    "    #print(\"\\tCalculating tasmin...\")\n",
    "    results_tasmin = map_data.tasmin.mean(['lat','lon'])\n",
    "    results_tasmin.attrs[\"units\"]  = 'degC'\n",
    "\n",
    "    #print(\"\\tCalculating tasmax...\")\n",
    "    results_tasmax = map_data.tasmax.mean(['lat','lon'])\n",
    "    results_tasmax.attrs[\"units\"]  = 'degC'\n",
    "\n",
    "    #print(\"\\tMerging...\")\n",
    "    ds_all= xr.merge([results_precip,results_tasmax,results_tasmin])\n",
    "\n",
    "    #print(\"\\tConverting to pandas dataframe...\")\n",
    "    df = ds_all.to_pandas()\n",
    "\n",
    "    df.drop('spatial_ref',axis=1, inplace=True)\n",
    "\n",
    "    df['Year'] = df.index.strftime('%Y')\n",
    "    df['Month'] = df.index.month\n",
    "    df['Tave (degC)'] = df[['tasmax','tasmin']].mean(axis=1)\n",
    "    df.rename({'pr': 'Pr (mm)','tasmax': 'Tasmax (degC)','tasmin' : 'Tasmin (degC)'}, axis=1,inplace=True)\n",
    "\n",
    "    df_r = df.iloc[:,[3,4,0,1,2,5]]\n",
    "    df_n = df_r.reset_index()\n",
    "    # don't drop the time to writing the output df.  We need this for the rolling average.\n",
    "    #df_n.drop('time' , axis=1, inplace=True)\n",
    "    return df_n\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02c5ffe-4985-4495-8f97-932c877ef003",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weighted_dataframe(df_in, weight):\n",
    "    \"\"\" Computes the weighted from from df_in and returns the resulting dataframe. \"\"\"\n",
    "    df_in['Pr (mm)'] = df_in['Pr (mm)'] * weighting_factor\n",
    "    df_in['Tasmax (degC)'] = df_in['Tasmax (degC)'] * weighting_factor\n",
    "    df_in['Tasmin (degC)'] = df_in['Tasmin (degC)'] * weighting_factor\n",
    "    df_in['Tave (degC)'] = df_in['Tave (degC)'] * weighting_factor\n",
    "    #df_in = df_in.drop(['Pr (mm)', 'Tasmax (degC)', 'Tasmin (degC)', 'Tave (degC)'], axis=1)\n",
    "    return df_in\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fb4db7-db40-4955-9cd6-2a9270e98526",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sum_dataframes(df_in, df_to_add):\n",
    "    \"\"\" Adds the weighted results from df_to_add to the df_in and returns the resulting dataframe. \"\"\"\n",
    "    df_in['Pr (mm)'] = df_in['Pr (mm)'].add(df_to_add['Pr (mm)'], fill_value=0)\n",
    "    df_in['Tasmax (degC)'] =  df_in['Tasmax (degC)'].add(df_to_add['Tasmax (degC)'], fill_value=0)\n",
    "    df_in['Tasmin (degC)'] =  df_in['Tasmin (degC)'].add(df_to_add['Tasmin (degC)'], fill_value=0)\n",
    "    df_in['Tave (degC)'] =  df_in['Tave (degC)'].add(df_to_add['Tave (degC)'], fill_value=0)\n",
    "    return df_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dee56f-bbfd-407d-af98-23b70feccd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rolling_ave(dict_df_weighted_all,num_month_rolling,append_name):\n",
    "    \"\"\"Gets rolling average for each ssp\"\"\"\n",
    "    dict_fil_rolling_df = {}  # dict with filename keys and rolling average dataframe as value.\n",
    "    dict_gcm_hist_realization = {}  # dict with historical dataframes.\n",
    "    dict_gcm_other_realization = {} # dict with ssp dataframes.\n",
    "    #Key is the formated file name and value is dataframe.\n",
    "    for key_file, value_df in dict_df_weighted_all.items():\n",
    "        lst_file_parts = key_file.split('_')\n",
    "        if 'historical' in key_file:\n",
    "            if not lst_file_parts[0] in dict_gcm_hist_realization:\n",
    "                dict_gcm_hist_realization[lst_file_parts[0]] = {}\n",
    "            if not lst_file_parts[1] in dict_gcm_hist_realization[lst_file_parts[0]]:\n",
    "                dict_gcm_hist_realization[lst_file_parts[0]][lst_file_parts[1]] = {}\n",
    "            if not lst_file_parts[2] in dict_gcm_hist_realization[lst_file_parts[0]][lst_file_parts[1]]:\n",
    "                dict_gcm_hist_realization[lst_file_parts[0]][lst_file_parts[1]][lst_file_parts[2]] = []\n",
    "            dict_gcm_hist_realization[lst_file_parts[0]][lst_file_parts[1]][lst_file_parts[2]].append(value_df)\n",
    "        else:\n",
    "            if not lst_file_parts[0] in dict_gcm_other_realization:\n",
    "                dict_gcm_other_realization[lst_file_parts[0]] = {}\n",
    "            if not lst_file_parts[1] in dict_gcm_other_realization[lst_file_parts[0]]:\n",
    "                dict_gcm_other_realization[lst_file_parts[0]][lst_file_parts[1]] = {}\n",
    "            if not lst_file_parts[2] in dict_gcm_other_realization[lst_file_parts[0]][lst_file_parts[1]]:\n",
    "                dict_gcm_other_realization[lst_file_parts[0]][lst_file_parts[1]][lst_file_parts[2]] = []\n",
    "            dict_gcm_other_realization[lst_file_parts[0]][lst_file_parts[1]][lst_file_parts[2]].append(value_df)        \n",
    "    \n",
    "    # Do rolling average and output with dictionary key as filename and values as rolling average dataframe.\n",
    "    df_rolling = pd.DataFrame\n",
    "    for key_gcm in dict_gcm_hist_realization:\n",
    "        for key_ssp in dict_gcm_hist_realization[key_gcm]:\n",
    "            for key_relization in dict_gcm_hist_realization[key_gcm][key_ssp]:\n",
    "                for df_history in dict_gcm_hist_realization[key_gcm][key_ssp][key_relization]:\n",
    "                    for key_ssp_other in dict_gcm_other_realization[key_gcm]:\n",
    "                        for df_ssp in dict_gcm_other_realization[key_gcm][key_ssp_other][key_relization]:\n",
    "                            df_rolling = pd.concat([df_history,df_ssp], axis=0)\n",
    "                            df_rolling['Pr (mm)'] = df_rolling['Pr (mm)'].rolling(360).mean()\n",
    "                            df_rolling['Tasmax (degC)'] = df_rolling['Tasmax (degC)'].rolling(360).mean()\n",
    "                            df_rolling['Tasmin (degC)'] = df_rolling['Tasmin (degC)'].rolling(360).mean()\n",
    "                            df_rolling['Tave (degC)'] = df_rolling['Tave (degC)'].rolling(360).mean()\n",
    "                            file_out = '%s_%s_%s_%s.csv'%(key_gcm,key_ssp_other,key_relization,append_name)\n",
    "                            dict_fil_rolling_df[file_out] = df_rolling\n",
    "    return dict_fil_rolling_df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501cd359-2f63-4723-ada5-9a759182adb2",
   "metadata": {},
   "source": [
    "The loop goes through all GCMs and writes the individual subbasin, the area weighted, and the flow weighted to results dictionaries with filname as the key and value equal to result dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0af3b2e-5cb6-495c-a786-7964323026bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# main loop\n",
    "all_model_params = get_model_params(run_list_path)\n",
    "\n",
    "#Defile output dicts.\n",
    "results_dict = {}\n",
    "flow_weighted_results_dict = {}\n",
    "area_weighted_basin_results_dict = {}\n",
    "\n",
    "for model_params in all_model_params:\n",
    "    #Add masking to the dataset.\n",
    "    ds = load_dataset_with_mask(esm_datastore, model_params, mask_path,bbox)\n",
    "   \n",
    "    #Force load the dataset.\n",
    "    ds = ds.compute()\n",
    "    output_filename = '%s_%s_%s_%s.nc'%(model_params['source_id'],model_params['experiment_id'],model_params['member_id'],'1950')\n",
    "    map_data = ds.where(ds.mask != -1)\n",
    "    map_data.to_netcdf(os.path.join(output_folder, output_filename))\n",
    "        \n",
    "    df_w = None\n",
    "    df_a = None\n",
    "    for id_region, v in region_dict.items():     \n",
    "        # Get this regions results\n",
    "        df_n = get_df_map_mask(id_region,ds)\n",
    "        output_filename = get_output_file_name_monthly(model_params, '%s-19'%'{:02d}'.format(id_region))\n",
    "        #print('Adding.. %s'%output_filename)\n",
    "        #df_out = df_n.drop('time' , axis=1)\n",
    "        results_dict[output_filename] = df_n\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a128fe-3ead-4a52-a601-e6e637c5b557",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write to ouput.\n",
    "zip_path = os.path.join(output_folder, file_zip)\n",
    "\n",
    "with zipfile.ZipFile(zip_path, mode=\"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
    "    for k, v in results_dict.items():\n",
    "        text_stream = StringIO();\n",
    "        v.to_csv(text_stream, index=False)\n",
    "        fileout = dir_individual + '/' + k\n",
    "        zf.writestr(fileout, text_stream.getvalue())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee98484-2c9e-4768-ad92-c1dd039787d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae43c49-a49d-4ff7-bf17-1171693deefc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
