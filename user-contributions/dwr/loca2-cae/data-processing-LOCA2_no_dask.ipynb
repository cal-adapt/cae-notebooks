{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cce06bd7-61f8-4c22-894f-ad9941064f9f",
   "metadata": {},
   "source": [
    "## Cal-Adapt Analytics Engine Data processing script for DWR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ee17fd-e9a4-4b1d-9165-c4cd089fbfde",
   "metadata": {},
   "source": [
    "This script download climate data for pr, tasmax, and tasmin, process the data to monthly time step, weights each subasins of interest by Area and Flow and combines that result to average monthing value for the entire watershed. The average weighted results for Area and Flow are then processed into a monthly rolling average series for pr, tasmax, tasmin, and tasave.\n",
    "The output format of processed climate data are listed below:\n",
    "* Each subasin is stored stored to exported to CSV without the weights with file name format {source_id}_{experiment_id}_{member_id}_{basin_id}_19.csv.  \n",
    "* The Area and Flow weighted value are exported to CSV files in seperate directories with the file name format {source_id}_{experiment_id}_{member_id}_19{weight_type}Weighted.csv.\n",
    "* The monthly rolling average results are exported to CSV files in seperate diretories based on inputs weighting.  The file format is {source_id}_{member_id}_{experiment_id}_30yrAve.csv.\n",
    "Note: All SSPs/realization(member_1d) must have the corrisponding historical/realization(member_1d) otherwise the 30YrAve postproccesing will throw an error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb58d327-b995-4189-b98b-ce8794c0546a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, js_modules, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "    if (js_modules == null) js_modules = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls.length === 0 && js_modules.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error() {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < css_urls.length; i++) {\n",
       "      var url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    var skip = [];\n",
       "    if (window.requirejs) {\n",
       "      window.requirejs.config({'packages': {}, 'paths': {'gridstack': 'https://cdn.jsdelivr.net/npm/gridstack@4.2.5/dist/gridstack-h5', 'notyf': 'https://cdn.jsdelivr.net/npm/notyf@3/notyf.min'}, 'shim': {'gridstack': {'exports': 'GridStack'}}});\n",
       "      require([\"gridstack\"], function(GridStack) {\n",
       "\twindow.GridStack = GridStack\n",
       "\ton_load()\n",
       "      })\n",
       "      require([\"notyf\"], function() {\n",
       "\ton_load()\n",
       "      })\n",
       "      root._bokeh_is_loading = css_urls.length + 2;\n",
       "    } else {\n",
       "      root._bokeh_is_loading = css_urls.length + js_urls.length + js_modules.length;\n",
       "    }    if (((window['GridStack'] !== undefined) && (!(window['GridStack'] instanceof HTMLElement))) || window.requirejs) {\n",
       "      var urls = ['https://cdn.jsdelivr.net/npm/gridstack@4.2.5/dist/gridstack-h5.js'];\n",
       "      for (var i = 0; i < urls.length; i++) {\n",
       "        skip.push(urls[i])\n",
       "      }\n",
       "    }    if (((window['Notyf'] !== undefined) && (!(window['Notyf'] instanceof HTMLElement))) || window.requirejs) {\n",
       "      var urls = ['https://cdn.jsdelivr.net/npm/notyf@3/notyf.min.js'];\n",
       "      for (var i = 0; i < urls.length; i++) {\n",
       "        skip.push(urls[i])\n",
       "      }\n",
       "    }    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      if (skip.indexOf(url) >= 0) {\n",
       "\tif (!window.requirejs) {\n",
       "\t  on_load();\n",
       "\t}\n",
       "\tcontinue;\n",
       "      }\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "    for (var i = 0; i < js_modules.length; i++) {\n",
       "      var url = js_modules[i];\n",
       "      if (skip.indexOf(url) >= 0) {\n",
       "\tif (!window.requirejs) {\n",
       "\t  on_load();\n",
       "\t}\n",
       "\tcontinue;\n",
       "      }\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      element.type = \"module\";\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "    if (!js_urls.length && !js_modules.length) {\n",
       "      on_load()\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-2.4.3.min.js\", \"https://cdn.jsdelivr.net/npm/notyf@3/notyf.min.js\", \"https://unpkg.com/@holoviz/panel@0.13.1/dist/panel.min.js\"];\n",
       "  var js_modules = [];\n",
       "  var css_urls = [\"https://cdn.jsdelivr.net/npm/notyf@3/notyf.min.css\", \"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css\", \"https://unpkg.com/@holoviz/panel@0.13.1/dist/css/alerts.css\", \"https://unpkg.com/@holoviz/panel@0.13.1/dist/css/card.css\", \"https://unpkg.com/@holoviz/panel@0.13.1/dist/css/dataframe.css\", \"https://unpkg.com/@holoviz/panel@0.13.1/dist/css/debugger.css\", \"https://unpkg.com/@holoviz/panel@0.13.1/dist/css/json.css\", \"https://unpkg.com/@holoviz/panel@0.13.1/dist/css/loading.css\", \"https://unpkg.com/@holoviz/panel@0.13.1/dist/css/markdown.css\", \"https://unpkg.com/@holoviz/panel@0.13.1/dist/css/widgets.css\"];\n",
       "  var inline_js = [    function(Bokeh) {\n",
       "      inject_raw_css(\"\\n    .bk.pn-loading.arcs:before {\\n      background-image: url(\\\"data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiBzdHlsZT0ibWFyZ2luOiBhdXRvOyBiYWNrZ3JvdW5kOiBub25lOyBkaXNwbGF5OiBibG9jazsgc2hhcGUtcmVuZGVyaW5nOiBhdXRvOyIgdmlld0JveD0iMCAwIDEwMCAxMDAiIHByZXNlcnZlQXNwZWN0UmF0aW89InhNaWRZTWlkIj4gIDxjaXJjbGUgY3g9IjUwIiBjeT0iNTAiIHI9IjMyIiBzdHJva2Utd2lkdGg9IjgiIHN0cm9rZT0iI2MzYzNjMyIgc3Ryb2tlLWRhc2hhcnJheT0iNTAuMjY1NDgyNDU3NDM2NjkgNTAuMjY1NDgyNDU3NDM2NjkiIGZpbGw9Im5vbmUiIHN0cm9rZS1saW5lY2FwPSJyb3VuZCI+ICAgIDxhbmltYXRlVHJhbnNmb3JtIGF0dHJpYnV0ZU5hbWU9InRyYW5zZm9ybSIgdHlwZT0icm90YXRlIiByZXBlYXRDb3VudD0iaW5kZWZpbml0ZSIgZHVyPSIxcyIga2V5VGltZXM9IjA7MSIgdmFsdWVzPSIwIDUwIDUwOzM2MCA1MCA1MCI+PC9hbmltYXRlVHJhbnNmb3JtPiAgPC9jaXJjbGU+PC9zdmc+\\\");\\n      background-size: auto calc(min(50%, 400px));\\n    }\\n    \");\n",
       "    },    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "function(Bokeh) {} // ensure no trailing comma for IE\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    }\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, js_modules, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.holoviews_load.v0+json": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, js_modules, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n    if (js_modules == null) js_modules = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls.length === 0 && js_modules.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    var skip = [];\n    if (window.requirejs) {\n      window.requirejs.config({'packages': {}, 'paths': {'gridstack': 'https://cdn.jsdelivr.net/npm/gridstack@4.2.5/dist/gridstack-h5', 'notyf': 'https://cdn.jsdelivr.net/npm/notyf@3/notyf.min'}, 'shim': {'gridstack': {'exports': 'GridStack'}}});\n      require([\"gridstack\"], function(GridStack) {\n\twindow.GridStack = GridStack\n\ton_load()\n      })\n      require([\"notyf\"], function() {\n\ton_load()\n      })\n      root._bokeh_is_loading = css_urls.length + 2;\n    } else {\n      root._bokeh_is_loading = css_urls.length + js_urls.length + js_modules.length;\n    }    if (((window['GridStack'] !== undefined) && (!(window['GridStack'] instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://cdn.jsdelivr.net/npm/gridstack@4.2.5/dist/gridstack-h5.js'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(urls[i])\n      }\n    }    if (((window['Notyf'] !== undefined) && (!(window['Notyf'] instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://cdn.jsdelivr.net/npm/notyf@3/notyf.min.js'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(urls[i])\n      }\n    }    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      if (skip.indexOf(url) >= 0) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (var i = 0; i < js_modules.length; i++) {\n      var url = js_modules[i];\n      if (skip.indexOf(url) >= 0) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    if (!js_urls.length && !js_modules.length) {\n      on_load()\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-2.4.3.min.js\", \"https://cdn.jsdelivr.net/npm/notyf@3/notyf.min.js\", \"https://unpkg.com/@holoviz/panel@0.13.1/dist/panel.min.js\"];\n  var js_modules = [];\n  var css_urls = [\"https://cdn.jsdelivr.net/npm/notyf@3/notyf.min.css\", \"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css\", \"https://unpkg.com/@holoviz/panel@0.13.1/dist/css/alerts.css\", \"https://unpkg.com/@holoviz/panel@0.13.1/dist/css/card.css\", \"https://unpkg.com/@holoviz/panel@0.13.1/dist/css/dataframe.css\", \"https://unpkg.com/@holoviz/panel@0.13.1/dist/css/debugger.css\", \"https://unpkg.com/@holoviz/panel@0.13.1/dist/css/json.css\", \"https://unpkg.com/@holoviz/panel@0.13.1/dist/css/loading.css\", \"https://unpkg.com/@holoviz/panel@0.13.1/dist/css/markdown.css\", \"https://unpkg.com/@holoviz/panel@0.13.1/dist/css/widgets.css\"];\n  var inline_js = [    function(Bokeh) {\n      inject_raw_css(\"\\n    .bk.pn-loading.arcs:before {\\n      background-image: url(\\\"data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiBzdHlsZT0ibWFyZ2luOiBhdXRvOyBiYWNrZ3JvdW5kOiBub25lOyBkaXNwbGF5OiBibG9jazsgc2hhcGUtcmVuZGVyaW5nOiBhdXRvOyIgdmlld0JveD0iMCAwIDEwMCAxMDAiIHByZXNlcnZlQXNwZWN0UmF0aW89InhNaWRZTWlkIj4gIDxjaXJjbGUgY3g9IjUwIiBjeT0iNTAiIHI9IjMyIiBzdHJva2Utd2lkdGg9IjgiIHN0cm9rZT0iI2MzYzNjMyIgc3Ryb2tlLWRhc2hhcnJheT0iNTAuMjY1NDgyNDU3NDM2NjkgNTAuMjY1NDgyNDU3NDM2NjkiIGZpbGw9Im5vbmUiIHN0cm9rZS1saW5lY2FwPSJyb3VuZCI+ICAgIDxhbmltYXRlVHJhbnNmb3JtIGF0dHJpYnV0ZU5hbWU9InRyYW5zZm9ybSIgdHlwZT0icm90YXRlIiByZXBlYXRDb3VudD0iaW5kZWZpbml0ZSIgZHVyPSIxcyIga2V5VGltZXM9IjA7MSIgdmFsdWVzPSIwIDUwIDUwOzM2MCA1MCA1MCI+PC9hbmltYXRlVHJhbnNmb3JtPiAgPC9jaXJjbGU+PC9zdmc+\\\");\\n      background-size: auto calc(min(50%, 400px));\\n    }\\n    \");\n    },    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {} // ensure no trailing comma for IE\n  ];\n\n  function run_inline_js() {\n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, js_modules, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "if ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n",
       "  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n",
       "}\n",
       "\n",
       "\n",
       "    function JupyterCommManager() {\n",
       "    }\n",
       "\n",
       "    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n",
       "      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n",
       "        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n",
       "        comm_manager.register_target(comm_id, function(comm) {\n",
       "          comm.on_msg(msg_handler);\n",
       "        });\n",
       "      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n",
       "        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n",
       "          comm.onMsg = msg_handler;\n",
       "        });\n",
       "      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n",
       "        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n",
       "          var messages = comm.messages[Symbol.asyncIterator]();\n",
       "          function processIteratorResult(result) {\n",
       "            var message = result.value;\n",
       "            console.log(message)\n",
       "            var content = {data: message.data, comm_id};\n",
       "            var buffers = []\n",
       "            for (var buffer of message.buffers || []) {\n",
       "              buffers.push(new DataView(buffer))\n",
       "            }\n",
       "            var metadata = message.metadata || {};\n",
       "            var msg = {content, buffers, metadata}\n",
       "            msg_handler(msg);\n",
       "            return messages.next().then(processIteratorResult);\n",
       "          }\n",
       "          return messages.next().then(processIteratorResult);\n",
       "        })\n",
       "      }\n",
       "    }\n",
       "\n",
       "    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n",
       "      if (comm_id in window.PyViz.comms) {\n",
       "        return window.PyViz.comms[comm_id];\n",
       "      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n",
       "        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n",
       "        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n",
       "        if (msg_handler) {\n",
       "          comm.on_msg(msg_handler);\n",
       "        }\n",
       "      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n",
       "        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n",
       "        comm.open();\n",
       "        if (msg_handler) {\n",
       "          comm.onMsg = msg_handler;\n",
       "        }\n",
       "      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n",
       "        var comm_promise = google.colab.kernel.comms.open(comm_id)\n",
       "        comm_promise.then((comm) => {\n",
       "          window.PyViz.comms[comm_id] = comm;\n",
       "          if (msg_handler) {\n",
       "            var messages = comm.messages[Symbol.asyncIterator]();\n",
       "            function processIteratorResult(result) {\n",
       "              var message = result.value;\n",
       "              var content = {data: message.data};\n",
       "              var metadata = message.metadata || {comm_id};\n",
       "              var msg = {content, metadata}\n",
       "              msg_handler(msg);\n",
       "              return messages.next().then(processIteratorResult);\n",
       "            }\n",
       "            return messages.next().then(processIteratorResult);\n",
       "          }\n",
       "        }) \n",
       "        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n",
       "          return comm_promise.then((comm) => {\n",
       "            comm.send(data, metadata, buffers, disposeOnDone);\n",
       "          });\n",
       "        };\n",
       "        var comm = {\n",
       "          send: sendClosure\n",
       "        };\n",
       "      }\n",
       "      window.PyViz.comms[comm_id] = comm;\n",
       "      return comm;\n",
       "    }\n",
       "    window.PyViz.comm_manager = new JupyterCommManager();\n",
       "    \n",
       "\n",
       "\n",
       "var JS_MIME_TYPE = 'application/javascript';\n",
       "var HTML_MIME_TYPE = 'text/html';\n",
       "var EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\n",
       "var CLASS_NAME = 'output';\n",
       "\n",
       "/**\n",
       " * Render data to the DOM node\n",
       " */\n",
       "function render(props, node) {\n",
       "  var div = document.createElement(\"div\");\n",
       "  var script = document.createElement(\"script\");\n",
       "  node.appendChild(div);\n",
       "  node.appendChild(script);\n",
       "}\n",
       "\n",
       "/**\n",
       " * Handle when a new output is added\n",
       " */\n",
       "function handle_add_output(event, handle) {\n",
       "  var output_area = handle.output_area;\n",
       "  var output = handle.output;\n",
       "  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "    return\n",
       "  }\n",
       "  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "  if (id !== undefined) {\n",
       "    var nchildren = toinsert.length;\n",
       "    var html_node = toinsert[nchildren-1].children[0];\n",
       "    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "    var scripts = [];\n",
       "    var nodelist = html_node.querySelectorAll(\"script\");\n",
       "    for (var i in nodelist) {\n",
       "      if (nodelist.hasOwnProperty(i)) {\n",
       "        scripts.push(nodelist[i])\n",
       "      }\n",
       "    }\n",
       "\n",
       "    scripts.forEach( function (oldScript) {\n",
       "      var newScript = document.createElement(\"script\");\n",
       "      var attrs = [];\n",
       "      var nodemap = oldScript.attributes;\n",
       "      for (var j in nodemap) {\n",
       "        if (nodemap.hasOwnProperty(j)) {\n",
       "          attrs.push(nodemap[j])\n",
       "        }\n",
       "      }\n",
       "      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n",
       "      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n",
       "      oldScript.parentNode.replaceChild(newScript, oldScript);\n",
       "    });\n",
       "    if (JS_MIME_TYPE in output.data) {\n",
       "      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n",
       "    }\n",
       "    output_area._hv_plot_id = id;\n",
       "    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n",
       "      window.PyViz.plot_index[id] = Bokeh.index[id];\n",
       "    } else {\n",
       "      window.PyViz.plot_index[id] = null;\n",
       "    }\n",
       "  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "    var bk_div = document.createElement(\"div\");\n",
       "    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "    var script_attrs = bk_div.children[0].attributes;\n",
       "    for (var i = 0; i < script_attrs.length; i++) {\n",
       "      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "    }\n",
       "    // store reference to server id on output_area\n",
       "    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "  }\n",
       "}\n",
       "\n",
       "/**\n",
       " * Handle when an output is cleared or removed\n",
       " */\n",
       "function handle_clear_output(event, handle) {\n",
       "  var id = handle.cell.output_area._hv_plot_id;\n",
       "  var server_id = handle.cell.output_area._bokeh_server_id;\n",
       "  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n",
       "  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n",
       "  if (server_id !== null) {\n",
       "    comm.send({event_type: 'server_delete', 'id': server_id});\n",
       "    return;\n",
       "  } else if (comm !== null) {\n",
       "    comm.send({event_type: 'delete', 'id': id});\n",
       "  }\n",
       "  delete PyViz.plot_index[id];\n",
       "  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n",
       "    var doc = window.Bokeh.index[id].model.document\n",
       "    doc.clear();\n",
       "    const i = window.Bokeh.documents.indexOf(doc);\n",
       "    if (i > -1) {\n",
       "      window.Bokeh.documents.splice(i, 1);\n",
       "    }\n",
       "  }\n",
       "}\n",
       "\n",
       "/**\n",
       " * Handle kernel restart event\n",
       " */\n",
       "function handle_kernel_cleanup(event, handle) {\n",
       "  delete PyViz.comms[\"hv-extension-comm\"];\n",
       "  window.PyViz.plot_index = {}\n",
       "}\n",
       "\n",
       "/**\n",
       " * Handle update_display_data messages\n",
       " */\n",
       "function handle_update_output(event, handle) {\n",
       "  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n",
       "  handle_add_output(event, handle)\n",
       "}\n",
       "\n",
       "function register_renderer(events, OutputArea) {\n",
       "  function append_mime(data, metadata, element) {\n",
       "    // create a DOM node to render to\n",
       "    var toinsert = this.create_output_subarea(\n",
       "    metadata,\n",
       "    CLASS_NAME,\n",
       "    EXEC_MIME_TYPE\n",
       "    );\n",
       "    this.keyboard_manager.register_events(toinsert);\n",
       "    // Render to node\n",
       "    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "    render(props, toinsert[0]);\n",
       "    element.append(toinsert);\n",
       "    return toinsert\n",
       "  }\n",
       "\n",
       "  events.on('output_added.OutputArea', handle_add_output);\n",
       "  events.on('output_updated.OutputArea', handle_update_output);\n",
       "  events.on('clear_output.CodeCell', handle_clear_output);\n",
       "  events.on('delete.Cell', handle_clear_output);\n",
       "  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n",
       "\n",
       "  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "    safe: true,\n",
       "    index: 0\n",
       "  });\n",
       "}\n",
       "\n",
       "if (window.Jupyter !== undefined) {\n",
       "  try {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  } catch(err) {\n",
       "  }\n",
       "}\n"
      ],
      "application/vnd.holoviews_load.v0+json": "\nif ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n}\n\n\n    function JupyterCommManager() {\n    }\n\n    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        comm_manager.register_target(comm_id, function(comm) {\n          comm.on_msg(msg_handler);\n        });\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n          comm.onMsg = msg_handler;\n        });\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n          var messages = comm.messages[Symbol.asyncIterator]();\n          function processIteratorResult(result) {\n            var message = result.value;\n            console.log(message)\n            var content = {data: message.data, comm_id};\n            var buffers = []\n            for (var buffer of message.buffers || []) {\n              buffers.push(new DataView(buffer))\n            }\n            var metadata = message.metadata || {};\n            var msg = {content, buffers, metadata}\n            msg_handler(msg);\n            return messages.next().then(processIteratorResult);\n          }\n          return messages.next().then(processIteratorResult);\n        })\n      }\n    }\n\n    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n      if (comm_id in window.PyViz.comms) {\n        return window.PyViz.comms[comm_id];\n      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n        if (msg_handler) {\n          comm.on_msg(msg_handler);\n        }\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n        comm.open();\n        if (msg_handler) {\n          comm.onMsg = msg_handler;\n        }\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        var comm_promise = google.colab.kernel.comms.open(comm_id)\n        comm_promise.then((comm) => {\n          window.PyViz.comms[comm_id] = comm;\n          if (msg_handler) {\n            var messages = comm.messages[Symbol.asyncIterator]();\n            function processIteratorResult(result) {\n              var message = result.value;\n              var content = {data: message.data};\n              var metadata = message.metadata || {comm_id};\n              var msg = {content, metadata}\n              msg_handler(msg);\n              return messages.next().then(processIteratorResult);\n            }\n            return messages.next().then(processIteratorResult);\n          }\n        }) \n        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n          return comm_promise.then((comm) => {\n            comm.send(data, metadata, buffers, disposeOnDone);\n          });\n        };\n        var comm = {\n          send: sendClosure\n        };\n      }\n      window.PyViz.comms[comm_id] = comm;\n      return comm;\n    }\n    window.PyViz.comm_manager = new JupyterCommManager();\n    \n\n\nvar JS_MIME_TYPE = 'application/javascript';\nvar HTML_MIME_TYPE = 'text/html';\nvar EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\nvar CLASS_NAME = 'output';\n\n/**\n * Render data to the DOM node\n */\nfunction render(props, node) {\n  var div = document.createElement(\"div\");\n  var script = document.createElement(\"script\");\n  node.appendChild(div);\n  node.appendChild(script);\n}\n\n/**\n * Handle when a new output is added\n */\nfunction handle_add_output(event, handle) {\n  var output_area = handle.output_area;\n  var output = handle.output;\n  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n    return\n  }\n  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n  if (id !== undefined) {\n    var nchildren = toinsert.length;\n    var html_node = toinsert[nchildren-1].children[0];\n    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n    var scripts = [];\n    var nodelist = html_node.querySelectorAll(\"script\");\n    for (var i in nodelist) {\n      if (nodelist.hasOwnProperty(i)) {\n        scripts.push(nodelist[i])\n      }\n    }\n\n    scripts.forEach( function (oldScript) {\n      var newScript = document.createElement(\"script\");\n      var attrs = [];\n      var nodemap = oldScript.attributes;\n      for (var j in nodemap) {\n        if (nodemap.hasOwnProperty(j)) {\n          attrs.push(nodemap[j])\n        }\n      }\n      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n      oldScript.parentNode.replaceChild(newScript, oldScript);\n    });\n    if (JS_MIME_TYPE in output.data) {\n      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n    }\n    output_area._hv_plot_id = id;\n    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n      window.PyViz.plot_index[id] = Bokeh.index[id];\n    } else {\n      window.PyViz.plot_index[id] = null;\n    }\n  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n    var bk_div = document.createElement(\"div\");\n    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n    var script_attrs = bk_div.children[0].attributes;\n    for (var i = 0; i < script_attrs.length; i++) {\n      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n    }\n    // store reference to server id on output_area\n    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n  }\n}\n\n/**\n * Handle when an output is cleared or removed\n */\nfunction handle_clear_output(event, handle) {\n  var id = handle.cell.output_area._hv_plot_id;\n  var server_id = handle.cell.output_area._bokeh_server_id;\n  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n  if (server_id !== null) {\n    comm.send({event_type: 'server_delete', 'id': server_id});\n    return;\n  } else if (comm !== null) {\n    comm.send({event_type: 'delete', 'id': id});\n  }\n  delete PyViz.plot_index[id];\n  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n    var doc = window.Bokeh.index[id].model.document\n    doc.clear();\n    const i = window.Bokeh.documents.indexOf(doc);\n    if (i > -1) {\n      window.Bokeh.documents.splice(i, 1);\n    }\n  }\n}\n\n/**\n * Handle kernel restart event\n */\nfunction handle_kernel_cleanup(event, handle) {\n  delete PyViz.comms[\"hv-extension-comm\"];\n  window.PyViz.plot_index = {}\n}\n\n/**\n * Handle update_display_data messages\n */\nfunction handle_update_output(event, handle) {\n  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n  handle_add_output(event, handle)\n}\n\nfunction register_renderer(events, OutputArea) {\n  function append_mime(data, metadata, element) {\n    // create a DOM node to render to\n    var toinsert = this.create_output_subarea(\n    metadata,\n    CLASS_NAME,\n    EXEC_MIME_TYPE\n    );\n    this.keyboard_manager.register_events(toinsert);\n    // Render to node\n    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n    render(props, toinsert[0]);\n    element.append(toinsert);\n    return toinsert\n  }\n\n  events.on('output_added.OutputArea', handle_add_output);\n  events.on('output_updated.OutputArea', handle_update_output);\n  events.on('clear_output.CodeCell', handle_clear_output);\n  events.on('delete.Cell', handle_clear_output);\n  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n\n  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n    safe: true,\n    index: 0\n  });\n}\n\nif (window.Jupyter !== undefined) {\n  try {\n    var events = require('base/js/events');\n    var OutputArea = require('notebook/js/outputarea').OutputArea;\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  } catch(err) {\n  }\n}\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>.bk-root, .bk-root .bk:before, .bk-root .bk:after {\n",
       "  font-family: var(--jp-ui-font-size1);\n",
       "  font-size: var(--jp-ui-font-size1);\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       "</style>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<xarray.core.options.set_options at 0x7fcf53f6d940>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "from io import StringIO\n",
    "import json\n",
    "import os\n",
    "import tempfile\n",
    "import zipfile\n",
    "\n",
    "import pandas as pd\n",
    "import intake\n",
    "from shapely.geometry import Point, Polygon\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import dask\n",
    "import panel as pn\n",
    "#from dask.distributed import progress\n",
    "#from dask.distributed import Client\n",
    "#from climakitae.cluster import Cluster\n",
    "\n",
    "pn.extension()\n",
    "xr.set_options(keep_attrs=True)\n",
    "#3dask.config.set({\"array.slicing.split_large_chunks\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01e40c6c-9aaa-4365-ab1d-3bf8267c42e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VARIABLES\n",
    "#Use these cordinates to clip around the watershed of interest.\n",
    "# latitude = [34.775317,42.432494]\n",
    "# longitude = [-123.097421,-117.980799]\n",
    "bbox = {\n",
    "    \"maxy\": 42.432494,\n",
    "    \"miny\": 34.775317,\n",
    "    \"minx\": -123.097421,\n",
    "    \"maxx\": -117.980799,\n",
    "}\n",
    "\n",
    "#run_list_path = \"data/GCM_Run_List_1-43.csv\"\n",
    "#file_zip = \"GCM_1-43.zip\"\n",
    "\n",
    "run_list_path = \"data/GCM_Run_List_1-19.csv\"\n",
    "file_zip = \"GCM_1-19.zip\"\n",
    "\n",
    "run_list_path = \"data/GCM_Run_List_20-43.csv\"\n",
    "file_zip = \"GCM_20-43.zip\"\n",
    "\n",
    "run_list_path = \"data/GCM_Run_List_44-82.csv\"\n",
    "file_zip = \"GCM_44-82.zip\"\n",
    "\n",
    "run_list_path = \"data/GCM_Run_List_83-100.csv\"\n",
    "file_zip = \"GCM_83-100.zip\"\n",
    "\n",
    "run_list_path = \"data/GCM_Run_List_Add_23-47.csv\"\n",
    "file_zip = \"GCM_Add_23-47.zip\"\n",
    "\n",
    "#run_list_path = \"data/GCM_Run_List_101-117.csv\"\n",
    "#file_zip = \"GCM_101-117.zip\"\n",
    "\n",
    "#run_list_path = \"data/GCM_Run_List_Test_Rolling.csv\"\n",
    "#file_zip = \"GCM_Rolling_Test.zip\"\n",
    "\n",
    "basin_weights_csv = \"data/Basin_Weights.csv\"\n",
    "esm_datastore = \"https://cadcat.s3.amazonaws.com/cae-collection.json\"\n",
    "output_folder = \"outputs\"\n",
    "mask_path = \"mask/mask.npy\"\n",
    "dir_area_weighted = 'AREA_WEIGHTED_CENTRALVALLEY'\n",
    "dir_flow_weighted = 'FLOW_WEIGHTED_CENTRALVALLEY'\n",
    "dir_non_weighted = 'NON_WEIGHTED_CENTRALVALLEY'\n",
    "dir_individual = 'INDIVIDUAL_BASIN_LOCA2'\n",
    "dir_area_weighted_rolling = 'AREA_WEIGHTED_30_YEAR_ROLLING_AVE_CENTRALVALLEY'\n",
    "dir_flow_weighted_rolling = 'FLOW_WEIGHTED_30_YEAR_ROLLING_AVE_CENTRALVALLEY'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef6091f-e369-4137-9551-6b661cea5a57",
   "metadata": {},
   "source": [
    "This loads the dask client for faster processing.  If client is running, do not run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "264d2e29-399c-427f-9efd-7e59edf7c611",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load dask Area for faster computing. \n",
    "#Note, this will take awhile but in long run processing should be faster when compute is called.\n",
    "#cluster = Cluster()\n",
    "#cluster.adapt(minimum=0, maximum=16)\n",
    "#cluster.adapt(minimum=0, maximum=30)\n",
    "#client = cluster.get_client()\n",
    "\n",
    "#Get client link.\n",
    "#client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2649b7a-d8d1-44a3-9860-cf2715649e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_region_dict():\n",
    "    \"\"\" Gets flow and area weights for each region\"\"\"\n",
    "    region_dict = {}\n",
    "    with open(basin_weights_csv, \"r\") as csv_file:\n",
    "        reader = csv.DictReader(csv_file)\n",
    "        for row in reader:\n",
    "            region_dict[int(row[\"ID\"])] = {\n",
    "                                \"region_name\": row[\"Regions\"],\n",
    "                                \"flow_ratio\": float(row[\"Flow Ratio\"]),\n",
    "                                \"area_ratio\": float(row[\"Area Ratio\"]),\n",
    "                            }\n",
    "    return region_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55bfb833-a0ed-4d94-ac16-a92df5614c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removed 99: {'region_name': 'Diversion from Echo Lake', 'weighting_factor': 0.0},\n",
    "# DEPRECATED\n",
    "# region_dict = {\n",
    "#     0:  {'region_name': 'Goose Lake', 'weighting_factor': 0.0},\n",
    "#     11: {'region_name': 'Westside Streams of SJR', 'weighting_factor': 0.002758980030193925},\n",
    "#     16: {'region_name': 'Other Rim Inflows of Sac', 'weighting_factor': 0.06520559638738632},\n",
    "#     9:  {'region_name': 'Eastside Streams of Delta', 'weighting_factor': 0.047054700553417206},\n",
    "#     17: {'region_name': 'Other Rim Inflows of SJR', 'weighting_factor': 0.008974390104413033},\n",
    "#     5:  {'region_name': 'Upper Stanislaus River', 'weighting_factor': 0.03673909977078438},\n",
    "#     8:  {'region_name': 'Lake Millerton', 'weighting_factor': 0.05571430176496506},\n",
    "#     10: {'region_name': 'Westside Streams of Sac', 'weighting_factor': 0.0789882019162178},\n",
    "#     12: {'region_name': 'Valley Floor of Sac', 'weighting_factor': 0.06745839864015579},\n",
    "#     18: {'region_name': 'Lower Yuba-Bear Rim Inflow', 'weighting_factor': 0.018660200759768486},\n",
    "#     14: {'region_name': 'Tulare Basin', 'weighting_factor': 0.0},\n",
    "#     1:  {'region_name': 'Lake Shasta', 'weighting_factor': 0.1778690069913864},\n",
    "#     15: {'region_name': 'Lake Trinity', 'weighting_factor': 0.04051230102777481},\n",
    "#     2:  {'region_name': 'Upper Feather River', 'weighting_factor': 0.13809999823570251},\n",
    "#     13: {'region_name': 'Valley Floor of SJR', 'weighting_factor': 0.008356500416994095},\n",
    "#     3:  {'region_name': 'Upper Yuba River', 'weighting_factor': 0.07005230337381363},\n",
    "#     19: {'region_name': 'Delta', 'weighting_factor': 0.026663200929760933},\n",
    "#     4:  {'region_name': 'Upper American River', 'weighting_factor': 0.08627369999885559},\n",
    "#     6:  {'region_name': 'Upper Tuolumne River', 'weighting_factor': 0.05876690149307251},\n",
    "#     7:  {'region_name': 'Upper Merced River', 'weighting_factor': 0.030512800440192223}\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a5e0ccc-88e4-488c-94fa-8b62c0b7bdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_params(run_list_path):\n",
    "    \"\"\" Read each set of model parameters into dictionary from csv file.\n",
    "        Return list of dictionaries.\n",
    "    \"\"\"\n",
    "    model_params = []\n",
    "    with open(run_list_path, \"r\") as src:\n",
    "        d = csv.DictReader(src)\n",
    "        for row in d:\n",
    "            model_params.append(row)\n",
    "    return model_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "203b349d-4c0d-408d-bd45-4b7a2f5d9938",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(esm_datastore, model_params):\n",
    "    \"\"\"Return xarray.DataSet from model parameters.\"\"\"\n",
    "    # Open catalog of available data sets using intake-esm package\n",
    "    cat = intake.open_esm_datastore(esm_datastore)\n",
    "    cat_item = cat.search(\n",
    "        activity_id=model_params[\"activity_id\"],\n",
    "        institution_id=model_params[\"institution_id\"],\n",
    "        table_id=model_params[\"table_id\"], \n",
    "        variable_id=['pr','tasmax','tasmin'],\n",
    "        experiment_id=model_params[\"experiment_id\"],\n",
    "        grid_label=model_params[\"grid_label\"],\n",
    "        member_id=model_params[\"member_id\"],\n",
    "        source_id=model_params[\"source_id\"],  \n",
    "    )\n",
    "    \n",
    "    # Add catalog item to dataset dict\n",
    "    data_dict = cat_item.to_dataset_dict(\n",
    "      #  xarray_open_kwargs={'consolidated': True},\n",
    "        storage_options={'anon': True}\n",
    "    )\n",
    "    \n",
    "    # Construct dataset key to retrieve from the dictionary\n",
    "    key = \"{}.{}.{}.{}.{}.{}\".format(\n",
    "            model_params['activity_id'],\n",
    "            model_params['institution_id'],\n",
    "            model_params['source_id'],\n",
    "            model_params['experiment_id'],\n",
    "            model_params['table_id'],\n",
    "            model_params['grid_label'],)\n",
    "    \n",
    "    # Slice the dataset to the input time window.\n",
    "    ds = slice_by_time_years_dataset(data_dict[key],model_params['start_year'],model_params['end_year'])\n",
    "    ds = convert_daily_to_monthly_dataset(ds)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48d990d3-0d54-470d-be7a-aa828ef0da5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_mask_to_dataset(mask_path, ds):\n",
    "    \"\"\" Attach the mask input dataset ds. \"\"\"\n",
    "    with open(mask_path, 'rb') as f:\n",
    "        mask = np.load(f, allow_pickle=True)\n",
    "    ds.coords['mask'] = (('lat', 'lon'), mask)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca8381dd-f218-4b17-a1fd-0d22e562b9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_daily_to_monthly_dataset(ds):\n",
    "    \"\"\" Convert our daily values to monthly.  Precip is the accumulated and temperature is the average.\n",
    "        Precip is converted to mm from kg/m2/s: 86400 x kg/m2/s = daily value (mm).\n",
    "        Temperature is converts to C.  C = K - 273.15\n",
    "    \"\"\"\n",
    "    #Convert our daily values to monthly.  Precip is the accumulated and temperature is the average.\n",
    "    # 86400 x kg/m2/s = daily value (mm)\n",
    "    ds['pr'] = ds.pr * 86400\n",
    "    ds.pr.attrs[\"units\"] = 'mm/day' \n",
    "    ds_precip = ds['pr'].resample(time=\"M\").sum()\n",
    "    ds_precip.attrs[\"units\"] = 'mm/mon' \n",
    "    ds_temp = ds[['tasmin','tasmax']].resample(time=\"M\").mean()\n",
    "    \n",
    "    #Change the temp to C\n",
    "    ds_temp = ds_temp[['tasmin','tasmax']] - 273.15\n",
    "    ds_temp.tasmin.attrs[\"units\"]  = 'degC'\n",
    "    ds_temp.tasmax.attrs[\"units\"]  = 'degC'\n",
    "        \n",
    "    #Merge the dataset back into on dataset.\n",
    "    ds= xr.merge([ds_precip,ds_temp])\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d91935c5-dff2-48ae-8858-111927c9cf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_by_time_years_dataset(ds,startyear,endyear):\n",
    "    \"\"\" Slice the dataset to years of interest. \"\"\"\n",
    "    ds = ds.sel(\n",
    "        time=slice(str(startyear), str(endyear))\n",
    "        )\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3901ea78-2ae9-4335-b386-91c0124bbefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_dataset_to_bbox(ds, bbox):\n",
    "    \"\"\" Clip the dataset to a box. \"\"\"\n",
    "    #This needs to be done for the cliping.\n",
    "    ds.rio.set_spatial_dims(x_dim=\"lon\", y_dim=\"lat\", inplace=True)\n",
    "    ds.rio.write_crs(\"EPSG:4326\", inplace=True)\n",
    "\n",
    "    #Get the subset of data for watershed.\n",
    "    ds = ds.rio.clip_box(\n",
    "        minx=bbox[\"minx\"],\n",
    "        miny=bbox[\"miny\"],\n",
    "        maxx=bbox[\"maxx\"],\n",
    "        maxy=bbox[\"maxy\"],\n",
    "    )\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "405fe5e9-361c-4b9f-8561-1a4d7478a1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_file_name_monthly(model_params,end_part):\n",
    "    \"\"\" Format the output file name from model_params \"\"\"\n",
    "    return '%s_%s_%s_%s.csv'%(model_params['source_id'],model_params['experiment_id'],model_params['member_id'],end_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "adf74e9e-2b0d-4344-af9f-c8bafc6859a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_with_mask(esm_datastore_in, model_params_in, mask_path,bbox):\n",
    "    \"\"\" Loads the dataset, adds mask, and trims dataset to box arround area of interest\"\"\"\n",
    "    ds = get_dataset(esm_datastore, model_params)\n",
    "    ds = add_mask_to_dataset(mask_path, ds)\n",
    "    ds = trim_dataset_to_bbox(ds, bbox)\n",
    "    return ds\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca0d9d89-1c25-48e6-a52e-2942252a9644",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_map_mask(id_region,ds,use_full_mask = False):\n",
    "    \"\"\" Returns a dataframe for the id_region. Set id_region = -1 and use_full_mask to get the entire domain. \"\"\" \n",
    "   \n",
    "    if use_full_mask:\n",
    "        map_data = ds.where(ds.mask != id_region)\n",
    "    else:\n",
    "        map_data = ds.where(ds.mask == id_region)\n",
    "\n",
    "    results_precip = map_data.pr.mean(['lat','lon'],skipna=True)\n",
    "    results_precip.attrs[\"units\"]  = 'mm/mon'\n",
    "\n",
    "    results_tasmin = map_data.tasmin.mean(['lat','lon'],skipna=True)\n",
    "    results_tasmin.attrs[\"units\"]  = 'degC'\n",
    "\n",
    "    results_tasmax = map_data.tasmax.mean(['lat','lon'],skipna=True)\n",
    "    results_tasmax.attrs[\"units\"]  = 'degC'\n",
    "\n",
    "    ds_all= xr.merge([results_precip,results_tasmax,results_tasmin])\n",
    "    df = ds_all.to_pandas()\n",
    "\n",
    "    df.drop('spatial_ref',axis=1, inplace=True)\n",
    "\n",
    "    df['Year'] = df.index.strftime('%Y')\n",
    "    df['Month'] = df.index.month\n",
    "    df['Tave (degC)'] = df[['tasmax','tasmin']].mean(axis=1)\n",
    "    df.rename({'pr': 'Pr (mm)','tasmax': 'Tasmax (degC)','tasmin' : 'Tasmin (degC)'}, axis=1,inplace=True)\n",
    "\n",
    "    df_r = df.iloc[:,[3,4,0,1,2,5]]\n",
    "    df_n = df_r.reset_index()\n",
    "    return df_n\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e02c5ffe-4985-4495-8f97-932c877ef003",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weighted_dataframe(df_in, weight):\n",
    "    \"\"\" Computes the weighted from from df_in and returns the resulting dataframe. \"\"\"\n",
    "    df_in['Pr (mm)'] = df_in['Pr (mm)'] * weighting_factor\n",
    "    df_in['Tasmax (degC)'] = df_in['Tasmax (degC)'] * weighting_factor\n",
    "    df_in['Tasmin (degC)'] = df_in['Tasmin (degC)'] * weighting_factor\n",
    "    df_in['Tave (degC)'] = df_in['Tave (degC)'] * weighting_factor\n",
    "    return df_in\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3fb4db7-db40-4955-9cd6-2a9270e98526",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sum_dataframes(df_in, df_to_add):\n",
    "    \"\"\" Adds the weighted results from df_to_add to the df_in and returns the resulting dataframe. \"\"\"\n",
    "    df_in['Pr (mm)'] = df_in['Pr (mm)'].add(df_to_add['Pr (mm)'], fill_value=0)\n",
    "    df_in['Tasmax (degC)'] =  df_in['Tasmax (degC)'].add(df_to_add['Tasmax (degC)'], fill_value=0)\n",
    "    df_in['Tasmin (degC)'] =  df_in['Tasmin (degC)'].add(df_to_add['Tasmin (degC)'], fill_value=0)\n",
    "    df_in['Tave (degC)'] =  df_in['Tave (degC)'].add(df_to_add['Tave (degC)'], fill_value=0)\n",
    "    return df_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1dee56f-bbfd-407d-af98-23b70feccd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_monthly_rolling_ave(dict_df_weighted_all,average_over_years,append_name):\n",
    "    \"\"\"Gets rolling average for each SSP\"\"\"\n",
    "    dict_fil_rolling_df = {}  # dict with filename keys and rolling average dataframe as value.\n",
    "    dict_gcm_hist_realization = {}  # dict with historical dataframes.\n",
    "    dict_gcm_other_realization = {} # dict with ssp dataframes.\n",
    "    #Key is the formated file name and value is dataframe.\n",
    "    for key_file, value_df in dict_df_weighted_all.items():\n",
    "        lst_file_parts = key_file.split('_')\n",
    "        if 'historical' in key_file:\n",
    "            if not lst_file_parts[0] in dict_gcm_hist_realization:\n",
    "                dict_gcm_hist_realization[lst_file_parts[0]] = {}\n",
    "            if not lst_file_parts[1] in dict_gcm_hist_realization[lst_file_parts[0]]:\n",
    "                dict_gcm_hist_realization[lst_file_parts[0]][lst_file_parts[1]] = {}\n",
    "            dict_gcm_hist_realization[lst_file_parts[0]][lst_file_parts[1]][lst_file_parts[2]] = value_df\n",
    "        else:\n",
    "            if not lst_file_parts[0] in dict_gcm_other_realization:\n",
    "                dict_gcm_other_realization[lst_file_parts[0]] = {}\n",
    "            if not lst_file_parts[1] in dict_gcm_other_realization[lst_file_parts[0]]:\n",
    "                dict_gcm_other_realization[lst_file_parts[0]][lst_file_parts[1]] = {}\n",
    "            dict_gcm_other_realization[lst_file_parts[0]][lst_file_parts[1]][lst_file_parts[2]]=value_df      \n",
    "    \n",
    "    # Do rolling average and output with dictionary key as filename and values as rolling average dataframe.\n",
    "    df_rolling = pd.DataFrame\n",
    "    start_year = 1950\n",
    "    end_year = 2072\n",
    "    for key_gcm in dict_gcm_hist_realization:\n",
    "        for key_ssp in dict_gcm_hist_realization[key_gcm]:\n",
    "            for key_relization in dict_gcm_hist_realization[key_gcm][key_ssp]:\n",
    "                df_history = dict_gcm_hist_realization[key_gcm][key_ssp][key_relization]\n",
    "                for key_ssp_other in dict_gcm_other_realization[key_gcm]:\n",
    "                    df_ssp = dict_gcm_other_realization[key_gcm][key_ssp_other][key_relization]\n",
    "                    df_out = None\n",
    "                    df_rolling = pd.concat([df_history,df_ssp], axis=0)\n",
    "                    df_rolling.drop('time' , axis=1, inplace=True)  \n",
    "                    for currentYear in range(start_year,end_year):\n",
    "                        year_30 = currentYear + average_over_years\n",
    "                        df30year = df_rolling[(df_rolling['Year'].astype(int) >= currentYear) & (df_rolling['Year'].astype(int) < year_30)]\n",
    "                        dftemp = df30year.groupby(df30year.Month, as_index=False, sort=True)[['Pr (mm)','Tasmax (degC)','Tasmin (degC)','Tave (degC)']].mean().reset_index()\n",
    "                        dftemp.insert(loc=0,column=\"Year (30y start)\",value=currentYear)\n",
    "                        dftemp.insert(loc=0,column=\"Year Range\",value='%s-%s'%(currentYear,year_30-1))\n",
    "                        if df_out is None:\n",
    "                            df_out = dftemp.copy(deep=True)\n",
    "                        else:\n",
    "                            df_out = pd.concat([df_out,dftemp], axis=0)\n",
    "                    df_out.drop('index', axis=1, inplace=True) \n",
    "                    file_out = '%s_%s_%s_%s.csv'%(key_gcm,key_relization,key_ssp_other,append_name)\n",
    "                    dict_fil_rolling_df[file_out] = df_out\n",
    "    return dict_fil_rolling_df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501cd359-2f63-4723-ada5-9a759182adb2",
   "metadata": {},
   "source": [
    "The loop goes through all GCMs and writes the individual subbasin, the area weighted, and the flow weighted to results dictionaries with filname as the key and value equal to result dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0af3b2e-5cb6-495c-a786-7964323026bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.table_id.grid_label'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='1' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [1/1 00:01&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: LOCA2.UCSD.HadGEM3-GC31-LL.historical.day.d03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.9/site-packages/flox/aggregations.py:258: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed HadGEM3-GC31-LL_historical_r2i1p1f3_...\n",
      "\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.table_id.grid_label'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='1' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [1/1 00:00&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: LOCA2.UCSD.HadGEM3-GC31-LL.historical.day.d03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.9/site-packages/flox/aggregations.py:258: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed HadGEM3-GC31-LL_historical_r3i1p1f3_...\n",
      "\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.table_id.grid_label'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='1' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [1/1 00:00&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: LOCA2.UCSD.HadGEM3-GC31-LL.ssp585.day.d03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.9/site-packages/flox/aggregations.py:258: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed HadGEM3-GC31-LL_ssp585_r2i1p1f3_...\n",
      "\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.table_id.grid_label'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='1' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [1/1 00:00&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: LOCA2.UCSD.HadGEM3-GC31-LL.ssp585.day.d03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.9/site-packages/flox/aggregations.py:258: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed HadGEM3-GC31-LL_ssp585_r3i1p1f3_...\n",
      "\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.table_id.grid_label'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='1' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [1/1 00:00&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: LOCA2.UCSD.INM-CM5-0.historical.day.d03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.9/site-packages/flox/aggregations.py:258: RuntimeWarning: invalid value encountered in divide\n",
      "  finalize=lambda sum_, count: sum_ / count,\n"
     ]
    }
   ],
   "source": [
    "region_dict = get_region_dict()\n",
    "# main loop\n",
    "all_model_params = get_model_params(run_list_path)\n",
    "\n",
    "#Defile output dicts.\n",
    "results_dict = {}\n",
    "flow_weighted_results_dict = {}\n",
    "area_weighted_basin_results_dict = {}\n",
    "for model_params in all_model_params:\n",
    "    #Add masking to the dataset.\n",
    "    ds = load_dataset_with_mask(esm_datastore, model_params, mask_path,bbox)\n",
    "   \n",
    "    key = \"{}.{}.{}.{}.{}.{}\".format(\n",
    "        model_params['activity_id'],\n",
    "        model_params['institution_id'],\n",
    "        model_params['source_id'],\n",
    "        model_params['experiment_id'],\n",
    "        model_params['table_id'],\n",
    "        model_params['grid_label'],)\n",
    "    #Force load the dataset.\n",
    "    print('Loading: %s'%key)\n",
    "    ds = ds.compute()\n",
    "        \n",
    "    df_w = None\n",
    "    df_a = None\n",
    "    df_nw = None\n",
    "    for id_region, v in region_dict.items():     \n",
    "        # Get this regions results\n",
    "        df_n = get_df_map_mask(id_region,ds)\n",
    "        output_filename = get_output_file_name_monthly(model_params, '%s-19'%'{:02d}'.format(id_region))\n",
    "\n",
    "        df_out = df_n.drop('time' , axis=1)\n",
    "        results_dict[output_filename] = df_out\n",
    "                \n",
    "        # Get Area Weighted dataframe\n",
    "        weighting_factor = v['area_ratio']\n",
    "        df_weighted_a = get_weighted_dataframe(df_n.copy(deep=True),weighting_factor)\n",
    "        if df_a is None:\n",
    "            df_a = df_weighted_a.copy(deep=True)\n",
    "        else:\n",
    "            df_a = get_sum_dataframes(df_a,df_weighted_a)\n",
    "            \n",
    "        # Get Flow Weighted results \n",
    "        weighting_factor = v['flow_ratio']\n",
    "        df_weighted = get_weighted_dataframe(df_n.copy(deep=True),weighting_factor)\n",
    "        if df_w is None:\n",
    "            df_w = df_weighted.copy(deep=True)\n",
    "        else:\n",
    "            df_w = get_sum_dataframes(df_w,df_weighted)\n",
    "    print('Processed %s...'%get_output_file_name_monthly(model_params,'').replace('.csv',''))       \n",
    "    #Add weighted dataframes to output.\n",
    "    output_filename = get_output_file_name_monthly(model_params, \"19FlowWeighted\")\n",
    "    flow_weighted_results_dict[output_filename] = df_w\n",
    "    output_filename = get_output_file_name_monthly(model_params, \"19AreaWeighted\")\n",
    "    area_weighted_basin_results_dict[output_filename] = df_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a128fe-3ead-4a52-a601-e6e637c5b557",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write to ouput.\n",
    "zip_path = os.path.join(output_folder, file_zip)\n",
    "\n",
    "dict_rolling_flow_weighted = get_monthly_rolling_ave(flow_weighted_results_dict,30,'30yrAve')\n",
    "dict_rolling_area_weighted = get_monthly_rolling_ave(area_weighted_basin_results_dict,30,'30yrAve')\n",
    "\n",
    "with zipfile.ZipFile(zip_path, mode=\"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
    "    for k, v in results_dict.items():\n",
    "        text_stream = StringIO();\n",
    "        v.to_csv(text_stream, index=False)\n",
    "        fileout = dir_individual + '/' + k\n",
    "        zf.writestr(fileout, text_stream.getvalue())\n",
    "    for k, v in flow_weighted_results_dict.items():\n",
    "        fileout = dir_flow_weighted + '/' + k\n",
    "        fileout_raw = dir_flow_weighted + '/Raw/' + k\n",
    "        text_stream = StringIO();\n",
    "        v.to_csv(text_stream, index=False)\n",
    "        zf.writestr(fileout_raw, text_stream.getvalue())\n",
    "        v.drop('time' , axis=1, inplace=True)\n",
    "        text_stream = StringIO();\n",
    "        v.to_csv(text_stream, index=False)\n",
    "        zf.writestr(fileout, text_stream.getvalue())\n",
    "    for k, v in area_weighted_basin_results_dict.items():\n",
    "        fileout = dir_area_weighted + '/' + k\n",
    "        fileout_raw = dir_area_weighted + '/Raw/' + k\n",
    "        text_stream = StringIO();\n",
    "        v.to_csv(text_stream, index=False)\n",
    "        zf.writestr(fileout_raw, text_stream.getvalue())\n",
    "        v.drop('time' , axis=1, inplace=True)\n",
    "        text_stream = StringIO();\n",
    "        v.to_csv(text_stream, index=False)\n",
    "        zf.writestr(fileout, text_stream.getvalue())\n",
    "    for k, v in dict_rolling_area_weighted.items():\n",
    "        fileout = dir_area_weighted_rolling + '/' + k\n",
    "        text_stream = StringIO();\n",
    "        v.to_csv(text_stream, index=False)\n",
    "        zf.writestr(fileout, text_stream.getvalue())\n",
    "    for k, v in dict_rolling_flow_weighted.items():\n",
    "        fileout = dir_flow_weighted_rolling + '/' + k\n",
    "        text_stream = StringIO();\n",
    "        v.to_csv(text_stream, index=False)\n",
    "        zf.writestr(fileout, text_stream.getvalue())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee98484-2c9e-4768-ad92-c1dd039787d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
