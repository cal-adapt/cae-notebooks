{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d144d7c0-02c8-4b37-a357-a9c9f3112f08",
   "metadata": {},
   "source": [
    "## Calculating a Typical Meteorological Year\n",
    "\n",
    "This notebook walks through the process of calculating a [Typical Meteorological Year](https://nsrdb.nrel.gov/data-sets/tmy), an hourly dataset used for applications in energy and building systems modeling. Because this represents average rather than extreme conditions, an TMY dataset is not suited for designing systems to meet the worst-case conditions occurring at a location. \n",
    "\n",
    "The TMY methodology here mirrors that of the Sandia/NREL TMY3 methodology, and uses historic and projected downscaled climate data available through the Cal-Adapt: Analytics Engine catalog. As this methodology heavily weights the solar radiation input data, be aware that the final selection of \"typical\" months may not be typical for other variables. \n",
    "\n",
    "**Intended Application** As a user, I want to <span style=\"color:#FF0000\">**generate a typical meteorological year file**</span> for a location of interest:\n",
    "- Understand the methods that are involved in a TMY\n",
    "- Visualize the TMY across all input variables\n",
    "- Export the TMY for available models for input into my workflow\n",
    "\n",
    "**Note**: \n",
    "1. The Analytics Engine at present has an *Average Meteorological Year* functionality. The methods shown throughout this notebook will soon replace the underlying backend `climakitae` code for the AMY in order to better address our user needs, i.e., we are working to replace the AMY with the TMY methods. We provide this walkthrough to demonstrate confidence in the \"AMY to TMY\" conversion process for our users in the meantime. \n",
    "\n",
    "**Runtime**: With the default settings, this notebook takes approximately **30 minutes** to run from start to finish. Modifications to selections may increase the runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3a4e19-0c14-4bb8-b410-68f93cc7a2a4",
   "metadata": {},
   "source": [
    "### Step 0: Set-up\n",
    "\n",
    "Import the [climakitae](https://github.com/cal-adapt/climakitae) library and other dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915116c8-7f12-473e-baa2-656349be1070",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import climakitae as ck\n",
    "import climakitaegui as ckg\n",
    "import panel as pn\n",
    "pn.extension()\n",
    "\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pkg_resources\n",
    "from tqdm.auto import tqdm  # Progress bar\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from climakitae.util.utils import convert_to_local_time, get_closest_gridcell\n",
    "from climakitae.core.data_export import write_tmy_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bd29b6-4bd4-4e3e-864e-688480d8edb1",
   "metadata": {},
   "source": [
    "### Step 1: Grab and process all required input data\n",
    "\n",
    "The [TMY3 method](https://www.nrel.gov/docs/fy08osti/43156.pdf) selects a \"typical\" month based on ten daily variables: max, min, and mean air and dew point temperatures, max and mean wind speed, global irradiance and direct irradiance.  \n",
    "\n",
    "#### Step 1a: Select location of interest\n",
    "TMYs are usually calculated for a specific location of interest, like a building or power plant. Here, we will use a known weather station location, via their latitude and longitude to extract the data that we need to calculate the TMY.  In the example below, we will look specifically at Los Angeles International Airport, but will note in the code below how you can provide your own location coordinates too. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93a86a0-7a14-47c7-bf89-ee5c76a34aa2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read in station file of CA HadISD stations\n",
    "stn_file = pkg_resources.resource_filename(\"climakitae\", \"data/hadisd_stations.csv\")\n",
    "stn_file = pd.read_csv(stn_file, index_col=[0])\n",
    "stn_file.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7e25d5-a9da-4605-837d-2e93b0f77c45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# grab airport\n",
    "stn_name = \"Los Angeles International Airport (KLAX)\"\n",
    "stn_code = stn_file.loc[stn_file['station'] == stn_name]['station id'].item()\n",
    "one_station = stn_file.loc[stn_file['station'] == stn_name]\n",
    "stn_lat = one_station.LAT_Y.item()\n",
    "stn_lon = one_station.LON_X.item()\n",
    "stn_state = one_station.state.item()\n",
    "stn_lat, stn_lon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba66963-c12f-4b47-8e37-b532fe8e38a0",
   "metadata": {},
   "source": [
    "Alternatively, you may want to provide your own location instead of one of the HadISD stations above. If so, uncomment the cell below by removing the `#` symbol and modifying the lines of code. Note, with custom locations, if an elevation is not provided, a default value of 0.0 m will be used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f7f107-2221-4e95-be18-8e2db5dc3850",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## provide your own location, via latitude and longitude coordinates\n",
    "# stn_lat = YOUR_LAT_HERE\n",
    "# stn_lon = YOUR_LON_HERE\n",
    "# stn_state = 'YOUR_STATE_HERE'\n",
    "# stn_name = 'YOUR_STATION_NAME_HERE'\n",
    "# stn_code = 'custom' \n",
    "# stn_elev = YOUR_ELEV_HERE # meters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15867d87-1951-4ec5-a503-35a61516810f",
   "metadata": {},
   "source": [
    "#### Step 1b: Variables in catalog\n",
    "When selecting data, there are several considerations to be aware of. The recommended minimum input of data is 15-20 years worth of daily data. First, we will pre-load some data options to ensure that the same constraints are applied to every variable we retrieve from the catalog and calculate. It is important to note that not all models in the Cal-Adapt: Analytics Engine have the solar variables critical for TMY file generation - in fact, only 4 do! We will carefully subset our variables to ensure that the same 4 models are selected for consistency. We will also process the data for our designated station location (latitude, and longitude) at 9 km over the 1990-2020 period. For data post-2014, we will utilize SSP 3-7.0, although scenario selection in the near-future is relatively independent. If calculating a TMY for the far-future, **carefully consider which scenario SSP to include**, as there will be **significant** differences present. \n",
    "\n",
    "Lastly, because the dynamically downscaled WRF data in the Cal-Adapt: Analytics Engine is in UTC time, we'll convert to the timezone of the station we've selected. This is particularly important for the timing of solar radiation max and min, which is a critical piece of a TMY. The handy `convert_to_local_time` function corrects for this, and ensures that all data within the same daily timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57645bd8-517c-4de8-95fa-a565b66d0cb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "selections = ckg.Select()\n",
    "\n",
    "# default selections applicable to all variables selected\n",
    "selections.data_type = \"Gridded\"\n",
    "selections.area_average = \"Yes\"\n",
    "selections.scenario_historical = [\"Historical Climate\"]\n",
    "selections.scenario_ssp = [\"SSP 3-7.0\"] # Important based on time period considered!!\n",
    "selections.time_slice = (1990, 2020)\n",
    "selections.timescale = \"hourly\"\n",
    "selections.resolution = \"9 km\"\n",
    "selections.cached_area = ['coordinate selection']\n",
    "selections.latitude = (stn_lat-0.08, stn_lat+0.08)\n",
    "selections.longitude = (stn_lon-0.08, stn_lon+0.08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1efd48-d816-41d3-9c11-a2ac65699373",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# selected models\n",
    "data_models = ['WRF_EC-Earth3_r1i1p1f1', 'WRF_MPI-ESM1-2-HR_r3i1p1f1','WRF_TaiESM1_r1i1p1f1', 'WRF_MIROC6_r1i1p1f1']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0799c331-5024-484d-beb2-cf97333aed9b",
   "metadata": {},
   "source": [
    "Now that we have set up default settings, let's start retrieving data! We will need to calculate summary statistics and reduce to daily timescales for the following variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122c91ab-8946-4bca-a0dc-12836581b722",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# air temperature\n",
    "selections.variable = \"Air Temperature at 2m\"\n",
    "selections.units = \"degC\"\n",
    "temp_data = selections.retrieve()\n",
    "temp_data = convert_to_local_time(temp_data, selections) # convert to local timezone\n",
    "temp_data = temp_data.sel(simulation = data_models)\n",
    "\n",
    "# max air temp\n",
    "max_airtemp_data = temp_data.resample(time=\"1D\").max() # daily max air temp\n",
    "max_airtemp_data.name = \"Daily max air temperature\" # rename for clarity\n",
    "\n",
    "# min air temp\n",
    "min_airtemp_data = temp_data.resample(time=\"1D\").min() # daily min air temp\n",
    "min_airtemp_data.name = \"Daily min air temperature\" # rename for clarity\n",
    "\n",
    "# mean air temp\n",
    "mean_airtemp_data = temp_data.resample(time=\"1D\").mean() # daily mean air temp\n",
    "mean_airtemp_data.name = \"Daily mean air temperature\" # rename for clarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868fff96-9508-4435-932b-bce39a02427a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-11T19:09:40.599835Z",
     "iopub.status.busy": "2023-05-11T19:09:40.599455Z",
     "iopub.status.idle": "2023-05-11T19:09:44.067646Z",
     "shell.execute_reply": "2023-05-11T19:09:44.066937Z",
     "shell.execute_reply.started": "2023-05-11T19:09:40.599805Z"
    }
   },
   "source": [
    "Retrieve and calculate max and mean wind speed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee63c6e-48b0-47bd-9903-2d4862c91ee0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# wind speed\n",
    "selections.variable = \"Wind speed at 10m\"\n",
    "selections.units = \"m s-1\"\n",
    "ws_data = selections.retrieve()\n",
    "ws_data = convert_to_local_time(ws_data, selections) # convert to local timezone\n",
    "ws_data = ws_data.sel(simulation = data_models)\n",
    "\n",
    "# max wind speed\n",
    "max_windspd_data = ws_data.resample(time=\"1D\").max() # daily max wind speed\n",
    "max_windspd_data.name = \"Daily max wind speed\" # rename for clarity\n",
    "\n",
    "# mean wind speed\n",
    "mean_windspd_data = ws_data.resample(time=\"1D\").mean() # daily mean wind speed\n",
    "mean_windspd_data.name = \"Daily mean wind speed\" # rename for clarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5768520-77e1-45b3-92f8-ab7c3a9f7932",
   "metadata": {},
   "source": [
    "Retrieve and calculate max, min, and mean dew point temperature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e54570e-f44b-4eeb-87f9-8d1128cbb1c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dew point temperature\n",
    "selections.variable = \"Dew point temperature\"\n",
    "selections.units = \"degC\"\n",
    "dewpt_data = selections.retrieve()\n",
    "dewpt_data = convert_to_local_time(dewpt_data, selections) # convert to local timezone\n",
    "dewpt_data = dewpt_data.sel(simulation = data_models)\n",
    "\n",
    "# max dew point\n",
    "max_dewpt_data = dewpt_data.resample(time=\"1D\").max() # daily max dewpoint temp\n",
    "max_dewpt_data.name = \"Daily max dewpoint temperature\" # rename for clarity\n",
    "\n",
    "# min dew point\n",
    "min_dewpt_data = dewpt_data.resample(time=\"1D\").min() # daily min dewpoint temp\n",
    "min_dewpt_data.name = \"Daily min dewpoint temperature\" # rename for clarity\n",
    "\n",
    "# mean dew point\n",
    "mean_dewpt_data = dewpt_data.resample(time=\"1D\").mean() # daily mean dewpoint temp\n",
    "mean_dewpt_data.name = \"Daily mean dewpoint temperature\" # rename for clarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2421571f-fcac-44fa-aedb-b3f8e77bfa3c",
   "metadata": {},
   "source": [
    "Next, retrieve global horizontal irradiance. GHI is within the Analytics Engine catalog at daily resolutions, but for the TMY methodology, we need to calculate the total accumulated GHI received over the course of the day, so we will retrieve hourly data instead and calculate the necessary information below. The same process is repeated for the direct normal irradiance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31adc629-3205-412d-9274-2c447ce21621",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# global irradiance\n",
    "selections.variable = \"Instantaneous downwelling shortwave flux at bottom\"\n",
    "global_irradiance_data = selections.retrieve()\n",
    "global_irradiance_data = convert_to_local_time(global_irradiance_data, selections) # convert to local timezone\n",
    "global_irradiance_data = global_irradiance_data.sel(simulation = data_models)\n",
    "\n",
    "global_irradiance_data.name = \"Global horizontal irradiance\" # rename for clarity\n",
    "total_ghi_data = global_irradiance_data.resample(time=\"1D\").sum() # total global horizontal irradiance (accumulation of hourly data over a 24-hour period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2fd1f3-fbee-4dd6-9ad9-61a9c925cd25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# direct normal irradiance\n",
    "selections.variable = \"Shortwave surface downward direct normal irradiance\"\n",
    "direct_irradiance_data = selections.retrieve()\n",
    "direct_irradiance_data = convert_to_local_time(direct_irradiance_data, selections) # convert to local timezone\n",
    "direct_irradiance_data = direct_irradiance_data.sel(simulation = data_models)\n",
    "\n",
    "direct_irradiance_data.name = \"Direct normal irradiance\" # rename for clarity\n",
    "total_dni_data = direct_irradiance_data.resample(time=\"1D\").sum() # total direct normal irradiance (accumulation of hourly data over a 24-hour period)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf2ea23-b6f9-4665-be6e-0dc5bf5da726",
   "metadata": {},
   "source": [
    "#### Step 1d: Load all variables\n",
    "Now that we have all of our data retrieved and calculated, it is time to actually load the data into memory. Previously, xarray has lazily loaded the data, but we will actually grab it now. This will take approximately **7 minutes**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6806e4f2-2828-4e1a-9f22-0ae1aac9dfca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_vars = xr.merge([max_airtemp_data.squeeze(), min_airtemp_data.squeeze(), mean_airtemp_data.squeeze(),\n",
    "                     max_dewpt_data.squeeze(), min_dewpt_data.squeeze(), mean_dewpt_data.squeeze(),\n",
    "                     max_windspd_data.squeeze(), mean_windspd_data.squeeze(),\n",
    "                     total_ghi_data.squeeze(), total_dni_data.squeeze()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39f1e1f-c0da-405b-848d-8f584c3fa804",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load all indices in\n",
    "all_vars = all_vars.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b65ebb8-f730-4ab3-9eb4-4642624b4a59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccfed5a-4a71-4ff3-be45-d20565d44cf4",
   "metadata": {},
   "source": [
    "### Step 2: Calculate cumulative distribution functions\n",
    "\n",
    "For the TMY, the cumulative distribution function gives the proportion of values that are less than or equal to a specified value of the index. In this case, we want to identify months that are as close to the long-term climatology for each variable as possible, indicating months that are \"typical\".  \n",
    "\n",
    "#### Step 2a: Calculate long-term climatology CDFs for each index\n",
    "First, we need to calculate the long-term climatology for each index for each month so we can establish the baseline pattern. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e54317-35e5-4465-93a7-9cd4772f11a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_cdf(da): \n",
    "    \"\"\"Compute the cumulative density function for an input DataArray\"\"\"\n",
    "    da_np = da.values # Get numpy array of values\n",
    "    num_samples = 1024 # Number of samples to generate\n",
    "    count, bins_count = np.histogram( # Create a numpy histogram of the values \n",
    "        da_np, bins = np.linspace(\n",
    "            da_np.min(), # Start at the minimum value of the array \n",
    "            da_np.max(), # End at the maximum value of the array \n",
    "            num_samples\n",
    "        )\n",
    "    )\n",
    "    cdf_np = np.cumsum(count/sum(count)) # Compute the CDF \n",
    "    \n",
    "    # Turn the CDF array into xarray DataArray \n",
    "    # New dimension is the bin values \n",
    "    cdf_da = xr.DataArray( \n",
    "        [bins_count[1:],cdf_np],\n",
    "        dims=[\"data\",\"bin_number\"],\n",
    "        coords = {\n",
    "            \"data\":[\"bins\",\"probability\"],\n",
    "        }\n",
    "    )\n",
    "    cdf_da.name = da.name\n",
    "    return cdf_da\n",
    "\n",
    "def get_cdf_by_sim(da): \n",
    "    # Group the DataArray by simulation\n",
    "    return da.groupby(\"simulation\").apply(compute_cdf)\n",
    "\n",
    "def get_cdf_by_mon_and_sim(da): \n",
    "    # Group the DataArray by month in the year \n",
    "    return da.groupby(\"time.month\").apply(get_cdf_by_sim)\n",
    "\n",
    "def get_cdf(ds): \n",
    "    \"\"\"Get the cumulative density function. \n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    ds: xr.Dataset\n",
    "        Input data to compute CDF for\n",
    "    Returns\n",
    "    -------\n",
    "    xr.Dataset\n",
    "    \"\"\"\n",
    "    return ds.apply(get_cdf_by_mon_and_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b852f3a-5b9e-42b8-9224-ee5c17341456",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cdf_climatology = get_cdf(all_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff77894a-c554-4407-8153-41ab7115687c",
   "metadata": {},
   "source": [
    "Next, we set up a handy plotting function so that we can view the CDF patterns for each variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24608790-baed-408f-897f-2d2c2a034ce4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_one_var_cdf(cdf_da): \n",
    "    \"\"\"Plot CDF for a single variable \n",
    "    Written to function for the unique configuration of the CDF DataArray object\n",
    "    Silences an annoying hvplot warning \n",
    "    Will show every simulation together on the plot \n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    cdf: xr.DataArray \n",
    "       Cumulative density function for a single variable \n",
    "    \n",
    "    Returns \n",
    "    -------\n",
    "    panel.layout.base.Column\n",
    "        Hvplot lineplot \n",
    "        \n",
    "    \"\"\"\n",
    "    prob_da = cdf_da.sel(data=\"probability\",drop=True).rename(\"probability\") # Grab only probability da \n",
    "    bins_da = cdf_da.sel(data=\"bins\",drop=True).rename(\"bins\") # Grab just bin values\n",
    "    ds = xr.merge([prob_da,bins_da]) # Merge the two to form a single Dataset object\n",
    "    cdf_pl = ds.hvplot(\n",
    "        \"bins\",\"probability\", \n",
    "        by=\"simulation\", # Simulations should all be displayed together \n",
    "        widget_location=\"bottom\",\n",
    "        grid=True,\n",
    "        xlabel=\"{0} ({1})\".format(var,cdf_da.attrs[\"units\"]), \n",
    "        xlim=(bins_da.min().item(), bins_da.max().item()), # Fix the x-limits for all months\n",
    "        ylabel=\"Probability (0-1)\", \n",
    "    )\n",
    "    return cdf_pl "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87af453d-6f91-4668-804f-7264d2929e58",
   "metadata": {},
   "source": [
    "In the plot below, we'll display maximum air temperature to assess the climatological CDF pattern, but you can modify the variable here to one of your choosing to see the pattern too! Also select a different month by moving the slider bar to see the pattern throughout the year. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca17cbe-1b58-4767-978c-ddf91056e45d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Choose your desired variable\n",
    "var = \"Daily max air temperature\"\n",
    "\n",
    "# Make the plot\n",
    "cdf_plot = plot_one_var_cdf(cdf_climatology[var])\n",
    "display(cdf_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe84c83b-e764-4bdd-8a6f-8ab67016ca97",
   "metadata": {},
   "source": [
    "#### Step 2b: Calculate CDFs for each index for all months\n",
    "\n",
    "Next, we will calculate CDF for each month and each variable, for which we ultimately will compare against climatology. For the individual months, we must also exclude the period of time during a major volcanic eruption (Pinatubo: June 1991 to December 1994) as the aerosols have an impact on solar variables. The cells below functions exclude this data from our data next. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79db16c3-26be-411a-a409-da1b6985958f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_cdf_monthly(ds):\n",
    "    \"\"\"Get the cumulative density function by unique mon-yr combos\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    ds: xr.Dataset\n",
    "        Input data to compute CDF for\n",
    "    Returns\n",
    "    -------\n",
    "    xr.Dataset\n",
    "    \"\"\"\n",
    "    def get_cdf_mon_yr(da): \n",
    "        return da.groupby(\"time.year\").apply(get_cdf_by_mon_and_sim)\n",
    "    return ds.apply(get_cdf_mon_yr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826631ec-f25b-42dc-9928-6a0d5ed4f376",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cdf_monthly = get_cdf_monthly(all_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881f5728-de7b-4d21-8622-e39437b64dbd",
   "metadata": {},
   "source": [
    "Now we'll remove the volcanic years: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dabb491-0c8b-48a7-b519-bf135fceaf9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remove the years for the Pinatubo eruption \n",
    "cdf_monthly = cdf_monthly.where(\n",
    "    (~cdf_monthly.year.isin([1991,1992,1993,1994])), \n",
    "    np.nan, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f526746-efce-48ec-ae3c-e86359097ac2",
   "metadata": {},
   "source": [
    "Like the climatology CDF figure above, let's check out the individual months next. You can modify the variable, and month-year to display too. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57e582a-e24f-41bc-8da7-6c8567338240",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Choose your desired variable\n",
    "var = \"Daily max air temperature\"\n",
    "\n",
    "# Make the plot\n",
    "cdf_plot_mon_yr = plot_one_var_cdf(cdf_monthly[var])\n",
    "display(cdf_plot_mon_yr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad80428-e476-408a-927a-ddb33f801c37",
   "metadata": {},
   "source": [
    "### Step 3: Compare climatology CDF to monthly CDF for each variable\n",
    "\n",
    "Now that we have the distributions for the long-term climatology of our 30-year period, and the corresponding distribution for each month in that 30-year period, we need to assess how each individual month compares to the long-term climatology. In essence, we are looking for the individual months that best capture the climatology distribution. \n",
    "\n",
    "#### Step 3a: Calculate the Finkelstein-Schafer statistic \n",
    "The [Finkelstein-Schafer statistic](https://academic.oup.com/biomet/article-abstract/58/3/641/233677) determines the absolute difference between the long-term climatology and candidate CDF profiles, and considers the number of days within each month. We will use a handy function `fs_statistic` to calculate this below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b931f7-6733-415c-8f6d-1d44a976910d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fs_statistic(cdf_climatology, cdf_month):\n",
    "    \"\"\"\n",
    "    Calculates the Finkelstein-Schafer statistic:\n",
    "    Absolute difference between long-term climatology and candidate CDF, divided by number of days in month\n",
    "    \"\"\"\n",
    "    days_per_mon = xr.DataArray(\n",
    "        data=[31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31], \n",
    "        coords={\"month\":np.arange(1,13)}\n",
    "    )\n",
    "    fs_stat = abs(cdf_monthly - cdf_climatology).sel(data=\"probability\") / days_per_mon\n",
    "    return fs_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dab26fd-1c1c-4122-9158-4f35035dfe6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_vars_fs = fs_statistic(cdf_climatology, cdf_monthly)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85fb8c2-2e5f-48eb-bfea-9fe0d3fc37e2",
   "metadata": {},
   "source": [
    "#### Step 3b: Weight the F-S statistic\n",
    "\n",
    "Next, we weight the F-S statistic results based on the input variables. The [TMY3](https://www.nrel.gov/docs/fy08osti/43156.pdf) method places a higher weight on the solar variables (global irradiance and direct irradiance), which we follow here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178e4fd1-6297-4321-9186-ae5e57e5254b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_weighted_fs(da_fs):\n",
    "    \"\"\"Weights the F-S statistics based on TMY3 methodology\"\"\"\n",
    "    weights_per_var = {\n",
    "        'Daily max air temperature':1/20,\n",
    "        'Daily min air temperature':1/20,\n",
    "        'Daily mean air temperature':2/20,\n",
    "        'Daily max dewpoint temperature':1/20,\n",
    "        'Daily min dewpoint temperature':1/20,\n",
    "        'Daily mean dewpoint temperature':2/20,\n",
    "        'Daily max wind speed':1/20,\n",
    "        'Daily mean wind speed':1/20,\n",
    "        'Global horizontal irradiance':5/20,\n",
    "        'Direct normal irradiance':5/20 \n",
    "    }\n",
    "\n",
    "    for var, weight in weights_per_var.items(): \n",
    "        # Multiply each variable by it's appropriate weight \n",
    "        da_fs[var] = da_fs[var]*weight \n",
    "    return da_fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d48c015-b629-4504-8913-02b8ec9544df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "weighted_fs = compute_weighted_fs(all_vars_fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3711568f-3622-4103-89e9-a908d545a569",
   "metadata": {},
   "source": [
    "#### Step 3c: Select candidate months for consideration\n",
    "Once weighted, we select the top candidate months for each month that have lowest weighted sums, meaning that these candidate months are the closest to the long-term climatology for that month. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1145ea68-735c-46c7-8f5e-bc314c900248",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sum \n",
    "weighted_fs_sum = weighted_fs.to_array().sum(dim=[\"variable\",\"bin_number\"]).drop([\"data\"])\n",
    "\n",
    "# Pass the weighted F-S sum data for simplicity\n",
    "ds = weighted_fs_sum\n",
    "\n",
    "df_list = []\n",
    "num_values = 1 # Selecting the top value for now, persistence statistics calls for top 5\n",
    "for sim in ds.simulation.values: \n",
    "    for mon in ds.month.values:\n",
    "        da_i = ds.sel(month=mon, simulation=sim)\n",
    "        top_xr = da_i.sortby(da_i, ascending=True)[:num_values].expand_dims([\"month\",\"simulation\"])\n",
    "        top_df_i = top_xr.to_dataframe(name=\"top_values\")\n",
    "        df_list.append(top_df_i)\n",
    "        \n",
    "# Concatenate list together for all months and simulations\n",
    "top_df = pd.concat(df_list).drop(columns=[\"top_values\"]).reset_index()\n",
    "top_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ddeab6-c43b-42ca-ad8e-396cb90fce49",
   "metadata": {},
   "source": [
    "The data table above represents the ideal months that represent the long term climatology based on the 10 indices for the 4 simulations in the Analytics Engine catalog. Meaning, for a \"typical\" meteorological year, WRF_EC-Earth3 data for Jan will come from Jan 2017, data for Feb will come from 2018, and so on. In the next step, we will generate the resulting \"TMY\" file that is commonly used in such applications. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd2214b-0368-49a6-bf71-f17916a7f860",
   "metadata": {},
   "source": [
    "### Step 4: Generate the TMY data outputs\n",
    "\n",
    "Generally, the following data is outputted using the TMY months:\n",
    "- Date & time (UTC)\n",
    "- Air temperature at 2m [°C]\n",
    "- Dew point temperature [°C]\n",
    "- Relative humidity [%]\n",
    "- Global horizontal irradiance [W/m2]\n",
    "- Direct normal irradiance [W/m2]\n",
    "- Diffuse horizontal irradiance [W/m2]\n",
    "- Downwelling infrared radiation [W/m2]\n",
    "- Wind speed at 10m [m/s]\n",
    "- Wind direction at 10m [°]\n",
    "- Surface air pressure [Pa]\n",
    "\n",
    "The following function will retrieve all of this data for the designated TMY month and concatenate it together into a pandas dataframe so that we can export it as a csv file. We'll do this next. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7cb538-f892-42cc-b6cf-b5cfa05c7f68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_tmy_data(top_df): \n",
    "    \"\"\"Generate typical meteorological year data \n",
    "    Output will be a list of dataframes per simulation. \n",
    "    Print statements throughout the function indicate to the user the progress of the computatioconvert_to_local_time   Parameters\n",
    "    -----------\n",
    "    top_df: pd.DataFrame \n",
    "        Table with column values month, simulation, and year \n",
    "        Each month-sim-yr combo represents the top candidate that has the lowest weighted sum from the FS statistic \n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    dict of str:pd.DataFrame \n",
    "        Dictionary in the format of {simulation:TMY corresponding to that simulation} \n",
    "    \n",
    "    \"\"\"\n",
    "    ## ================== GET DATA FROM CATALOG ==================\n",
    "    vars_and_units = {\n",
    "        'Air Temperature at 2m':'degC',\n",
    "        'Dew point temperature':'degC',\n",
    "        'Relative humidity':'[0 to 100]', \n",
    "        'Instantaneous downwelling shortwave flux at bottom':'W/m2',\n",
    "        'Shortwave surface downward direct normal irradiance':'W/m2',  \n",
    "        'Shortwave surface downward diffuse irradiance':'W/m2',\n",
    "        'Instantaneous downwelling longwave flux at bottom':'W/m2',\n",
    "        'Wind speed at 10m':'m s-1',\n",
    "        'Wind direction at 10m':'degrees',\n",
    "        'Surface Pressure':'Pa'\n",
    "    }\n",
    "    \n",
    "    # Set up shared catalog access settings\n",
    "    selections.data_type = \"Gridded\"\n",
    "    selections.area_average = \"No\"\n",
    "    selections.scenario_historical = [\"Historical Climate\"]\n",
    "    selections.scenario_ssp = [\"SSP 3-7.0\"] # Important based on time period considered!!\n",
    "    selections.timescale = \"hourly\"\n",
    "    selections.resolution = \"9 km\"\n",
    "    selections.cached_area = ['coordinate selection']\n",
    "    selections.latitude = (stn_lat-0.08, stn_lat+0.08)\n",
    "    selections.longitude = (stn_lon-0.08, stn_lon+0.08)\n",
    "    selections.time_slice = (1990, 2020)\n",
    "\n",
    "    # Loop through each variable and grab data from catalog \n",
    "    all_vars_list = []\n",
    "    print(\"STEP 1: RETRIEVING HOURLY DATA FROM CATALOG\\n\")\n",
    "    for var, units in vars_and_units.items(): \n",
    "        print(\"Retrieving data for {0}\".format(var), end=\"... \")\n",
    "        selections.variable = var # Set variable\n",
    "        selections.units = units # Set units  \n",
    "        data_by_var = selections.retrieve() # Retrieve data from catalog\n",
    "        data_by_var = convert_to_local_time(data_by_var, selections) # convert to local timezone\n",
    "        data_by_var = get_closest_gridcell(data_by_var, stn_lat, stn_lon, print_coords=False) # retrieve only closest gridcell\n",
    "        data_by_var = data_by_var.sel(simulation = data_models) # Subset for only the models that have solar variables\n",
    "\n",
    "        # Drop unwanted coords\n",
    "        data_by_var = data_by_var.squeeze().drop(['lakemask','landmask','x','y','Lambert_Conformal'])\n",
    "\n",
    "        all_vars_list.append(data_by_var) # Append to list \n",
    "        print(\"complete!\")\n",
    "\n",
    "    # Merge data from all variables into a single xr.Dataset object \n",
    "    all_vars_ds = xr.merge(all_vars_list)\n",
    "\n",
    "\n",
    "    ## ================== CONSTRUCT TMY ==================\n",
    "    print(\"\\nSTEP 2: CALCULATING TYPICAL METEOROLOGICAL YEAR PER MODEL SIMULATION\\nProgress bar shows code looping through each month in the year.\\n\")\n",
    "    tmy_df_all = {}\n",
    "    for sim in all_vars_ds.simulation.values: \n",
    "        df_list = []\n",
    "        print(\"Calculating TMY for simulation: {0}\".format(sim))\n",
    "        for mon in tqdm(np.arange(1,13,1)): \n",
    "            # Get year corresponding to month and simulation combo \n",
    "            year = top_df.loc[(top_df[\"month\"] == mon) & (top_df[\"simulation\"] == sim)].year.item()\n",
    "\n",
    "            # Select data for unique month, year, and simulation\n",
    "            data_at_stn_mon_sim_yr = all_vars_ds.sel(simulation=sim, time=\"{0}-{1}\".format(mon, year)).expand_dims(\"simulation\")\n",
    "\n",
    "            # Reformat as dataframe \n",
    "            df_by_mon_sim_yr = data_at_stn_mon_sim_yr.to_dataframe()\n",
    "            df_by_mon_sim_yr = df_by_mon_sim_yr.reset_index()\n",
    "\n",
    "            # Reformat time index to remove seconds\n",
    "            df_by_mon_sim_yr[\"time\"] = pd.to_datetime(df_by_mon_sim_yr[\"time\"].values).strftime(\"%Y-%m-%d %H:%M\")\n",
    "            df_list.append(df_by_mon_sim_yr)\n",
    "\n",
    "        # Concatenate all DataFrames together \n",
    "        tmy_df_by_sim = pd.concat(df_list)\n",
    "        tmy_df_all[sim] = tmy_df_by_sim\n",
    "        \n",
    "    return tmy_df_all # Return dict of TMY by simulation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e792e895-ac9f-4bcf-8c56-9fa0c9e6597a",
   "metadata": {},
   "source": [
    "In the next cell, we will run the `generate_tmy_data` function which will retrieve, subset, and format the data for each month according to the TMY months for all requested variables. We have included print statements so you can watch the progress for each variable in each month as it builds. \n",
    "\n",
    "<span style=\"color:#FF0000\">**Note**: <span style=\"color:#000000\"> This will take time! On the Analytics Engine JupyterHub, this takes approximately 22 minutes. Progress bars will indicate the status of generating the TMY data for each simulation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8e87db-afc8-4bb2-a653-1f735b02449b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tmy_data_to_export = generate_tmy_data(top_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25572b6-2e07-442f-9ea3-90825943ad54",
   "metadata": {},
   "source": [
    "Let's observe what the TMY data looks like for one of the simulations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272fe2e0-7f10-4e8a-9c2e-fde2cd9f7cf7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "simulation = \"WRF_MPI-ESM1-2-HR_r3i1p1f1\"\n",
    "tmy_data_to_export[simulation].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9951a416-b2bf-4924-84b2-f5fd409913bc",
   "metadata": {},
   "source": [
    "Next, we visualize the TMY data itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ceebfa-a927-43e3-8bb9-28ee700d1c27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tmy_data_to_export[simulation].plot(x='time',y=['Air Temperature at 2m',\n",
    "                                                'Dew point temperature',\n",
    "                                                'Relative humidity',\n",
    "                                                'Instantaneous downwelling shortwave flux at bottom',\n",
    "                                                'Shortwave surface downward direct normal irradiance',\n",
    "                                                'Shortwave surface downward diffuse irradiance',\n",
    "                                                'Instantaneous downwelling longwave flux at bottom',\n",
    "                                                'Wind speed at 10m', 'Wind direction at 10m', 'Surface Pressure'], \n",
    "                                    title='Typical Meteorological Year ({})'.format(simulation),\n",
    "                                    subplots=True, figsize=(10,8), legend=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab490aa2-3b4e-428a-8740-1f174b7c55f5",
   "metadata": {},
   "source": [
    "Lastly, let's export the TMY data below as csv files. There will be a file per simulation downloaded. When utilizing TMY data in your own workflows, we recommend that **all simulations are considered** in your analyses, especially for future scenarios. Note, if you are working with a custom location, please also provide the optional `stn_elev` argument to `write_tmy_file`; if no elevation value is provided, an elevation value of 0.0 is set as the default. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1388c1c3-f141-4ff4-8155-79fc606a0854",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for sim, tmy in tmy_data_to_export.items():\n",
    "    filename = 'TMY_{0}_{1}'.format(stn_name.replace(\" \", \"_\").replace(\"(\",\"\").replace(\")\",\"\"), sim).lower()\n",
    "    write_tmy_file(filename, tmy_data_to_export[sim], stn_name, stn_code, stn_lat, stn_lon, stn_state, file_ext=\"epw\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
