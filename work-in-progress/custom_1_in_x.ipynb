{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df6a7be8-d390-4951-8e9d-78bc80231f3f",
   "metadata": {},
   "source": [
    "# Custom 1-in-X Analysis for Effective Temperature\n",
    "\n",
    "## Table of Contents\n",
    "- [Overview](#overview)\n",
    "- [Key Features](#key-features)\n",
    "- [Requirements](#requirements)\n",
    "- [Configuration](#configuration)\n",
    "- [Implementation Details](#implementation-details)\n",
    "- [Usage Examples](#usage-examples)\n",
    "- [Performance Considerations](#performance-considerations)\n",
    "- [Troubleshooting](#troubleshooting)\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates how to calculate 1-in-X year return values for custom metrics (like Effective Temperature) using extreme value analysis.\n",
    "\n",
    "The workflow follows `cava_data()` patterns from `climakitae.explore.vulnerability` with \n",
    "a specially created function called `calculate_1_in_X_custom()`.\n",
    "\n",
    "The function is currently designed to take in an xr.Dataset and a pd.DataFrame of (lat, lon)\n",
    "locations at which to calculate the 1-in-X values.\n",
    "\n",
    "> [!WARNING]\n",
    "> **Thread Safety**: `DataInterface` does not play nicely with multi-threading. If you want to multi-thread your calls to `get_data()`, please use code cell 2 to monkey patch the `__init__()` method.\n",
    "\n",
    "> [!TIP]\n",
    "> For quiet output, set `VERBOSE = False` in the configuration section.\n",
    "\n",
    "> [!IMPORTANT]\n",
    "> This notebook is optimized for speed over precision. Bootstrap samples are set to 0, which means confidence intervals are not representative. Increase `bootstrap_runs` in `_process_one_simulation()` for accurate confidence intervals.\n",
    "\n",
    "## Requirements\n",
    "\n",
    "### Python Packages\n",
    "- `climakitae` (latest version)\n",
    "- `xarray`\n",
    "- `pandas`\n",
    "- `numpy`\n",
    "- `concurrent.futures` (for parallelization)\n",
    "\n",
    "### System Requirements\n",
    "- **Memory**: ~5 GB RAM per worker\n",
    "- **Recommended**: JupyterHub with 30+ GB RAM for parallel processing\n",
    "- **Runtime**: ~3.5 hours per 50 locations per worker\n",
    "\n",
    "### Custom Metric\n",
    "\n",
    "The custom metric used as the example in this notebook is effective temperature calculated \n",
    "as:  \n",
    "`Teff = 0.7*Tmax0 + 0.003*Tmin0*Tmax1 + 0.002*Tmin1*Tmax2`\n",
    "\n",
    "### Parallelization\n",
    "\n",
    "`DataInterface` does not play nicely with multi-threading, if you want to multi-thread your calls to `get_data()` please use code cell 2 to monkey patch the `__init__()` method or update your version of `climakitae` by pulling from the main repo on GitHub.\n",
    "\n",
    "### Run Time & Memory\n",
    "\n",
    "On a JupyterHub machine with 30 GB RAM, I can have a maximum of 4 or 5 workers since each\n",
    "worker uses about 5 GB ram. Each worker takes around 3.5 hours to finish a batch of 50 locations.\n",
    "The testing location size is ~1600 meaning roughly 32 batches. Since there are 4 batches\n",
    "running at a time this results in a total run time of 28 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c510f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T16:38:25.879657Z",
     "iopub.status.busy": "2025-10-28T16:38:25.879402Z",
     "iopub.status.idle": "2025-10-28T16:38:41.176625Z",
     "shell.execute_reply": "2025-10-28T16:38:41.175982Z",
     "shell.execute_reply.started": "2025-10-28T16:38:25.879637Z"
    }
   },
   "outputs": [],
   "source": [
    "import concurrent\n",
    "import traceback\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from climakitae.core.data_interface import get_data, DataInterface\n",
    "from climakitae.core.data_export import export\n",
    "\n",
    "# Import functions for 1-in-X event calculations\n",
    "from climakitae.explore.threshold_tools import (\n",
    "    get_block_maxima,\n",
    "    get_ks_stat,\n",
    "    get_return_value,\n",
    ")\n",
    "from climakitae.core.constants import UNSET\n",
    "from climakitae.util.utils import add_dummy_time_to_wl, get_closest_gridcells\n",
    "\n",
    "from lat_lons import lat_lons\n",
    "\n",
    "VERBOSE = True\n",
    "\n",
    "data_interface = DataInterface() # create to avoid race conditions\n",
    "# Verify everything loaded correctly\n",
    "if VERBOSE:\n",
    "    print(\"✓ DataInterface initialized successfully!\")\n",
    "    print(f\"  Data catalog loaded: {len(data_interface.data_catalog.df)} entries\")\n",
    "    print(f\"  Variable descriptions loaded: {len(data_interface.variable_descriptions)} variables\")\n",
    "    print(f\"  Stations loaded: {len(data_interface.stations)} stations\")\n",
    "    print(f\"  Boundaries loaded:\")\n",
    "    print(f\"    - US States: {len(data_interface.geographies._us_states)} states\")\n",
    "    print(f\"    - CA Counties: {len(data_interface.geographies._ca_counties)} counties\")\n",
    "    print(f\"    - CA Watersheds: {len(data_interface.geographies._ca_watersheds)} watersheds\")\n",
    "    print(f\"    - CA Utilities: {len(data_interface.geographies._ca_utilities)} utilities\")\n",
    "    print()\n",
    "    print(\"🎉 Now safe to use multi-threading with get_data()!\")\n",
    "    print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a156787",
   "metadata": {},
   "source": [
    "## Monkey Patch `DataInterface.__init__` for Thread Safety\n",
    "\n",
    "When running parallel batch processing (e.g., with `ThreadPoolExecutor`), multiple threads may attempt to instantiate the `DataInterface` at the same time. The original `__init__` method is not thread-safe and can lead to race conditions, causing errors or inconsistent state if two threads initialize it simultaneously.\n",
    "\n",
    "**Monkey patching** the `__init__` method with a thread lock ensures that initialization occurs only once, regardless of how many threads attempt to create a `DataInterface` instance. This guarantees safe, predictable behavior in multi-threaded workflows and prevents issues such as duplicate resource loading or partial initialization.\n",
    "\n",
    "## Additional Resolutions\n",
    "\n",
    "This fix applies to all instances of attempting to parallelize data access calls in \n",
    "`climakitae`. The following cell should be included in any instance where data access\n",
    "needs to be multi-threaded including: `cava_data()`, `get_data()`, `DataParameters.retrieve()`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2defb9e-0b79-40a4-b8bc-cee3505de048",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T16:38:41.178022Z",
     "iopub.status.busy": "2025-10-28T16:38:41.177401Z",
     "iopub.status.idle": "2025-10-28T16:38:41.182353Z",
     "shell.execute_reply": "2025-10-28T16:38:41.181853Z",
     "shell.execute_reply.started": "2025-10-28T16:38:41.177967Z"
    }
   },
   "outputs": [],
   "source": [
    "# ADVANCED: Monkey-patch to prevent re-initialization\n",
    "# Only use this if kernel restart + STEP 1 doesn't work\n",
    "import threading\n",
    "\n",
    "# Save the original __init__\n",
    "_original_init = DataInterface.__init__\n",
    "\n",
    "# Create a lock for thread safety\n",
    "_init_lock = threading.Lock()\n",
    "_initialized = False\n",
    "\n",
    "def _patched_init(self, **params):\n",
    "    \"\"\"Patched __init__ that only runs once.\"\"\"\n",
    "    global _initialized\n",
    "    \n",
    "    with _init_lock:\n",
    "        if _initialized:\n",
    "            # Already initialized, skip\n",
    "            return\n",
    "        \n",
    "        # First time - run original init\n",
    "        _original_init(self, **params)\n",
    "        _initialized = True\n",
    "        print(\"✓ DataInterface initialized (first time only)\")\n",
    "\n",
    "# Apply the patch\n",
    "DataInterface.__init__ = _patched_init\n",
    "\n",
    "if VERBOSE:\n",
    "    print(\"✓ Monkey-patch applied - DataInterface will only initialize once\")\n",
    "    print(\"Now run STEP 1 to initialize it:\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2270beff",
   "metadata": {},
   "source": [
    "## Custom Metric Definition\n",
    "\n",
    "The effective temperature (T<sub>eff</sub>) is calculated using a weighted combination of current and lagged temperature values:\n",
    "\n",
    "**Formula:**\n",
    "```\n",
    "T_eff = 0.7 × T_max(0) + 0.003 × T_min(0) × T_max(1) + 0.002 × T_min(1) × T_max(2)\n",
    "```\n",
    "\n",
    "**Where:**\n",
    "| Variable | Description |\n",
    "|----------|-------------|\n",
    "| T_max(0) | Current day maximum temperature |\n",
    "| T_min(0) | Current day minimum temperature |\n",
    "| T_max(1) | Previous day maximum temperature (1-day lag) |\n",
    "| T_min(1) | Previous day minimum temperature (1-day lag) |\n",
    "| T_max(2) | Maximum temperature from 2 days ago (2-day lag) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6b3bee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T16:38:41.183710Z",
     "iopub.status.busy": "2025-10-28T16:38:41.183055Z",
     "iopub.status.idle": "2025-10-28T16:38:41.191635Z",
     "shell.execute_reply": "2025-10-28T16:38:41.191110Z",
     "shell.execute_reply.started": "2025-10-28T16:38:41.183682Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_teff(min_temp, max_temp):\n",
    "    \"\"\"\n",
    "    Calculate effective temperature index using min and max temperature data.\n",
    "    \n",
    "    Teff = 0.7*Tmax0 + 0.003*Tmin0*Tmax1 + 0.002*Tmin1*Tmax2\n",
    "    \n",
    "    Where:\n",
    "    - Tmax0: current day max temperature\n",
    "    - Tmin0: current day min temperature\n",
    "    - Tmax1: 1-day lag max temperature\n",
    "    - Tmin1: 1-day lag min temperature\n",
    "    - Tmax2: 2-day lag max temperature\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    min_temp : xr.DataArray or xr.Dataset\n",
    "        Minimum temperature data with time or time_delta dimension\n",
    "    max_temp : xr.DataArray or xr.Dataset\n",
    "        Maximum temperature data with time or time_delta dimension\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    xr.DataArray or xr.Dataset\n",
    "        Effective temperature index (Teff)\n",
    "        \n",
    "    Notes\n",
    "    -----\n",
    "    The first two time steps will contain NaN values due to lagging.\n",
    "    \"\"\"\n",
    "    # Determine which temporal dimension is present\n",
    "    if 'time_delta' in max_temp.dims:\n",
    "        time_dim = 'time_delta'\n",
    "    elif 'time' in max_temp.dims:\n",
    "        time_dim = 'time'\n",
    "    else:\n",
    "        raise ValueError(\"Data must have either 'time' or 'time_delta' dimension\")\n",
    "    \n",
    "    # Create lagged versions using the appropriate dimension\n",
    "    tmax0 = max_temp  # Current day\n",
    "    tmin0 = min_temp  # Current day\n",
    "    tmax1 = max_temp.shift({time_dim: 1})  # 1-day lag\n",
    "    tmin1 = min_temp.shift({time_dim: 1})  # 1-day lag\n",
    "    tmax2 = max_temp.shift({time_dim: 2})  # 2-day lag\n",
    "    \n",
    "    # Calculate effective temperature\n",
    "    teff = 0.7 * tmax0 + 0.003 * tmin0 * tmax1 + 0.002 * tmin1 * tmax2\n",
    "    \n",
    "    return teff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b6e683",
   "metadata": {},
   "source": [
    "## Understanding the 1-in-X Custom Calculation Functions\n",
    "\n",
    "This section explains the two main functions used for calculating 1-in-X year return values for custom climate metrics.\n",
    "\n",
    "---\n",
    "\n",
    "### `calculate_1_in_x_custom()` - Main Calculation Function\n",
    "\n",
    "This function calculates 1-in-X year return values (e.g., 1-in-10 year, 1-in-100 year events) for custom climate metrics using extreme value analysis. It processes multiple locations and climate model simulations efficiently in batch mode.\n",
    "\n",
    "#### **What It Does**\n",
    "\n",
    "The function follows a 5-step process:\n",
    "\n",
    "1. **Validation & Setup**: Validates input data structure and parameters\n",
    "2. **Spatial Extraction**: Extracts data for all requested locations using `get_closest_gridcells()`\n",
    "3. **Data Preparation**: Converts time dimensions if needed (e.g., `time_delta` → `time`)\n",
    "4. **Simulation Processing**: Iterates through each simulation (and warming level if present), computing return values\n",
    "5. **Output Structuring**: Assembles results into a well-structured xarray Dataset\n",
    "\n",
    "#### **Key Design Principles**\n",
    "\n",
    "- **Batch Processing**: Processes all locations together rather than one at a time for efficiency\n",
    "- **Lazy Evaluation**: Keeps data as dask arrays (not computed) as long as possible to minimize memory usage\n",
    "- **Memory Management**: Only computes one simulation at a time into memory, then releases it\n",
    "- **Flexible Dimensions**: Handles both historical (`time`) and warming level (`time_delta`) data\n",
    "- **Multi-scenario Support**: Can process data with optional `warming_level` dimension\n",
    "\n",
    "#### **Input Parameters**\n",
    "\n",
    "| Parameter | Type | Description | Default |\n",
    "|-----------|------|-------------|---------|\n",
    "| `custom_data` | xr.DataArray or xr.Dataset | Gridded climate metric with spatial (`lat`, `lon`), temporal (`time` or `time_delta`), and `simulation` dimensions | Required |\n",
    "| `input_locations` | pd.DataFrame | DataFrame with `lat` and `lon` columns for analysis locations | Required |\n",
    "| `return_periods` | list of int | Return periods in years (e.g., `[10, 100]` for 1-in-10 and 1-in-100 year events) | `[10, 100]` |\n",
    "| `metric` | str | Type of extreme: `'max'` for maxima or `'min'` for minima | `'max'` |\n",
    "| `distr` | str | Distribution to fit: `'gev'` (Generalized Extreme Value) or `'gumbel'` | `'gev'` |\n",
    "| `event_duration` | tuple | Event duration as `(value, unit)`, e.g., `(1, \"day\")` | `(1, \"day\")` |\n",
    "\n",
    "#### **Output Structure**\n",
    "\n",
    "Returns an xarray Dataset with:\n",
    "\n",
    "- **`return_value`**: Return values for each location, simulation, and return period\n",
    "  - Dimensions: `(location, simulation, [warming_level,] one_in_x)`\n",
    "  - Units: Same as input data\n",
    "  \n",
    "- **`p_values`**: Kolmogorov-Smirnov goodness-of-fit p-values\n",
    "  - Dimensions: `(location, simulation, [warming_level])`\n",
    "  - Interpretation: Values > 0.05 indicate good distribution fit\n",
    "  \n",
    "- **Coordinates**:\n",
    "  - `location_lat`: Latitude for each location\n",
    "  - `location_lon`: Longitude for each location\n",
    "  - `simulation`: Model simulation identifiers\n",
    "  - `warming_level`: Global warming levels (if applicable)\n",
    "  - `one_in_x`: Return period values\n",
    "\n",
    "#### **Example Usage**\n",
    "\n",
    "```python\n",
    "# Calculate 1-in-10 and 1-in-100 year maximum effective temperature\n",
    "results = calculate_1_in_x_custom(\n",
    "    teff_data,                 # Your custom metric (e.g., effective temperature)\n",
    "    locations_df,              # DataFrame with lat/lon columns\n",
    "    return_periods=[10, 100],  # 1-in-10 and 1-in-100 year events\n",
    "    metric=\"max\",              # Maximum events\n",
    "    distr=\"gev\"                # Use GEV distribution\n",
    ")\n",
    "\n",
    "# Access results\n",
    "return_vals = results['return_value']  # Shape: (n_locations, n_simulations, n_return_periods)\n",
    "p_values = results['p_values']         # Shape: (n_locations, n_simulations)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### `_process_one_simulation()` - Helper Function\n",
    "\n",
    "This internal helper function processes a single climate simulation (or simulation-warming_level combination) to calculate return values and goodness-of-fit statistics.\n",
    "\n",
    "#### **What It Does**\n",
    "\n",
    "1. **Dimension Reshaping**: Converts 1D `location` dimension into 2D spatial grid (`y`, `x`)\n",
    "   - Required because `threshold_tools` functions expect 2D spatial data\n",
    "   - Creates a virtual grid where each location occupies a unique (y, x) position\n",
    "   \n",
    "2. **Block Maxima Extraction**: Extracts annual maxima (or minima) using `get_block_maxima()`\n",
    "   - For daily data with 1-day duration, automatically extracts annual maxima\n",
    "   - Respects the `extremes_type` parameter ('max' or 'min')\n",
    "   \n",
    "3. **Return Value Calculation**: Fits extreme value distribution and calculates return values\n",
    "   - Uses `get_return_value()` to fit GEV or Gumbel distribution\n",
    "   - Calculates values for specified return periods\n",
    "   \n",
    "4. **Goodness-of-Fit Testing**: Performs Kolmogorov-Smirnov test using `get_ks_stat()`\n",
    "   - Tests whether the fitted distribution matches the data well\n",
    "   - Returns p-values for statistical validation\n",
    "   \n",
    "5. **Dimension Restoration**: Reshapes results back to 1D `location` dimension\n",
    "\n",
    "#### **Why the Reshaping?**\n",
    "\n",
    "The `threshold_tools` functions (`get_block_maxima`, `get_return_value`, `get_ks_stat`) were designed to work with gridded climate data having 2D spatial dimensions (`y`, `x`). However, after extracting specific locations, our data has a 1D `location` dimension. \n",
    "\n",
    "The reshaping process:\n",
    "- **Before**: `(time, location)` where `location` is 1D\n",
    "- **Intermediate**: `(time, y, x)` where `y=1` and `x=n_locations` (virtual 2D grid)\n",
    "- **After**: `(location, one_in_x)` for return values, `(location)` for p-values\n",
    "\n",
    "This allows us to leverage the existing `multiple_points=True` functionality in `threshold_tools` while working with point locations rather than full grids.\n",
    "\n",
    "#### **Parameters**\n",
    "\n",
    "| Parameter | Type | Description |\n",
    "|-----------|------|-------------|\n",
    "| `one_sim_computed` | xr.DataArray | Computed data for one simulation with dims `(time, location)` |\n",
    "| `return_periods` | list | List of return period values |\n",
    "| `metric` | str | `'max'` or `'min'` for extreme type |\n",
    "| `distr` | str | Distribution name (`'gev'` or `'gumbel'`) |\n",
    "| `groupby` | value or UNSET | Groupby parameter for `get_block_maxima` |\n",
    "| `duration` | value or UNSET | Duration parameter for `get_block_maxima` |\n",
    "\n",
    "#### **Returns**\n",
    "\n",
    "Tuple of `(ret_val, p_val)`:\n",
    "- `ret_val`: Return values with dimensions `(location, one_in_x)`\n",
    "- `p_val`: P-values with dimension `(location)`\n",
    "\n",
    "#### **Internal Workflow Diagram**\n",
    "\n",
    "```\n",
    "Input: (time, location)\n",
    "       ↓\n",
    "[Reshape to 2D grid]\n",
    "       ↓\n",
    "Data: (time, y, x)  where y=1, x=n_locations\n",
    "       ↓\n",
    "[Extract block maxima]\n",
    "       ↓\n",
    "Annual Maxima: (year, y, x)\n",
    "       ↓\n",
    "[Fit distribution & calculate return values]\n",
    "       ↓\n",
    "Return Values: (y, x, one_in_x)\n",
    "       ↓\n",
    "[Reshape back to 1D]\n",
    "       ↓\n",
    "Output: (location, one_in_x)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Performance Considerations**\n",
    "\n",
    "- **Memory**: Each simulation is computed separately and released, keeping memory usage manageable\n",
    "- **Speed**: Batch processing all locations together is faster than iterating individually\n",
    "- **Parallelization**: The notebook demonstrates parallel batch processing using `ThreadPoolExecutor`\n",
    "- **Data Locality**: Uses lazy evaluation with dask arrays until computation is absolutely necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123b30a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T16:38:41.193290Z",
     "iopub.status.busy": "2025-10-28T16:38:41.193042Z",
     "iopub.status.idle": "2025-10-28T16:38:41.226722Z",
     "shell.execute_reply": "2025-10-28T16:38:41.226132Z",
     "shell.execute_reply.started": "2025-10-28T16:38:41.193274Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_1_in_x_custom(\n",
    "    custom_data,          # User's custom metric (gridded, with lat/lon)\n",
    "    input_locations,      # DataFrame with 'lat' and 'lon' columns\n",
    "    return_periods=[10, 100],\n",
    "    metric=\"max\",\n",
    "    distr=\"gev\",\n",
    "    event_duration=(1, \"day\"),\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculate 1-in-X year return values for custom metrics.\n",
    "    \n",
    "    Processes all locations and simulations together in batch mode,\n",
    "    following the cava_data(batch_mode=True) pattern. Keeps operations\n",
    "    lazy (dask arrays) as long as possible.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    custom_data : xr.DataArray or xr.Dataset\n",
    "        Gridded custom metric with lat/lon dimensions and simulation dimension.\n",
    "        Can have either 'time' or 'time_delta' dimension for temporal data.\n",
    "        Can optionally have 'warming_level' dimension which will be preserved.\n",
    "    input_locations : pd.DataFrame\n",
    "        DataFrame with 'lat' and 'lon' columns specifying locations to analyze.\n",
    "    return_periods : list of int, optional\n",
    "        Return periods for 1-in-X year events (e.g., [10, 100] for 1-in-10 \n",
    "        and 1-in-100 year events). Default is [10, 100].\n",
    "    metric : str, optional\n",
    "        Type of extreme to calculate: 'max' or 'min'. Default is 'max'.\n",
    "    distr : str, optional\n",
    "        Distribution to fit: 'gev' (Generalized Extreme Value) or 'gumbel'.\n",
    "        Default is 'gev'.\n",
    "    event_duration : tuple of (int, str), optional\n",
    "        Duration of events as (value, unit) where unit is 'day' or 'hour'.\n",
    "        Default is (1, \"day\").\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    xr.Dataset\n",
    "        Dataset containing:\n",
    "        - 'return_value': Return values for each location, simulation, and \n",
    "          return period. Dimensions: (location, simulation, [warming_level,] one_in_x)\n",
    "        - 'p_values': Kolmogorov-Smirnov goodness-of-fit p-values for each\n",
    "          location and simulation. Dimensions: (location, simulation, [warming_level])\n",
    "    \"\"\"\n",
    "    \n",
    "    # ============================================================\n",
    "    # STEP 1: VALIDATE AND SETUP\n",
    "    # ============================================================\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"BATCH MODE 1-IN-X CALCULATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Validate input_locations\n",
    "    if not isinstance(input_locations, pd.DataFrame):\n",
    "        raise TypeError(\n",
    "            f\"input_locations must be a pandas DataFrame, got {type(input_locations)}\"\n",
    "        )\n",
    "    \n",
    "    if 'lat' not in input_locations.columns:\n",
    "        raise ValueError(\n",
    "            \"input_locations DataFrame must contain 'lat' column\"\n",
    "        )\n",
    "    \n",
    "    if 'lon' not in input_locations.columns:\n",
    "        raise ValueError(\n",
    "            \"input_locations DataFrame must contain 'lon' column\"\n",
    "        )\n",
    "    \n",
    "    # Check for empty DataFrame\n",
    "    if len(input_locations) == 0:\n",
    "        raise ValueError(\n",
    "            \"input_locations DataFrame is empty. Please provide at least one location.\"\n",
    "        )\n",
    "    \n",
    "    # Validate custom_data has required dimensions\n",
    "    if not isinstance(custom_data, (xr.DataArray, xr.Dataset)):\n",
    "        raise TypeError(\n",
    "            f\"custom_data must be xarray DataArray or Dataset, got {type(custom_data)}\"\n",
    "        )\n",
    "    \n",
    "    # Check for spatial dimensions\n",
    "    if 'lat' not in custom_data.dims and 'latitude' not in custom_data.dims:\n",
    "        raise ValueError(\n",
    "            \"custom_data must have 'lat' or 'latitude' dimension for spatial extraction\"\n",
    "        )\n",
    "    \n",
    "    if 'lon' not in custom_data.dims and 'longitude' not in custom_data.dims:\n",
    "        raise ValueError(\n",
    "            \"custom_data must have 'lon' or 'longitude' dimension for spatial extraction\"\n",
    "        )\n",
    "    \n",
    "    # Check for temporal dimension\n",
    "    has_time = 'time' in custom_data.dims\n",
    "    has_time_delta = 'time_delta' in custom_data.dims\n",
    "    \n",
    "    if not (has_time or has_time_delta):\n",
    "        raise ValueError(\n",
    "            \"custom_data must have either 'time' or 'time_delta' dimension for temporal analysis\"\n",
    "        )\n",
    "    \n",
    "    # Check for simulation dimension\n",
    "    if 'simulation' not in custom_data.dims:\n",
    "        raise ValueError(\n",
    "            \"custom_data must have 'simulation' dimension for batch processing\"\n",
    "        )\n",
    "    \n",
    "    # Check for warming_level dimension\n",
    "    has_warming_level = 'warming_level' in custom_data.dims\n",
    "    \n",
    "    # Convert return_periods to list if needed\n",
    "    if not isinstance(return_periods, list):\n",
    "        return_periods = [return_periods]\n",
    "    \n",
    "    # Validate return_periods\n",
    "    if len(return_periods) == 0:\n",
    "        raise ValueError(\"return_periods must contain at least one value\")\n",
    "    \n",
    "    for rp in return_periods:\n",
    "        if not isinstance(rp, (int, float)) or rp <= 0:\n",
    "            raise ValueError(\n",
    "                f\"All return periods must be positive numbers, got {rp}\"\n",
    "            )\n",
    "    \n",
    "    # Print summary\n",
    "    num_locations = len(input_locations)\n",
    "    num_simulations = len(custom_data.simulation)\n",
    "    num_warming_levels = len(custom_data.warming_level) if has_warming_level else 1\n",
    "\n",
    "    if VERBOSE:\n",
    "        print(f\"\\nConfiguration:\")\n",
    "        print(f\"  Locations: {num_locations}\")\n",
    "        print(f\"  Simulations: {num_simulations}\")\n",
    "        if has_warming_level:\n",
    "            print(f\"  Warming Levels: {num_warming_levels}\")\n",
    "        print(f\"  Return periods: {return_periods}\")\n",
    "        print(f\"  Metric: {metric}\")\n",
    "        print(f\"  Distribution: {distr}\")\n",
    "        print(f\"  Event duration: {event_duration[0]} {event_duration[1]}\")\n",
    "        print(f\"  Temporal dimension: {'time_delta' if has_time_delta else 'time'}\")\n",
    "        print()\n",
    "    \n",
    "        print(\"\\n--- Step 2: Extracting Gridcells ---\")\n",
    "        print(f\"Extracting closest gridcells for {num_locations} location(s)...\")\n",
    "    \n",
    "    # Extract lat/lon arrays from DataFrame\n",
    "    lats = input_locations['lat'].values\n",
    "    lons = input_locations['lon'].values\n",
    "    \n",
    "    # Extract all locations at once (batch mode)\n",
    "    # This returns data with 'points' dimension stacking all locations\n",
    "    custom_data_batch = get_closest_gridcells(\n",
    "        custom_data, \n",
    "        lats, \n",
    "        lons\n",
    "    )\n",
    "    \n",
    "    # Rename 'points' to 'location' for clarity\n",
    "    custom_data_batch = custom_data_batch.rename({'points': 'location'})\n",
    "    \n",
    "    # Keep data lazy (don't compute yet)\n",
    "    if VERBOSE:\n",
    "        print(f\"✓ Extracted gridcells successfully\")\n",
    "        print(f\"  Data type: {type(custom_data_batch).__name__}\")\n",
    "        print(f\"  Dimensions: {list(custom_data_batch.dims)}\")\n",
    "        print(f\"  Sizes: {dict(custom_data_batch.sizes)}\")\n",
    "        print(f\"  Shape: {custom_data_batch.shape}\")\n",
    "        if hasattr(custom_data_batch, 'data'):\n",
    "            print(f\"  Data remains lazy: {hasattr(custom_data_batch.data, 'dask')}\")\n",
    "        print()\n",
    "    \n",
    "        print(\"\\n--- Step 3: Preparing Data ---\")\n",
    "    # Check if data has time_delta dimension (warming level data)\n",
    "    if has_time_delta:\n",
    "        if VERBOSE: print(\"Converting time_delta to time dimension for resampling...\")\n",
    "        custom_data_batch = add_dummy_time_to_wl(custom_data_batch)\n",
    "        if VERBOSE:\n",
    "            print(f\"✓ Converted to time dimension\")\n",
    "            print(f\"  New dimensions: {list(custom_data_batch.dims)}\")\n",
    "            if hasattr(custom_data_batch, 'data'):\n",
    "                print(f\"  Data remains lazy: {hasattr(custom_data_batch.data, 'dask')}\")\n",
    "        \n",
    "            # NOTE: We will drop NaN values inside the simulation loop\n",
    "            # to keep data lazy as long as possible\n",
    "            print(\"  Note: NaN values will be dropped during simulation processing\")\n",
    "    else:\n",
    "        if VERBOSE: print(\"Data already has 'time' dimension, no conversion needed\")\n",
    "\n",
    "    if VERBOSE:\n",
    "        print(\"\\n--- Step 4: Processing Simulations ---\")\n",
    "        total_iterations = num_simulations * num_warming_levels\n",
    "        print(f\"Processing {num_simulations} simulations\" + \n",
    "              (f\" × {num_warming_levels} warming levels = {total_iterations} total iterations...\" \n",
    "               if has_warming_level else \"...\"))\n",
    "    \n",
    "    # Prepare event duration parameters based on data frequency\n",
    "    # For daily data from warming levels, we use annual maxima directly\n",
    "    if event_duration[1] == \"day\":\n",
    "        if event_duration[0] == 1:\n",
    "            # For 1-day events with daily data, don't pass groupby or duration\n",
    "            # get_block_maxima will automatically extract annual maxima\n",
    "            groupby = UNSET\n",
    "            duration = UNSET\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Multi-day duration events ({event_duration[0]} days) not yet supported. \"\n",
    "                \"Use duration=(1, 'day') for daily data.\"\n",
    "            )\n",
    "    elif event_duration[1] == \"hour\":\n",
    "        # Hourly data can use duration parameter\n",
    "        groupby = UNSET\n",
    "        duration = event_duration\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported duration unit: {event_duration[1]}. Use 'day' or 'hour'.\")\n",
    "    \n",
    "    return_vals_list = []\n",
    "    p_vals_list = []\n",
    "    \n",
    "    # Template variables for creating NaN-filled results\n",
    "    ret_val_template = None\n",
    "    p_val_template = None\n",
    "    \n",
    "    # Determine iteration order: simulation first, then warming_level if present\n",
    "    iteration_count = 0\n",
    "    \n",
    "    # Loop over simulations\n",
    "    # Loop over simulations\n",
    "    for i, sim in enumerate(custom_data_batch.simulation.values):\n",
    "        \n",
    "        # Select this simulation (keeps all locations and warming levels!)\n",
    "        one_sim = custom_data_batch.sel(simulation=sim)\n",
    "        \n",
    "        # If warming_level dimension exists, loop over it\n",
    "        if has_warming_level:\n",
    "            for j, wl in enumerate(custom_data_batch.warming_level.values):\n",
    "                iteration_count += 1\n",
    "                if VERBOSE: print(f\"  Processing simulation {i+1}/{num_simulations}, \" +\n",
    "                      f\"warming level {j+1}/{num_warming_levels} ({wl}°C) \" +\n",
    "                      f\"[{iteration_count}/{total_iterations}]...\", end=\" \")\n",
    "                \n",
    "                # Select this warming level\n",
    "                # After selection, warming_level becomes a scalar coordinate\n",
    "                one_sim_wl = one_sim.sel(warming_level=wl)\n",
    "                \n",
    "                # Drop the warming_level coordinate if it exists (it's now scalar, not a dimension)\n",
    "                if 'warming_level' in one_sim_wl.coords and 'warming_level' not in one_sim_wl.dims:\n",
    "                    one_sim_wl = one_sim_wl.drop_vars('warming_level')\n",
    "                \n",
    "                # NOW compute to memory for this single simulation-warming_level combo\n",
    "                one_sim_computed = one_sim_wl.compute()\n",
    "                \n",
    "                # Drop NaN values AFTER computing\n",
    "                original_time = one_sim_computed.sizes['time']\n",
    "                one_sim_computed = one_sim_computed.dropna(dim='time')\n",
    "                final_time = one_sim_computed.sizes['time']\n",
    "                \n",
    "                if original_time != final_time:\n",
    "                    if VERBOSE: print(f\"(dropped {original_time - final_time} NaN timesteps)\", end=\" \")\n",
    "                \n",
    "                # Check if there's any data left after dropping NaN values\n",
    "                if final_time == 0:\n",
    "                    if VERBOSE: print(\"⚠️  WARNING: No valid data remaining - creating NaN results\")\n",
    "                    \n",
    "                    # Use template or create from scratch\n",
    "                    if ret_val_template is not None:\n",
    "                        # Copy structure from template and fill with NaNs\n",
    "                        ret_val = ret_val_template.copy(deep=True)\n",
    "                        ret_val.values[:] = np.nan\n",
    "                        p_val = p_val_template.copy(deep=True)\n",
    "                        p_val.values[:] = np.nan\n",
    "                    else:\n",
    "                        # Fallback: create simple structure (only for first failure before any success)\n",
    "                        n_locs = one_sim_computed.sizes['location']\n",
    "                        ret_val = xr.DataArray(\n",
    "                            np.full((n_locs, len(return_periods)), np.nan),\n",
    "                            dims=('location', 'one_in_x'),\n",
    "                            coords={\n",
    "                                'location': np.arange(n_locs),\n",
    "                                'one_in_x': return_periods\n",
    "                            }\n",
    "                        )\n",
    "                        p_val = xr.DataArray(\n",
    "                            np.full(n_locs, np.nan),\n",
    "                            dims=('location',),\n",
    "                            coords={'location': np.arange(n_locs)}\n",
    "                        )\n",
    "                else:\n",
    "                    # Process this simulation-warming_level combination\n",
    "                    ret_val, p_val = _process_one_simulation(\n",
    "                        one_sim_computed, \n",
    "                        return_periods, \n",
    "                        metric, \n",
    "                        distr, \n",
    "                        groupby, \n",
    "                        duration\n",
    "                    )\n",
    "                    \n",
    "                    # Save as template for future NaN results\n",
    "                    if ret_val_template is None:\n",
    "                        ret_val_template = ret_val.copy(deep=True)\n",
    "                        p_val_template = p_val.copy(deep=True)\n",
    "                \n",
    "                # Clean up coordinates to ensure consistency\n",
    "                ret_val = ret_val.drop_vars([c for c in ret_val.coords \n",
    "                                            if c not in ['location', 'one_in_x']], errors='ignore')\n",
    "                p_val = p_val.drop_vars([c for c in p_val.coords \n",
    "                                        if c != 'location'], errors='ignore')\n",
    "                \n",
    "                return_vals_list.append(ret_val)\n",
    "                p_vals_list.append(p_val)\n",
    "                \n",
    "                if VERBOSE: print(\"✓\")\n",
    "        else:\n",
    "            # No warming level dimension - process simulation directly\n",
    "            iteration_count += 1\n",
    "            if VERBOSE: print(f\"  Processing simulation {i+1}/{num_simulations} [{iteration_count}/{total_iterations}]: {sim}...\", end=\" \")\n",
    "            \n",
    "            # NOW compute to memory for this single simulation\n",
    "            one_sim_computed = one_sim.compute()\n",
    "            \n",
    "            # Drop NaN values AFTER computing\n",
    "            original_time = one_sim_computed.sizes['time']\n",
    "            one_sim_computed = one_sim_computed.dropna(dim='time')\n",
    "            final_time = one_sim_computed.sizes['time']\n",
    "            \n",
    "            if original_time != final_time:\n",
    "                if VERBOSE: print(f\"(dropped {original_time - final_time} NaN timesteps)\", end=\" \")\n",
    "            \n",
    "            # Check if there's any data left after dropping NaN values\n",
    "            if final_time == 0:\n",
    "                if VERBOSE: print(\"⚠️  WARNING: No valid data remaining - creating NaN results\")\n",
    "                \n",
    "                # Use template or create from scratch\n",
    "                if ret_val_template is not None:\n",
    "                    # Copy structure from template and fill with NaNs\n",
    "                    ret_val = ret_val_template.copy(deep=True)\n",
    "                    ret_val.values[:] = np.nan\n",
    "                    p_val = p_val_template.copy(deep=True)\n",
    "                    p_val.values[:] = np.nan\n",
    "                else:\n",
    "                    # Fallback: create simple structure (only for first failure before any success)\n",
    "                    n_locs = one_sim_computed.sizes['location']\n",
    "                    ret_val = xr.DataArray(\n",
    "                        np.full((n_locs, len(return_periods)), np.nan),\n",
    "                        dims=('location', 'one_in_x'),\n",
    "                        coords={\n",
    "                            'location': np.arange(n_locs),\n",
    "                            'one_in_x': return_periods\n",
    "                        }\n",
    "                    )\n",
    "                    p_val = xr.DataArray(\n",
    "                        np.full(n_locs, np.nan),\n",
    "                        dims=('location',),\n",
    "                        coords={'location': np.arange(n_locs)}\n",
    "                    )\n",
    "            else:\n",
    "                # Process this simulation\n",
    "                ret_val, p_val = _process_one_simulation(\n",
    "                    one_sim_computed, \n",
    "                    return_periods, \n",
    "                    metric, \n",
    "                    distr, \n",
    "                    groupby, \n",
    "                    duration\n",
    "                )\n",
    "                \n",
    "                # Save as template for future NaN results\n",
    "                if ret_val_template is None:\n",
    "                    ret_val_template = ret_val.copy(deep=True)\n",
    "                    p_val_template = p_val.copy(deep=True)\n",
    "            \n",
    "            # Clean up coordinates to ensure consistency\n",
    "            ret_val = ret_val.drop_vars([c for c in ret_val.coords \n",
    "                                        if c not in ['location', 'one_in_x']], errors='ignore')\n",
    "            p_val = p_val.drop_vars([c for c in p_val.coords \n",
    "                                    if c != 'location'], errors='ignore')\n",
    "            \n",
    "            return_vals_list.append(ret_val)\n",
    "            p_vals_list.append(p_val)\n",
    "            \n",
    "            if VERBOSE: print(\"✓\")\n",
    "    \n",
    "    if VERBOSE: \n",
    "        print(f\"✓ Completed all {total_iterations} iterations\")\n",
    "        print()\n",
    "    \n",
    "    # ============================================================\n",
    "    # STEP 5: STRUCTURE OUTPUT\n",
    "    # ============================================================\n",
    "    if VERBOSE: \n",
    "        print(\"\\n--- Step 5: Structuring Output ---\")\n",
    "        print(\"Assembling results into xarray Dataset...\")\n",
    "    \n",
    "    # Stack return values across simulations (and warming levels if present)\n",
    "    # Each element in return_vals_list has dims: (location, one_in_x)\n",
    "    # After concat, will have dims: (sim_wl_combo, location, one_in_x)\n",
    "    return_vals_stacked = xr.concat(\n",
    "        return_vals_list, \n",
    "        dim=\"sim_wl_combo\",\n",
    "        coords='minimal',\n",
    "        compat='override'\n",
    "    )\n",
    "    \n",
    "    # Stack p-values across simulations (and warming levels if present)\n",
    "    p_vals_stacked = xr.concat(\n",
    "        p_vals_list,\n",
    "        dim=\"sim_wl_combo\",\n",
    "        coords='minimal',\n",
    "        compat='override'\n",
    "    )\n",
    "    \n",
    "    # Reshape to separate simulation and warming_level dimensions\n",
    "    if has_warming_level:\n",
    "        # Create MultiIndex for sim_wl_combo dimension\n",
    "        sim_index = np.repeat(custom_data_batch.simulation.values, num_warming_levels)\n",
    "        wl_index = np.tile(custom_data_batch.warming_level.values, num_simulations)\n",
    "        \n",
    "        # Assign coordinates\n",
    "        return_vals_stacked = return_vals_stacked.assign_coords(\n",
    "            simulation=(\"sim_wl_combo\", sim_index),\n",
    "            warming_level=(\"sim_wl_combo\", wl_index)\n",
    "        )\n",
    "        p_vals_stacked = p_vals_stacked.assign_coords(\n",
    "            simulation=(\"sim_wl_combo\", sim_index),\n",
    "            warming_level=(\"sim_wl_combo\", wl_index)\n",
    "        )\n",
    "        \n",
    "        # Set multi-index and unstack\n",
    "        return_vals_stacked = return_vals_stacked.set_index(\n",
    "            sim_wl_combo=[\"simulation\", \"warming_level\"]\n",
    "        ).unstack(\"sim_wl_combo\")\n",
    "        \n",
    "        p_vals_stacked = p_vals_stacked.set_index(\n",
    "            sim_wl_combo=[\"simulation\", \"warming_level\"]\n",
    "        ).unstack(\"sim_wl_combo\")\n",
    "        \n",
    "        # Transpose to desired order: (location, simulation, warming_level, one_in_x)\n",
    "        return_vals_final = return_vals_stacked.transpose(\"location\", \"simulation\", \"warming_level\", \"one_in_x\")\n",
    "        p_vals_final = p_vals_stacked.transpose(\"location\", \"simulation\", \"warming_level\")\n",
    "        \n",
    "    else:\n",
    "        # No warming level - simpler structure\n",
    "        # Assign simulation coordinate values\n",
    "        return_vals_stacked = return_vals_stacked.assign_coords(\n",
    "            simulation=(\"sim_wl_combo\", custom_data_batch.simulation.values)\n",
    "        )\n",
    "        p_vals_stacked = p_vals_stacked.assign_coords(\n",
    "            simulation=(\"sim_wl_combo\", custom_data_batch.simulation.values)\n",
    "        )\n",
    "        \n",
    "        # Rename dim\n",
    "        return_vals_final = return_vals_stacked.rename({\"sim_wl_combo\": \"simulation\"})\n",
    "        p_vals_final = p_vals_stacked.rename({\"sim_wl_combo\": \"simulation\"})\n",
    "        \n",
    "        # Transpose to desired order: (location, simulation, one_in_x)\n",
    "        return_vals_final = return_vals_final.transpose(\"location\", \"simulation\", \"one_in_x\")\n",
    "        p_vals_final = p_vals_final.transpose(\"location\", \"simulation\")\n",
    "    \n",
    "    # Create output Dataset\n",
    "    result = xr.Dataset({\n",
    "        \"return_value\": return_vals_final,\n",
    "        \"p_values\": p_vals_final\n",
    "    })\n",
    "    \n",
    "    # Add lat/lon coordinates for each location\n",
    "    location_lats = input_locations['lat'].values\n",
    "    location_lons = input_locations['lon'].values\n",
    "    \n",
    "    result = result.assign_coords({\n",
    "        \"location_lat\": (\"location\", location_lats),\n",
    "        \"location_lon\": (\"location\", location_lons)\n",
    "    })\n",
    "    \n",
    "    # Add comprehensive metadata\n",
    "    # Convert return_periods to a string representation for NetCDF compatibility\n",
    "    return_periods_str = str(return_periods)\n",
    "    \n",
    "    result.attrs.update({\n",
    "        \"processing_mode\": \"batch\",\n",
    "        \"num_locations\": int(num_locations),\n",
    "        \"num_simulations\": int(num_simulations),\n",
    "        \"num_warming_levels\": int(num_warming_levels),  # Always an integer now\n",
    "        \"return_periods\": return_periods_str,  # Convert list to string\n",
    "        \"fitted_distribution\": distr,\n",
    "        \"extremes_type\": metric,\n",
    "        \"event_duration_value\": int(event_duration[0]),\n",
    "        \"event_duration_unit\": event_duration[1],\n",
    "        \"created_with\": \"calculate_1_in_x_custom\",\n",
    "        \"temporal_input_dimension\": \"time_delta\" if has_time_delta else \"time\",\n",
    "        \"has_warming_level_dimension\": int(has_warming_level),\n",
    "    })\n",
    "    \n",
    "    # Add variable-level metadata\n",
    "    result[\"return_value\"].attrs.update({\n",
    "        \"long_name\": f\"{metric.capitalize()} {event_duration[0]}-{event_duration[1]} return values\",\n",
    "        \"units\": \"same as input data\",\n",
    "        \"description\": f\"Return values for {return_periods_str} year return periods using {distr} distribution\",\n",
    "    })\n",
    "    \n",
    "    result[\"p_values\"].attrs.update({\n",
    "        \"long_name\": \"Kolmogorov-Smirnov goodness-of-fit p-values\",\n",
    "        \"description\": f\"P-values for {distr} distribution fit quality. Values > 0.05 indicate good fit.\",\n",
    "    })\n",
    "\n",
    "    if VERBOSE: \n",
    "        print(\"✓ Output Dataset created successfully\")\n",
    "        print(f\"  Dimensions: {list(result.dims)}\")\n",
    "        print(f\"  Data variables: {list(result.data_vars)}\")\n",
    "        print(f\"  Coordinates: {list(result.coords)}\")\n",
    "        print()\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        print(\"BATCH MODE CALCULATION COMPLETE!\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"\\nResults summary:\")\n",
    "        print(f\"  Locations analyzed: {num_locations}\")\n",
    "        print(f\"  Simulations processed: {num_simulations}\")\n",
    "        if has_warming_level:\n",
    "            print(f\"  Warming levels: {num_warming_levels}\")\n",
    "        print(f\"  Return periods: {return_periods}\")\n",
    "        print(f\"  Output shape: {result['return_value'].shape}\")\n",
    "        print()\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def _process_one_simulation(one_sim_computed, return_periods, metric, distr, groupby, duration):\n",
    "    \"\"\"\n",
    "    Helper function to process a single simulation (or simulation-warming_level combo).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    one_sim_computed : xr.DataArray\n",
    "        Computed data for one simulation, with dims (time, location)\n",
    "    return_periods : list\n",
    "        List of return periods\n",
    "    metric : str\n",
    "        'max' or 'min'\n",
    "    distr : str\n",
    "        Distribution name\n",
    "    groupby : value or UNSET\n",
    "        Groupby parameter for get_block_maxima\n",
    "    duration : value or UNSET\n",
    "        Duration parameter for get_block_maxima\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tuple of (ret_val, p_val)\n",
    "        Return values and p-values for this simulation\n",
    "    \"\"\"\n",
    "    # Reshape data to have separate x and y dimensions\n",
    "    # threshold_tools expects (time, y, x) not (time, location)\n",
    "    # We create a 2D grid where each location is at a unique (y, x) position\n",
    "    n_locations = one_sim_computed.sizes['location']\n",
    "    \n",
    "    # Create a virtual 2D grid (can be a row or column)\n",
    "    # Use a row layout: y=1, x varies\n",
    "    one_sim_reshaped = one_sim_computed.assign_coords({\n",
    "        'x': ('location', np.arange(n_locations)),\n",
    "        'y': ('location', np.zeros(n_locations, dtype=int))  # All same y\n",
    "    })\n",
    "    \n",
    "    # Expand location into y and x dimensions\n",
    "    one_sim_reshaped = one_sim_reshaped.set_index(location=['y', 'x']).unstack('location')\n",
    "    \n",
    "    # Now data has dims: (time, y, x)\n",
    "    # where y=1 and x=n_locations\n",
    "    \n",
    "    # Get block maxima (handles multiple locations via y, x dimensions)\n",
    "    ams = get_block_maxima(\n",
    "        one_sim_reshaped,\n",
    "        extremes_type=metric,\n",
    "        duration=duration,\n",
    "        groupby=groupby,\n",
    "        check_ess=False,\n",
    "    )\n",
    "    \n",
    "    # Calculate return values (multiple_points=True handles y, x dimensions)\n",
    "    ret_val = get_return_value(\n",
    "        ams,\n",
    "        return_period=return_periods,\n",
    "        multiple_points=True,\n",
    "        distr=distr,\n",
    "        bootstrap_runs=0,  # increase for better estimates on confidence intervals\n",
    "    )[\"return_value\"]\n",
    "    \n",
    "    # Reshape back to location dimension\n",
    "    # Stack x and y back into a single location dimension\n",
    "    ret_val = ret_val.stack(location=['y', 'x']).reset_index('location', drop=True)\n",
    "    ret_val = ret_val.assign_coords(location=np.arange(n_locations))\n",
    "    \n",
    "    # Drop any extra coordinates that might cause concat issues\n",
    "    # Keep only location and one_in_x coordinates\n",
    "    coords_to_keep = ['location', 'one_in_x']\n",
    "    extra_coords = [c for c in ret_val.coords if c not in coords_to_keep]\n",
    "    if extra_coords:\n",
    "        ret_val = ret_val.drop_vars(extra_coords)\n",
    "    \n",
    "    # Goodness of fit\n",
    "    ks_result = get_ks_stat(ams, distr=distr, multiple_points=True)\n",
    "    \n",
    "    # Reshape p_values back to location dimension\n",
    "    p_val = ks_result.p_value.stack(location=['y', 'x']).reset_index('location', drop=True)\n",
    "    p_val = p_val.assign_coords(location=np.arange(n_locations))\n",
    "    \n",
    "    # Drop any extra coordinates from p_val\n",
    "    # Keep only location coordinate\n",
    "    extra_coords = [c for c in p_val.coords if c != 'location']\n",
    "    if extra_coords:\n",
    "        p_val = p_val.drop_vars(extra_coords)\n",
    "    \n",
    "    return ret_val, p_val\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83e0c8d",
   "metadata": {},
   "source": [
    "## Example: Calculate 1-in-X Events for Effective Temperature\n",
    "\n",
    "Now you can calculate the Effective Temperature and then compute 1-in-X year return values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0047a522-0c75-41f7-9e65-0cc5165f773d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T16:38:41.228116Z",
     "iopub.status.busy": "2025-10-28T16:38:41.227490Z",
     "iopub.status.idle": "2025-10-28T16:38:41.254271Z",
     "shell.execute_reply": "2025-10-28T16:38:41.253773Z",
     "shell.execute_reply.started": "2025-10-28T16:38:41.228087Z"
    }
   },
   "outputs": [],
   "source": [
    "# location loading\n",
    "num_pts = -1\n",
    "locations = pd.DataFrame({\n",
    "    'lat': [x[0] for x in lat_lons[:num_pts]],\n",
    "    'lon': [x[1] for x in lat_lons[:num_pts]] \n",
    "})\n",
    "\n",
    "batch_size = 5\n",
    "batches = np.array_split(locations, len(locations)//batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8f6d7f",
   "metadata": {},
   "source": [
    "## Defining parameters\n",
    "In the following cell we define a bunch of programmatic variable used for fetching the required data.\n",
    "\n",
    "| Variable | Type / Example | Meaning |\n",
    "|---|---:|---|\n",
    "| space | float (0.02) | Padding in degrees added around batch bounding boxes when requesting data (lat/lon buffer). |\n",
    "| min_temp_var | str (\"Minimum air temperature at 2m\") | Name/description of the minimum temperature variable to request from the data API. |\n",
    "| max_temp_var | str (\"Maximum air temperature at 2m\") | Name/description of the maximum temperature variable to request from the data API. |\n",
    "| downscaling | str (\"Statistical\") | Downscaling method to request (e.g., \"Statistical\" or \"Dynamical\"). |\n",
    "| resolution | str (\"3 km\") | Spatial resolution of the downscaled data to request. |\n",
    "| timescale | str (\"daily\") | Temporal aggregation of the data (e.g., \"daily\", \"hourly\"). |\n",
    "| approach | str (\"Warming Level\") | Retrieval approach: e.g., warming level (\"Warming Level\") vs. time-slice/historical. |\n",
    "| units | str (\"degF\") | Output units to request from the API (ensures consistent units across variables). |\n",
    "| wls | list of float ([2.0, 2.5]) | Warming levels (°C) to request when using the warming-level approach. |\n",
    "| return_periods | list of int ([10, 100]) | Return periods (in years) for which to compute 1-in-X events. |\n",
    "| time_slice | tuple (2000, 2029) | Start and end years for historical/time-slice data requests. |\n",
    "| scenario | list of str ([\"Historical Climate\", \"SSP 3-7.0\"]) | Scenario names to request (historical and future scenario(s)). |\n",
    "| num_pts | int (-1) | Number of points from lat_lons to include (-1 means use all). |\n",
    "| batch_size | int (5) | Number of locations per processing batch. |\n",
    "| batches | list of DataFrame | List of DataFrame batches created by splitting locations for parallel processing. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e67f250",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T16:38:41.255428Z",
     "iopub.status.busy": "2025-10-28T16:38:41.254942Z",
     "iopub.status.idle": "2025-10-28T16:38:41.258474Z",
     "shell.execute_reply": "2025-10-28T16:38:41.257951Z",
     "shell.execute_reply.started": "2025-10-28T16:38:41.255400Z"
    }
   },
   "outputs": [],
   "source": [
    "# specific inputs for grabbing data\n",
    "space = 0.02 # you probably don't need to change this\n",
    "min_temp_var = \"Minimum air temperature at 2m\"\n",
    "max_temp_var = \"Maximum air temperature at 2m\"\n",
    "downscaling = \"Statistical\"\n",
    "resolution = \"3 km\"\n",
    "timescale = \"daily\"\n",
    "approach = \"Warming Level\"\n",
    "units = \"degF\"\n",
    "wls = [2.0, 2.5]\n",
    "return_periods = [10, 100]\n",
    "time_slice = (2000, 2029)\n",
    "scenario = [\"Historical Climate\", \"SSP 3-7.0\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4d74dc",
   "metadata": {},
   "source": [
    "## Purpose of the next cell — job functions for concurrent futures\n",
    "\n",
    "This cell defines two worker functions intended to be submitted to a ThreadPoolExecutor:\n",
    "\n",
    "- Names: `process_batch_time` and `process_batch_gwl`\n",
    "- Purpose: fetch min/max temperature data for a batch of locations, compute the effective-temperature metric, run the batch 1‑in‑X calculation, export results, and return the Dataset.\n",
    "- Signature: both accept a single argument `(idx, batch)` where `batch` is a pandas DataFrame with `lat`/`lon` columns; they return the computed xarray Dataset or `None` on failure.\n",
    "- Behavior:\n",
    "    - Compute bounding box from `batch` and call `get_data(...)` (different call for warming‑level vs time slice).\n",
    "    - Call `calculate_teff()` then `calculate_1_in_x_custom()` with shared globals.\n",
    "    - Export result with `export(..., filename=...)`.\n",
    "    - Catch exceptions, print detailed traceback and return `None`.\n",
    "- Dependencies (must exist in the kernel):\n",
    "    - `get_data`, `calculate_teff`, `calculate_1_in_x_custom`, `export`\n",
    "    - Configuration globals: `min_temp_var`, `max_temp_var`, `space`, `downscaling`, `resolution`, `timescale`, `time_slice`, `scenario`, `units`, `approach`, `wls`, `return_periods`\n",
    "    - `VERBOSE` / thread-safety monkey patch for `DataInterface` if using multithreading\n",
    "- Usage:\n",
    "    - Submit using `executor.submit(process_batch_time, (idx, batch))` or `executor.submit(process_batch_gwl, (idx, batch))`\n",
    "    - Inspect returned value in futures; exported files follow the patterns `teff_batch_time_{idx}` and `teff_batch_gwl_{idx}`\n",
    "- Notes and tips:\n",
    "    - Functions print progress and detailed tracebacks to help debugging.\n",
    "    - Keep `n_workers` small enough for available memory and API rate limits.\n",
    "    - Ensure the DataInterface monkey‑patch (cell 3) is applied if you will create multiple threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba38b71-73ce-466b-8256-f3463284f659",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T16:38:41.259400Z",
     "iopub.status.busy": "2025-10-28T16:38:41.259132Z",
     "iopub.status.idle": "2025-10-28T16:38:41.269996Z",
     "shell.execute_reply": "2025-10-28T16:38:41.269533Z",
     "shell.execute_reply.started": "2025-10-28T16:38:41.259383Z"
    }
   },
   "outputs": [],
   "source": [
    "import traceback\n",
    "\n",
    "def process_batch_time(args: tuple[int, pd.DataFrame]) -> pd.DataFrame | None:\n",
    "    \"\"\"\n",
    "    Process a batch of locations for time-based data retrieval and analysis.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    args : tuple\n",
    "        Tuple containing (idx, batch)\n",
    "        - idx : int\n",
    "            Batch index used for logging and output file naming.\n",
    "        - batch : pandas.DataFrame\n",
    "            DataFrame containing at least 'lat' and 'lon' columns that define the\n",
    "            set of locations to process.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame or None\n",
    "        The resulting DataFrame produced by calculate_1_in_x_custom for the batch,\n",
    "        or None if an exception occurred during processing.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    None\n",
    "        All exceptions are caught internally; details are printed and the function\n",
    "        returns None on error.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The function:\n",
    "    - Determines the spatial bounding box from batch lat/lon values.\n",
    "    - Fetches min/max temperature data for that bounding box.\n",
    "    - Computes teff and 1-in-x statistics.\n",
    "    - Exports the result to a file named \"teff_batch_time_{idx}\".\n",
    "    \"\"\"\n",
    "    idx, batch = args\n",
    "    \n",
    "    print(f\"[Batch {idx}] Starting processing...\")\n",
    "    \n",
    "    try:\n",
    "        min_lat = min(batch['lat'].values.tolist())\n",
    "        max_lat = max(batch['lat'].values.tolist())\n",
    "        min_lon = min(batch['lon'].values.tolist())\n",
    "        max_lon = max(batch['lon'].values.tolist())\n",
    "        \n",
    "        min_temp_df = get_data(\n",
    "            variable=min_temp_var,\n",
    "            latitude=(min_lat - space, max_lat + space),\n",
    "            longitude=(min_lon - space, max_lon + space),\n",
    "            downscaling_method=downscaling,\n",
    "            timescale=timescale,\n",
    "            time_slice=time_slice,\n",
    "            scenario=scenario,\n",
    "            resolution=resolution,\n",
    "            units=units\n",
    "        )\n",
    "        max_temp_df = get_data(\n",
    "            variable=max_temp_var,\n",
    "            latitude=(min_lat - space, max_lat + space),\n",
    "            longitude=(min_lon - space, max_lon + space),\n",
    "            downscaling_method=downscaling,\n",
    "            timescale=timescale,\n",
    "            time_slice=time_slice,\n",
    "            scenario=scenario,\n",
    "            resolution=resolution,\n",
    "            units=units\n",
    "        )\n",
    "        \n",
    "        teff_batch = calculate_teff(min_temp_df, max_temp_df)\n",
    "        result = calculate_1_in_x_custom(\n",
    "            teff_batch,\n",
    "            batch,\n",
    "            return_periods=return_periods,\n",
    "        )\n",
    "\n",
    "        export(result, filename=f\"teff_batch_time_{idx}\")        \n",
    "        return result\n",
    "        \n",
    "    except Exception as exc:\n",
    "        print(f\"\\n[Batch {idx}] ❌❌❌ EXCEPTION OCCURRED:\")\n",
    "        print(f\"[Batch {idx}] Exception type: {type(exc).__name__}\")\n",
    "        print(f\"[Batch {idx}] Exception message: {exc}\")\n",
    "        print(f\"[Batch {idx}] Full traceback:\")\n",
    "        print(\"-\" * 60)\n",
    "        traceback.print_exc()\n",
    "        print(\"-\" * 60)\n",
    "        return None\n",
    "\n",
    "\n",
    "def process_batch_gwl(args: tuple[int, pd.DataFrame]) -> pd.DataFrame | None:\n",
    "    \"\"\"\n",
    "    Process a batch of locations for global warming level (GWL) based data retrieval\n",
    "    and analysis.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    args : tuple\n",
    "        Tuple containing (idx, batch)\n",
    "        idx : int\n",
    "            Batch index used for logging and output file naming.\n",
    "        batch : pandas.DataFrame\n",
    "            DataFrame containing at least 'lat' and 'lon' columns that define the\n",
    "            set of locations to process.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame or None\n",
    "        The resulting DataFrame produced by calculate_1_in_x_custom for the batch,\n",
    "        or None if an exception occurred during processing.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    None\n",
    "        All exceptions are caught internally; details are printed and the function\n",
    "        returns None on error.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Uses get_data with `approach` and `warming_levels` (wls) parameters to fetch\n",
    "      min/max temperature data for the spatial bounding box of the batch.\n",
    "    - Computes teff and 1-in-x statistics via calculate_teff and\n",
    "      calculate_1_in_x_custom.\n",
    "    - Exports the result to a file named \"teff_batch_gwl_{idx}\".\n",
    "    \"\"\"\n",
    "    idx, batch = args\n",
    "    \n",
    "    print(f\"[Batch {idx}] Starting processing...\")\n",
    "    \n",
    "    try:\n",
    "        min_lat = min(batch['lat'].values.tolist())\n",
    "        max_lat = max(batch['lat'].values.tolist())\n",
    "        min_lon = min(batch['lon'].values.tolist())\n",
    "        max_lon = max(batch['lon'].values.tolist())\n",
    "        \n",
    "        min_temp_df = get_data(\n",
    "            variable=min_temp_var,\n",
    "            latitude=(min_lat - space, max_lat + space),\n",
    "            longitude=(min_lon - space, max_lon + space),\n",
    "            downscaling_method=downscaling,\n",
    "            timescale=timescale,\n",
    "            scenario=scenario,\n",
    "            resolution=resolution,\n",
    "            units=units,\n",
    "            approach=approach,\n",
    "            warming_levels=wls\n",
    "        )\n",
    "        max_temp_df = get_data(\n",
    "            variable=max_temp_var,\n",
    "            latitude=(min_lat - space, max_lat + space),\n",
    "            longitude=(min_lon - space, max_lon + space),\n",
    "            downscaling_method=downscaling,\n",
    "            timescale=timescale,\n",
    "            scenario=scenario,\n",
    "            resolution=resolution,\n",
    "            units=units,\n",
    "            approach=approach,\n",
    "            warming_levels=wls\n",
    "        )\n",
    "        \n",
    "        teff_batch = calculate_teff(min_temp_df, max_temp_df)\n",
    "        result = calculate_1_in_x_custom(\n",
    "            teff_batch,\n",
    "            batch,\n",
    "            return_periods=return_periods,\n",
    "        )\n",
    "\n",
    "        export(result, filename=f\"teff_batch_gwl_{idx}\")        \n",
    "        return result\n",
    "        \n",
    "    except Exception as exc:\n",
    "        print(f\"\\n[Batch {idx}] ❌❌❌ EXCEPTION OCCURRED:\")\n",
    "        print(f\"[Batch {idx}] Exception type: {type(exc).__name__}\")\n",
    "        print(f\"[Batch {idx}] Exception message: {exc}\")\n",
    "        print(f\"[Batch {idx}] Full traceback:\")\n",
    "        print(\"-\" * 60)\n",
    "        traceback.print_exc()\n",
    "        print(\"-\" * 60)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80259a61",
   "metadata": {},
   "source": [
    "### Purpose\n",
    "Run the batch processing workers in parallel (with thread pools) to compute and collect 1‑in‑X results for two processing modes:\n",
    "- time‑based retrieval (`process_batch_time`)\n",
    "- global warming level retrieval (`process_batch_gwl`)\n",
    "\n",
    "### What the cell does\n",
    "- Configures a thread pool size via `n_workers` (here 2).\n",
    "- Submits the first two batches (`batches[:2]`) to `process_batch_time` using `ThreadPoolExecutor`, collects completed futures, appends each returned result to `results` along with its batch index, and prints exceptions if any occur.\n",
    "- Repeats the same pattern for `process_batch_gwl`.\n",
    "- After both executors finish, sorts `results` by batch index and replaces `results` with the list of returned datasets (discarding the indices).\n",
    "\n",
    "### Key behaviors and notes\n",
    "- Only the first two batches are processed because of `batches[:2]`; change that slice to process more.\n",
    "- Each submitted job receives `(idx, batch)` (matching the worker function signatures).\n",
    "- Exceptions inside worker functions are captured both inside the worker (they return `None`) and here when accessing `future.result()`; both paths are handled.\n",
    "- `n_workers` should be tuned for available memory and API limits; the DataInterface monkey‑patch (cell 3) should be applied if using multiple threads to avoid race conditions.\n",
    "- Final `results` contains only the returned datasets (or `None` for failed batches), ordered by batch index.\n",
    "\n",
    "> [!NOTE]\n",
    "> Please comment out the version you aren't using for the current run. If you're using both then leave them both un-commented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6defe99f-4342-4562-a64b-571135da99d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T16:38:41.270782Z",
     "iopub.status.busy": "2025-10-28T16:38:41.270598Z"
    }
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "n_workers = 2  # Adjust based on your system capabilities\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=n_workers) as executor:\n",
    "    # Submit all batches for parallel processing\n",
    "    future_to_batch = {executor.submit(process_batch_time, args): args[0] for args in enumerate(batches[:2])}\n",
    "    for future in concurrent.futures.as_completed(future_to_batch):\n",
    "        idx = future_to_batch[future]\n",
    "        try:\n",
    "            result = future.result()\n",
    "            results.append((idx, result))\n",
    "        except Exception as exc:\n",
    "            print(f\"[Batch {idx}] generated an exception: {exc}\")\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=n_workers) as executor:\n",
    "    # Submit all batches for parallel processing\n",
    "    future_to_batch = {executor.submit(process_batch_gwl, args): args[0] for args in enumerate(batches[:2])}\n",
    "    for future in concurrent.futures.as_completed(future_to_batch):\n",
    "        idx = future_to_batch[future]\n",
    "        try:\n",
    "            result = future.result()\n",
    "            results.append((idx, result))\n",
    "        except Exception as exc:\n",
    "            print(f\"[Batch {idx}] generated an exception: {exc}\")\n",
    "\n",
    "# Optionally, sort results by batch index\n",
    "results.sort(key=lambda x: x[0])\n",
    "results = [r[1] for r in results]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
