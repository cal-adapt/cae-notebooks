{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df6a7be8-d390-4951-8e9d-78bc80231f3f",
   "metadata": {},
   "source": [
    "# Custom 1-in-X Analysis for Effective Temperature\n",
    "<br>\n",
    "\n",
    "This notebook demonstrates how to calculate **1-in-X year return values for a custom metric for multiple locations** in an optimized batch-mode (multi-threading). As an example, this notebook calculates a custom Effective Temperature metric, which is then evaluated using extreme value analysis. The workflow follows a generalized <span style=\"color: blue\">[Vulnerability Assessment framework](https://github.com/cal-adapt/cae-notebooks/blob/main/collaborative/IOU/vulnerability_assessment/vulnerability_assessment.ipynb)</span> with a fully customized function called `calculate_1_in_X_custom()`. \n",
    "\n",
    "**Why consider multi-threading for a large proccess?**\n",
    "Say for example you need to calculate a custom metric across your entire service territory. While the data available on the Analytics Engine is cloud-optimized, retrieving and processing hourly / daily data over a 30-year period for 115+ model simulations for a large gridded area is **a lot of data**! Such a \"single-thread\" process is time-intensive and computer memory-intensive. With \"multi-threading\", you are able to increase the number of workers that complete the same task, efficiently cutting down the run time. For example: without multi-threading, a batch size of **1600 locations** would take approximately **27 days to complete**. With multi-threading, the same batch size completes in **28 hours**.  \n",
    "\n",
    "**Intended Application**: As a user, I want to <span style=\"color: red\">**efficiently calculate a custom metric for 100s of locations**</span> by: \n",
    "1. Learning how to set-up multi-threading safely and efficiently\n",
    "2. Designing my own custom metric\n",
    "\n",
    "> [!WARNING]\n",
    "> - **Thread Safety**: `DataInterface` was not originally intended for multi-threading. This has been patched in `climakitae` versions above v1.4.0. Please ensure you are using the latest version.\n",
    "> - This notebook is optimized for speed over precision. Bootstrap samples are set to 1, which means confidence intervals are not representative. Increase `bootstrap_runs` in `_process_one_simulation()` for accurate confidence intervals.\n",
    "\n",
    "> [!NOTE]\n",
    "> With the default settings, this notebook will take **4+ hours per worker** to run from start to finish. Modifications may increase the runtime. On a JupyterHub machine with 30 GB RAM, there is a maximum of 4-5 workers since each worker uses ~5 GB. In testing, each worker took 3.5 hours to finish a batch of **50 locations**. When selecting your batch size, be aware of the potential runtime. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2a03ec-9026-49d1-aa84-dd83e0ce9545",
   "metadata": {},
   "source": [
    "#### Step 0: Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c510f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent\n",
    "import traceback\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from climakitae.core.data_interface import get_data, DataInterface\n",
    "from climakitae.core.data_export import export\n",
    "\n",
    "# Import functions for 1-in-X event calculations\n",
    "from climakitae.explore.threshold_tools import (\n",
    "    get_block_maxima,\n",
    "    get_ks_stat,\n",
    "    get_return_value,\n",
    ")\n",
    "from climakitae.core.constants import UNSET\n",
    "from climakitae.util.utils import add_dummy_time_to_wl, get_closest_gridcells\n",
    "\n",
    "data_interface = DataInterface() # create to avoid race conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15801bd2-b623-427a-8838-4edd46f564a4",
   "metadata": {},
   "source": [
    "For quiet output, set `VERBOSE = False` in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce592e5-cd0d-49a5-9fbf-a9000bc9079f",
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBOSE = True "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2270beff",
   "metadata": {},
   "source": [
    "#### Step 1: Custom Metric Definition\n",
    "\n",
    "As an example, we will build a customized version of effective temperature (T<sub>eff</sub>) metric, as many utilities may use different thresholds. Effective temperature is calculated using a weighted combination of current and lagged temperature values. \n",
    "```\n",
    "T_eff = 0.7 × T_max(0) + 0.003 × T_min(0) × T_max(1) + 0.002 × T_min(1) × T_max(2)\n",
    "```\n",
    "\n",
    "**Where:** `T_max(0)` is the current day maximum temperature, `T_min(0)` is the current day minimum temperature. `T_max(1)` and `T_min(1)` are the previous day's maximum and minimum temperature (1-day lag), respectively. `T_max(2)` is the maximum temperature from 2 days ago (2-day lag). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6b3bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_teff(min_temp, max_temp):\n",
    "    \"\"\"\n",
    "    Calculate effective temperature index using min and max temperature data.\n",
    "    \n",
    "    Teff = 0.7*Tmax0 + 0.003*Tmin0*Tmax1 + 0.002*Tmin1*Tmax2\n",
    "    \n",
    "    Where:\n",
    "    - Tmax0: current day max temperature\n",
    "    - Tmin0: current day min temperature\n",
    "    - Tmax1: 1-day lag max temperature\n",
    "    - Tmin1: 1-day lag min temperature\n",
    "    - Tmax2: 2-day lag max temperature\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    min_temp : xr.DataArray or xr.Dataset\n",
    "        Minimum temperature data with time or time_delta dimension\n",
    "    max_temp : xr.DataArray or xr.Dataset\n",
    "        Maximum temperature data with time or time_delta dimension\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    xr.DataArray or xr.Dataset\n",
    "        Effective temperature index (Teff)\n",
    "        \n",
    "    Notes\n",
    "    -----\n",
    "    The first two time steps will contain NaN values due to lagging.\n",
    "    \"\"\"\n",
    "    # Determine which temporal dimension is present\n",
    "    if 'time_delta' in max_temp.dims:\n",
    "        time_dim = 'time_delta'\n",
    "    elif 'time' in max_temp.dims:\n",
    "        time_dim = 'time'\n",
    "    else:\n",
    "        raise ValueError(\"Data must have either 'time' or 'time_delta' dimension\")\n",
    "    \n",
    "    # Create lagged versions using the appropriate dimension\n",
    "    tmax0 = max_temp  # Current day\n",
    "    tmin0 = min_temp  # Current day\n",
    "    tmax1 = max_temp.shift({time_dim: 1})  # 1-day lag\n",
    "    tmin1 = min_temp.shift({time_dim: 1})  # 1-day lag\n",
    "    tmax2 = max_temp.shift({time_dim: 2})  # 2-day lag\n",
    "    \n",
    "    # Calculate effective temperature\n",
    "    teff = 0.7 * tmax0 + 0.003 * tmin0 * tmax1 + 0.002 * tmin1 * tmax2\n",
    "    \n",
    "    return teff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b6e683",
   "metadata": {},
   "source": [
    "#### Step 2: Set-up the 1-in-X Custom Calculation Functions\n",
    "\n",
    "Now we'll set-up the core processing for calculating 1-in-X year return values for custom climate metrics. The `calculate_1_in_x_custom()` function calculates 1-in-X year return values (e.g., 1-in-10 year, 1-in-100 year events) for custom climate metrics using extreme value analysis. It processes multiple locations and climate model simulations efficiently in batch mode. `calculate_1_in_x_custom()` has a **5-step process**: (**1**) validation and set-up, (**2**) spatial data extraction, (**3**) data preparation, (**4**) simulation processing for computing return values, and (**5**) optimized output. This 5-step process ensures that batch-processing is efficient and minimizes memory usage as is feasible. The `_process_one_simulation()` internal helper function processes a single climate simulation (or simulation-warming_level combination) to calculate return values and goodness-of-fit statistics, ensuring multi-simulation processing efficiency.\n",
    "\n",
    "**Example Usage**\n",
    "\n",
    "```python\n",
    "# Calculate 1-in-10 and 1-in-100 year maximum effective temperature\n",
    "results = calculate_1_in_x_custom(\n",
    "    teff_data,                 # Your custom metric data (e.g., effective temperature)\n",
    "    locations_df,              # DataFrame with lat/lon columns\n",
    "    return_periods=[10, 100],  # 1-in-10 and 1-in-100 year events\n",
    "    metric=\"max\",              # Maximum events\n",
    "    distr=\"gev\",               # Use GEV distribution\n",
    "    event_duration=(1, \"day\")  # Optional event duration\n",
    ")\n",
    "\n",
    "# Access results\n",
    "return_vals = results['return_value']  # Return values for each location, simulation, and return period\n",
    "p_values = results['p_values']         # Kolmogorov-Smirnov goodness-of-fit p-values\n",
    "```\n",
    "\n",
    "> [!NOTE]\n",
    "> The following cell is **extensive** because of the optimization for batch processing and verbose information. When including your own customized metric, **carefully modify this function where appropriate**. We **do not recommend** any other modifications, unless you are confident with additional changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123b30a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_1_in_x_custom(\n",
    "    custom_data,               # Custom metric data (gridded, with lat/lon)\n",
    "    input_locations,           # DataFrame with 'lat' and 'lon' columns\n",
    "    return_periods=[10, 100],\n",
    "    metric=\"max\",\n",
    "    distr=\"gev\",\n",
    "    event_duration=(1, \"day\"),\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculate 1-in-X year return values for custom metrics.\n",
    "    \n",
    "    Processes all locations and simulations together in batch mode,\n",
    "    following the cava_data(batch_mode=True) pattern. Keeps operations\n",
    "    lazy (dask arrays) as long as possible.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    custom_data : xr.DataArray or xr.Dataset\n",
    "        Gridded custom metric with lat/lon dimensions and simulation dimension.\n",
    "        Can have either 'time' or 'time_delta' dimension for temporal data.\n",
    "        Can optionally have 'warming_level' dimension which will be preserved.\n",
    "    input_locations : pd.DataFrame\n",
    "        DataFrame with 'lat' and 'lon' columns specifying locations to analyze.\n",
    "    return_periods : list of int, optional\n",
    "        Return periods for 1-in-X year events (e.g., [10, 100] for 1-in-10 \n",
    "        and 1-in-100 year events). Default is [10, 100].\n",
    "    metric : str, optional\n",
    "        Type of extreme to calculate: 'max' or 'min'. Default is 'max'.\n",
    "    distr : str, optional\n",
    "        Distribution to fit: 'gev' (Generalized Extreme Value) or 'gumbel'.\n",
    "        Default is 'gev'.\n",
    "    event_duration : tuple of (int, str), optional\n",
    "        Duration of events as (value, unit) where unit is 'day' or 'hour'.\n",
    "        Default is (1, \"day\").\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    xr.Dataset\n",
    "        Dataset containing:\n",
    "        - 'return_value': Return values for each location, simulation, and \n",
    "          return period. Dimensions: (location, simulation, [warming_level,] one_in_x)\n",
    "        - 'p_values': Kolmogorov-Smirnov goodness-of-fit p-values for each\n",
    "          location and simulation. Dimensions: (location, simulation, [warming_level])\n",
    "    \"\"\"\n",
    "    \n",
    "    # ============================================================\n",
    "    # STEP 1: VALIDATE AND SETUP\n",
    "    # ============================================================\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"BATCH MODE 1-IN-X CALCULATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Validate input_locations\n",
    "    if not isinstance(input_locations, pd.DataFrame):\n",
    "        raise TypeError(\n",
    "            f\"input_locations must be a pandas DataFrame, got {type(input_locations)}\"\n",
    "        )\n",
    "    \n",
    "    if 'lat' not in input_locations.columns:\n",
    "        raise ValueError(\n",
    "            \"input_locations DataFrame must contain 'lat' column\"\n",
    "        )\n",
    "    \n",
    "    if 'lon' not in input_locations.columns:\n",
    "        raise ValueError(\n",
    "            \"input_locations DataFrame must contain 'lon' column\"\n",
    "        )\n",
    "    \n",
    "    # Check for empty DataFrame\n",
    "    if len(input_locations) == 0:\n",
    "        raise ValueError(\n",
    "            \"input_locations DataFrame is empty. Please provide at least one location.\"\n",
    "        )\n",
    "    \n",
    "    # Validate custom_data has required dimensions\n",
    "    if not isinstance(custom_data, (xr.DataArray, xr.Dataset)):\n",
    "        raise TypeError(\n",
    "            f\"custom_data must be xarray DataArray or Dataset, got {type(custom_data)}\"\n",
    "        )\n",
    "    \n",
    "    # Check for spatial dimensions\n",
    "    if 'lat' not in custom_data.dims and 'latitude' not in custom_data.dims:\n",
    "        raise ValueError(\n",
    "            \"custom_data must have 'lat' or 'latitude' dimension for spatial extraction\"\n",
    "        )\n",
    "    \n",
    "    if 'lon' not in custom_data.dims and 'longitude' not in custom_data.dims:\n",
    "        raise ValueError(\n",
    "            \"custom_data must have 'lon' or 'longitude' dimension for spatial extraction\"\n",
    "        )\n",
    "    \n",
    "    # Check for temporal dimension\n",
    "    has_time = 'time' in custom_data.dims\n",
    "    has_time_delta = 'time_delta' in custom_data.dims\n",
    "    \n",
    "    if not (has_time or has_time_delta):\n",
    "        raise ValueError(\n",
    "            \"custom_data must have either 'time' or 'time_delta' dimension for temporal analysis\"\n",
    "        )\n",
    "    \n",
    "    # Check for simulation dimension\n",
    "    if 'simulation' not in custom_data.dims:\n",
    "        raise ValueError(\n",
    "            \"custom_data must have 'simulation' dimension for batch processing\"\n",
    "        )\n",
    "    \n",
    "    # Check for warming_level dimension\n",
    "    has_warming_level = 'warming_level' in custom_data.dims\n",
    "    \n",
    "    # Convert return_periods to list if needed\n",
    "    if not isinstance(return_periods, list):\n",
    "        return_periods = [return_periods]\n",
    "    \n",
    "    # Validate return_periods\n",
    "    if len(return_periods) == 0:\n",
    "        raise ValueError(\"return_periods must contain at least one value\")\n",
    "    \n",
    "    for rp in return_periods:\n",
    "        if not isinstance(rp, (int, float)) or rp <= 0:\n",
    "            raise ValueError(\n",
    "                f\"All return periods must be positive numbers, got {rp}\"\n",
    "            )\n",
    "    \n",
    "    # Print summary\n",
    "    num_locations = len(input_locations)\n",
    "    num_simulations = len(custom_data.simulation)\n",
    "    num_warming_levels = len(custom_data.warming_level) if has_warming_level else 1\n",
    "\n",
    "    if VERBOSE:\n",
    "        print(f\"\\nConfiguration:\")\n",
    "        print(f\"  Locations: {num_locations}\")\n",
    "        print(f\"  Simulations: {num_simulations}\")\n",
    "        if has_warming_level:\n",
    "            print(f\"  Warming Levels: {num_warming_levels}\")\n",
    "        print(f\"  Return periods: {return_periods}\")\n",
    "        print(f\"  Metric: {metric}\")\n",
    "        print(f\"  Distribution: {distr}\")\n",
    "        print(f\"  Event duration: {event_duration[0]} {event_duration[1]}\")\n",
    "        print(f\"  Temporal dimension: {'time_delta' if has_time_delta else 'time'}\")\n",
    "        print()\n",
    "    \n",
    "        print(\"\\n--- Step 2: Extracting Gridcells ---\")\n",
    "        print(f\"Extracting closest gridcells for {num_locations} location(s)...\")\n",
    "    \n",
    "    # Extract lat/lon arrays from DataFrame\n",
    "    lats = input_locations['lat'].values\n",
    "    lons = input_locations['lon'].values\n",
    "    \n",
    "    # Extract all locations at once (batch mode)\n",
    "    # This returns data with 'points' dimension stacking all locations\n",
    "    custom_data_batch = get_closest_gridcells(\n",
    "        custom_data, \n",
    "        lats, \n",
    "        lons\n",
    "    )\n",
    "    \n",
    "    # Rename 'points' to 'location' for clarity\n",
    "    custom_data_batch = custom_data_batch.rename({'points': 'location'})\n",
    "    \n",
    "    # Keep data lazy (don't compute yet)\n",
    "    if VERBOSE:\n",
    "        print(f\"✓ Extracted gridcells successfully\")\n",
    "        print(f\"  Data type: {type(custom_data_batch).__name__}\")\n",
    "        print(f\"  Dimensions: {list(custom_data_batch.dims)}\")\n",
    "        print(f\"  Sizes: {dict(custom_data_batch.sizes)}\")\n",
    "        print(f\"  Shape: {custom_data_batch.shape}\")\n",
    "        if hasattr(custom_data_batch, 'data'):\n",
    "            print(f\"  Data remains lazy: {hasattr(custom_data_batch.data, 'dask')}\")\n",
    "        print()\n",
    "    \n",
    "        print(\"\\n--- Step 3: Preparing Data ---\")\n",
    "    # Check if data has time_delta dimension (warming level data)\n",
    "    if has_time_delta:\n",
    "        if VERBOSE: print(\"Converting time_delta to time dimension for resampling...\")\n",
    "        custom_data_batch = add_dummy_time_to_wl(custom_data_batch)\n",
    "        if VERBOSE:\n",
    "            print(f\"✓ Converted to time dimension\")\n",
    "            print(f\"  New dimensions: {list(custom_data_batch.dims)}\")\n",
    "            if hasattr(custom_data_batch, 'data'):\n",
    "                print(f\"  Data remains lazy: {hasattr(custom_data_batch.data, 'dask')}\")\n",
    "        \n",
    "            # NOTE: We will drop NaN values inside the simulation loop\n",
    "            # to keep data lazy as long as possible\n",
    "            print(\"  Note: NaN values will be dropped during simulation processing\")\n",
    "    else:\n",
    "        if VERBOSE: print(\"Data already has 'time' dimension, no conversion needed\")\n",
    "\n",
    "    if VERBOSE:\n",
    "        print(\"\\n--- Step 4: Processing Simulations ---\")\n",
    "        total_iterations = num_simulations * num_warming_levels\n",
    "        print(f\"Processing {num_simulations} simulations\" + \n",
    "              (f\" × {num_warming_levels} warming levels = {total_iterations} total iterations...\" \n",
    "               if has_warming_level else \"...\"))\n",
    "    \n",
    "    # Prepare event duration parameters based on data frequency\n",
    "    # For daily data from warming levels, we use annual maxima directly\n",
    "    if event_duration[1] == \"day\":\n",
    "        if event_duration[0] == 1:\n",
    "            # For 1-day events with daily data, don't pass groupby or duration\n",
    "            # get_block_maxima will automatically extract annual maxima\n",
    "            groupby = UNSET\n",
    "            duration = UNSET\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Multi-day duration events ({event_duration[0]} days) not yet supported. \"\n",
    "                \"Use duration=(1, 'day') for daily data.\"\n",
    "            )\n",
    "    elif event_duration[1] == \"hour\":\n",
    "        # Hourly data can use duration parameter\n",
    "        groupby = UNSET\n",
    "        duration = event_duration\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported duration unit: {event_duration[1]}. Use 'day' or 'hour'.\")\n",
    "    \n",
    "    return_vals_list = []\n",
    "    p_vals_list = []\n",
    "    \n",
    "    # Template variables for creating NaN-filled results\n",
    "    ret_val_template = None\n",
    "    p_val_template = None\n",
    "    \n",
    "    # Determine iteration order: simulation first, then warming_level if present\n",
    "    iteration_count = 0\n",
    "    \n",
    "    # Loop over simulations\n",
    "    for i, sim in enumerate(custom_data_batch.simulation.values):\n",
    "        \n",
    "        # Select this simulation (keeps all locations and warming levels!)\n",
    "        one_sim = custom_data_batch.sel(simulation=sim)\n",
    "        \n",
    "        # If warming_level dimension exists, loop over it\n",
    "        if has_warming_level:\n",
    "            for j, wl in enumerate(custom_data_batch.warming_level.values):\n",
    "                iteration_count += 1\n",
    "                if VERBOSE: print(f\"  Processing simulation {i+1}/{num_simulations}, \" +\n",
    "                      f\"warming level {j+1}/{num_warming_levels} ({wl}°C) \" +\n",
    "                      f\"[{iteration_count}/{total_iterations}]...\", end=\" \")\n",
    "                \n",
    "                # Select this warming level\n",
    "                # After selection, warming_level becomes a scalar coordinate\n",
    "                one_sim_wl = one_sim.sel(warming_level=wl)\n",
    "                \n",
    "                # Drop the warming_level coordinate if it exists (it's now scalar, not a dimension)\n",
    "                if 'warming_level' in one_sim_wl.coords and 'warming_level' not in one_sim_wl.dims:\n",
    "                    one_sim_wl = one_sim_wl.drop_vars('warming_level')\n",
    "                \n",
    "                # NOW compute to memory for this single simulation-warming_level combo\n",
    "                one_sim_computed = one_sim_wl.compute()\n",
    "                \n",
    "                # Drop NaN values AFTER computing\n",
    "                original_time = one_sim_computed.sizes['time']\n",
    "                one_sim_computed = one_sim_computed.dropna(dim='time')\n",
    "                final_time = one_sim_computed.sizes['time']\n",
    "                \n",
    "                if original_time != final_time:\n",
    "                    if VERBOSE: print(f\"(dropped {original_time - final_time} NaN timesteps)\", end=\" \")\n",
    "                \n",
    "                # Check if there's any data left after dropping NaN values\n",
    "                if final_time == 0:\n",
    "                    if VERBOSE: print(\"⚠️  WARNING: No valid data remaining - creating NaN results\")\n",
    "                    \n",
    "                    # Use template or create from scratch\n",
    "                    if ret_val_template is not None:\n",
    "                        # Copy structure from template and fill with NaNs\n",
    "                        ret_val = ret_val_template.copy(deep=True)\n",
    "                        ret_val.values[:] = np.nan\n",
    "                        p_val = p_val_template.copy(deep=True)\n",
    "                        p_val.values[:] = np.nan\n",
    "                    else:\n",
    "                        # Fallback: create simple structure (only for first failure before any success)\n",
    "                        n_locs = one_sim_computed.sizes['location']\n",
    "                        ret_val = xr.DataArray(\n",
    "                            np.full((n_locs, len(return_periods)), np.nan),\n",
    "                            dims=('location', 'one_in_x'),\n",
    "                            coords={\n",
    "                                'location': np.arange(n_locs),\n",
    "                                'one_in_x': return_periods\n",
    "                            }\n",
    "                        )\n",
    "                        p_val = xr.DataArray(\n",
    "                            np.full(n_locs, np.nan),\n",
    "                            dims=('location',),\n",
    "                            coords={'location': np.arange(n_locs)}\n",
    "                        )\n",
    "                else:\n",
    "                    # Process this simulation-warming_level combination\n",
    "                    ret_val, p_val = _process_one_simulation(\n",
    "                        one_sim_computed, \n",
    "                        return_periods, \n",
    "                        metric, \n",
    "                        distr, \n",
    "                        groupby, \n",
    "                        duration\n",
    "                    )\n",
    "                    \n",
    "                    # Save as template for future NaN results\n",
    "                    if ret_val_template is None:\n",
    "                        ret_val_template = ret_val.copy(deep=True)\n",
    "                        p_val_template = p_val.copy(deep=True)\n",
    "                \n",
    "                # Clean up coordinates to ensure consistency\n",
    "                ret_val = ret_val.drop_vars([c for c in ret_val.coords \n",
    "                                            if c not in ['location', 'one_in_x']], errors='ignore')\n",
    "                p_val = p_val.drop_vars([c for c in p_val.coords \n",
    "                                        if c != 'location'], errors='ignore')\n",
    "                \n",
    "                return_vals_list.append(ret_val)\n",
    "                p_vals_list.append(p_val)\n",
    "                \n",
    "                if VERBOSE: print(\"✓\")\n",
    "        else:\n",
    "            # No warming level dimension - process simulation directly\n",
    "            iteration_count += 1\n",
    "            if VERBOSE: print(f\"  Processing simulation {i+1}/{num_simulations} [{iteration_count}/{total_iterations}]: {sim}...\", end=\" \")\n",
    "            \n",
    "            # NOW compute to memory for this single simulation\n",
    "            one_sim_computed = one_sim.compute()\n",
    "            \n",
    "            # Drop NaN values AFTER computing\n",
    "            original_time = one_sim_computed.sizes['time']\n",
    "            one_sim_computed = one_sim_computed.dropna(dim='time')\n",
    "            final_time = one_sim_computed.sizes['time']\n",
    "            \n",
    "            if original_time != final_time:\n",
    "                if VERBOSE: print(f\"(dropped {original_time - final_time} NaN timesteps)\", end=\" \")\n",
    "            \n",
    "            # Check if there's any data left after dropping NaN values\n",
    "            if final_time == 0:\n",
    "                if VERBOSE: print(\"⚠️  WARNING: No valid data remaining - creating NaN results\")\n",
    "                \n",
    "                # Use template or create from scratch\n",
    "                if ret_val_template is not None:\n",
    "                    # Copy structure from template and fill with NaNs\n",
    "                    ret_val = ret_val_template.copy(deep=True)\n",
    "                    ret_val.values[:] = np.nan\n",
    "                    p_val = p_val_template.copy(deep=True)\n",
    "                    p_val.values[:] = np.nan\n",
    "                else:\n",
    "                    # Fallback: create simple structure (only for first failure before any success)\n",
    "                    n_locs = one_sim_computed.sizes['location']\n",
    "                    ret_val = xr.DataArray(\n",
    "                        np.full((n_locs, len(return_periods)), np.nan),\n",
    "                        dims=('location', 'one_in_x'),\n",
    "                        coords={\n",
    "                            'location': np.arange(n_locs),\n",
    "                            'one_in_x': return_periods\n",
    "                        }\n",
    "                    )\n",
    "                    p_val = xr.DataArray(\n",
    "                        np.full(n_locs, np.nan),\n",
    "                        dims=('location',),\n",
    "                        coords={'location': np.arange(n_locs)}\n",
    "                    )\n",
    "            else:\n",
    "                # Process this simulation\n",
    "                ret_val, p_val = _process_one_simulation(\n",
    "                    one_sim_computed, \n",
    "                    return_periods, \n",
    "                    metric, \n",
    "                    distr, \n",
    "                    groupby, \n",
    "                    duration\n",
    "                )\n",
    "                \n",
    "                # Save as template for future NaN results\n",
    "                if ret_val_template is None:\n",
    "                    ret_val_template = ret_val.copy(deep=True)\n",
    "                    p_val_template = p_val.copy(deep=True)\n",
    "            \n",
    "            # Clean up coordinates to ensure consistency\n",
    "            ret_val = ret_val.drop_vars([c for c in ret_val.coords \n",
    "                                        if c not in ['location', 'one_in_x']], errors='ignore')\n",
    "            p_val = p_val.drop_vars([c for c in p_val.coords \n",
    "                                    if c != 'location'], errors='ignore')\n",
    "            \n",
    "            return_vals_list.append(ret_val)\n",
    "            p_vals_list.append(p_val)\n",
    "            \n",
    "            if VERBOSE: print(\"✓\")\n",
    "    \n",
    "    if VERBOSE: \n",
    "        print(f\"✓ Completed all {total_iterations} iterations\")\n",
    "        print()\n",
    "    \n",
    "    # ============================================================\n",
    "    # STEP 5: STRUCTURE OUTPUT\n",
    "    # ============================================================\n",
    "    if VERBOSE: \n",
    "        print(\"\\n--- Step 5: Structuring Output ---\")\n",
    "        print(\"Assembling results into xarray Dataset...\")\n",
    "    \n",
    "    # Stack return values across simulations (and warming levels if present)\n",
    "    # Each element in return_vals_list has dims: (location, one_in_x)\n",
    "    # After concat, will have dims: (sim_wl_combo, location, one_in_x)\n",
    "    return_vals_stacked = xr.concat(\n",
    "        return_vals_list, \n",
    "        dim=\"sim_wl_combo\",\n",
    "        coords='minimal',\n",
    "        compat='override'\n",
    "    )\n",
    "    \n",
    "    # Stack p-values across simulations (and warming levels if present)\n",
    "    p_vals_stacked = xr.concat(\n",
    "        p_vals_list,\n",
    "        dim=\"sim_wl_combo\",\n",
    "        coords='minimal',\n",
    "        compat='override'\n",
    "    )\n",
    "    \n",
    "    # Reshape to separate simulation and warming_level dimensions\n",
    "    if has_warming_level:\n",
    "        # Create MultiIndex for sim_wl_combo dimension\n",
    "        sim_index = np.repeat(custom_data_batch.simulation.values, num_warming_levels)\n",
    "        wl_index = np.tile(custom_data_batch.warming_level.values, num_simulations)\n",
    "        \n",
    "        # Assign coordinates\n",
    "        return_vals_stacked = return_vals_stacked.assign_coords(\n",
    "            simulation=(\"sim_wl_combo\", sim_index),\n",
    "            warming_level=(\"sim_wl_combo\", wl_index)\n",
    "        )\n",
    "        p_vals_stacked = p_vals_stacked.assign_coords(\n",
    "            simulation=(\"sim_wl_combo\", sim_index),\n",
    "            warming_level=(\"sim_wl_combo\", wl_index)\n",
    "        )\n",
    "        \n",
    "        # Set multi-index and unstack\n",
    "        return_vals_stacked = return_vals_stacked.set_index(\n",
    "            sim_wl_combo=[\"simulation\", \"warming_level\"]\n",
    "        ).unstack(\"sim_wl_combo\")\n",
    "        \n",
    "        p_vals_stacked = p_vals_stacked.set_index(\n",
    "            sim_wl_combo=[\"simulation\", \"warming_level\"]\n",
    "        ).unstack(\"sim_wl_combo\")\n",
    "        \n",
    "        # Transpose to desired order: (location, simulation, warming_level, one_in_x)\n",
    "        return_vals_final = return_vals_stacked.transpose(\"location\", \"simulation\", \"warming_level\", \"one_in_x\")\n",
    "        p_vals_final = p_vals_stacked.transpose(\"location\", \"simulation\", \"warming_level\")\n",
    "        \n",
    "    else:\n",
    "        # No warming level - simpler structure\n",
    "        # Assign simulation coordinate values\n",
    "        return_vals_stacked = return_vals_stacked.assign_coords(\n",
    "            simulation=(\"sim_wl_combo\", custom_data_batch.simulation.values)\n",
    "        )\n",
    "        p_vals_stacked = p_vals_stacked.assign_coords(\n",
    "            simulation=(\"sim_wl_combo\", custom_data_batch.simulation.values)\n",
    "        )\n",
    "        \n",
    "        # Rename dim\n",
    "        return_vals_final = return_vals_stacked.rename({\"sim_wl_combo\": \"simulation\"})\n",
    "        p_vals_final = p_vals_stacked.rename({\"sim_wl_combo\": \"simulation\"})\n",
    "        \n",
    "        # Transpose to desired order: (location, simulation, one_in_x)\n",
    "        return_vals_final = return_vals_final.transpose(\"location\", \"simulation\", \"one_in_x\")\n",
    "        p_vals_final = p_vals_final.transpose(\"location\", \"simulation\")\n",
    "    \n",
    "    # Create output Dataset\n",
    "    result = xr.Dataset({\n",
    "        \"return_value\": return_vals_final,\n",
    "        \"p_values\": p_vals_final\n",
    "    })\n",
    "    \n",
    "    # Add lat/lon coordinates for each location\n",
    "    location_lats = input_locations['lat'].values\n",
    "    location_lons = input_locations['lon'].values\n",
    "    \n",
    "    result = result.assign_coords({\n",
    "        \"location_lat\": (\"location\", location_lats),\n",
    "        \"location_lon\": (\"location\", location_lons)\n",
    "    })\n",
    "    \n",
    "    # Add comprehensive metadata\n",
    "    # Convert return_periods to a string representation for NetCDF compatibility\n",
    "    return_periods_str = str(return_periods)\n",
    "    \n",
    "    result.attrs.update({\n",
    "        \"processing_mode\": \"batch\",\n",
    "        \"num_locations\": int(num_locations),\n",
    "        \"num_simulations\": int(num_simulations),\n",
    "        \"num_warming_levels\": int(num_warming_levels),  # Always an integer now\n",
    "        \"return_periods\": return_periods_str,  # Convert list to string\n",
    "        \"fitted_distribution\": distr,\n",
    "        \"extremes_type\": metric,\n",
    "        \"event_duration_value\": int(event_duration[0]),\n",
    "        \"event_duration_unit\": event_duration[1],\n",
    "        \"created_with\": \"calculate_1_in_x_custom\",\n",
    "        \"temporal_input_dimension\": \"time_delta\" if has_time_delta else \"time\",\n",
    "        \"has_warming_level_dimension\": int(has_warming_level),\n",
    "    })\n",
    "    \n",
    "    # Add variable-level metadata\n",
    "    result[\"return_value\"].attrs.update({\n",
    "        \"long_name\": f\"{metric.capitalize()} {event_duration[0]}-{event_duration[1]} return values\",\n",
    "        \"units\": \"same as input data\",\n",
    "        \"description\": f\"Return values for {return_periods_str} year return periods using {distr} distribution\",\n",
    "    })\n",
    "    \n",
    "    result[\"p_values\"].attrs.update({\n",
    "        \"long_name\": \"Kolmogorov-Smirnov goodness-of-fit p-values\",\n",
    "        \"description\": f\"P-values for {distr} distribution fit quality. Values > 0.05 indicate good fit.\",\n",
    "    })\n",
    "\n",
    "    if VERBOSE: \n",
    "        print(\"✓ Output Dataset created successfully\")\n",
    "        print(f\"  Dimensions: {list(result.dims)}\")\n",
    "        print(f\"  Data variables: {list(result.data_vars)}\")\n",
    "        print(f\"  Coordinates: {list(result.coords)}\")\n",
    "        print()\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        print(\"BATCH MODE CALCULATION COMPLETE!\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"\\nResults summary:\")\n",
    "        print(f\"  Locations analyzed: {num_locations}\")\n",
    "        print(f\"  Simulations processed: {num_simulations}\")\n",
    "        if has_warming_level:\n",
    "            print(f\"  Warming levels: {num_warming_levels}\")\n",
    "        print(f\"  Return periods: {return_periods}\")\n",
    "        print(f\"  Output shape: {result['return_value'].shape}\")\n",
    "        print()\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def _process_one_simulation(one_sim_computed, return_periods, metric, distr, groupby, duration):\n",
    "    \"\"\"\n",
    "    Helper function to process a single simulation (or simulation-warming_level combo).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    one_sim_computed : xr.DataArray\n",
    "        Computed data for one simulation, with dims (time, location)\n",
    "    return_periods : list\n",
    "        List of return periods\n",
    "    metric : str\n",
    "        'max' or 'min'\n",
    "    distr : str\n",
    "        Distribution name\n",
    "    groupby : value or UNSET\n",
    "        Groupby parameter for get_block_maxima\n",
    "    duration : value or UNSET\n",
    "        Duration parameter for get_block_maxima\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tuple of (ret_val, p_val)\n",
    "        Return values and p-values for this simulation\n",
    "    \"\"\"\n",
    "    # Reshape data to have separate x and y dimensions\n",
    "    # threshold_tools expects (time, y, x) not (time, location)\n",
    "    # We create a 2D grid where each location is at a unique (y, x) position\n",
    "    n_locations = one_sim_computed.sizes['location']\n",
    "    \n",
    "    # Create a virtual 2D grid (can be a row or column)\n",
    "    # Use a row layout: y=1, x varies\n",
    "    one_sim_reshaped = one_sim_computed.assign_coords({\n",
    "        'x': ('location', np.arange(n_locations)),\n",
    "        'y': ('location', np.zeros(n_locations, dtype=int))  # All same y\n",
    "    })\n",
    "    \n",
    "    # Expand location into y and x dimensions\n",
    "    one_sim_reshaped = one_sim_reshaped.set_index(location=['y', 'x']).unstack('location')\n",
    "    \n",
    "    # Now data has dims: (time, y, x)\n",
    "    # where y=1 and x=n_locations\n",
    "    \n",
    "    # Get block maxima (handles multiple locations via y, x dimensions)\n",
    "    ams = get_block_maxima(\n",
    "        one_sim_reshaped,\n",
    "        extremes_type=metric,\n",
    "        duration=duration,\n",
    "        groupby=groupby,\n",
    "        check_ess=False,\n",
    "    )\n",
    "    \n",
    "    # Calculate return values (multiple_points=True handles y, x dimensions)\n",
    "    ret_val = get_return_value(\n",
    "        ams,\n",
    "        return_period=return_periods,\n",
    "        multiple_points=True,\n",
    "        distr=distr,\n",
    "        bootstrap_runs=1,  # increase for better estimates on confidence intervals\n",
    "    )[\"return_value\"]\n",
    "    \n",
    "    # Reshape back to location dimension\n",
    "    # Stack x and y back into a single location dimension\n",
    "    ret_val = ret_val.stack(location=['y', 'x']).reset_index('location', drop=True)\n",
    "    ret_val = ret_val.assign_coords(location=np.arange(n_locations))\n",
    "    \n",
    "    # Drop any extra coordinates that might cause concat issues\n",
    "    # Keep only location and one_in_x coordinates\n",
    "    coords_to_keep = ['location', 'one_in_x']\n",
    "    extra_coords = [c for c in ret_val.coords if c not in coords_to_keep]\n",
    "    if extra_coords:\n",
    "        ret_val = ret_val.drop_vars(extra_coords)\n",
    "    \n",
    "    # Goodness of fit\n",
    "    ks_result = get_ks_stat(ams, distr=distr, multiple_points=True)\n",
    "    \n",
    "    # Reshape p_values back to location dimension\n",
    "    p_val = ks_result.p_value.stack(location=['y', 'x']).reset_index('location', drop=True)\n",
    "    p_val = p_val.assign_coords(location=np.arange(n_locations))\n",
    "    \n",
    "    # Drop any extra coordinates from p_val\n",
    "    # Keep only location coordinate\n",
    "    extra_coords = [c for c in p_val.coords if c != 'location']\n",
    "    if extra_coords:\n",
    "        p_val = p_val.drop_vars(extra_coords)\n",
    "    \n",
    "    return ret_val, p_val\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83e0c8d",
   "metadata": {},
   "source": [
    "#### Step 3: Calculate 1-in-X Events for Effective Temperature\n",
    "\n",
    "First, we'll set-up a number of useful arguments, including *location*, *data variable retrieval*, *1-in-X approach return periods*, and *batch size*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3602de56-96e8-45e0-8f35-6f4d253725aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location -- modifiy to your desired locations!\n",
    "lat_lons = [\n",
    "    (34.0522, -118.2437),  # Los Angeles\n",
    "    (37.7749, -122.4194),  # San Francisco\n",
    "    (32.7157, -117.1611),  # San Diego\n",
    "    (38.5816, -121.4944),  # Sacramento\n",
    "    (37.3382, -121.8863),  # San Jose\n",
    "    (33.7701, -118.1937),  # Long Beach\n",
    "    (36.7783, -119.4179),  # Fresno\n",
    "    (33.9806, -117.3755),  # Riverside\n",
    "    (34.4208, -119.6982),  # Santa Barbara\n",
    "    (39.7285, -121.8375),  # Chico\n",
    "]\n",
    "\n",
    "num_pts = -1 # \"-1\" means use all locations\n",
    "space = 0.02 # Padding in degrees added around batch bounding boxes when requesting data (lat/lon buffer)\n",
    "\n",
    "# Set-up location dataframe\n",
    "locations = pd.DataFrame({\n",
    "    'lat': [x[0] for x in lat_lons[:num_pts]],\n",
    "    'lon': [x[1] for x in lat_lons[:num_pts]] \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2f1bc0-2e4c-453e-bf4a-4dc49c09e5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data selections (based on effective temperature) -- modify for a different custom metric!\n",
    "min_temp_var = \"Minimum air temperature at 2m\"\n",
    "max_temp_var = \"Maximum air temperature at 2m\"\n",
    "units = \"degF\"\n",
    "downscaling = \"Statistical\"  # LOCA2 data\n",
    "resolution = \"3 km\"\n",
    "timescale = \"daily\"\n",
    "scenario = [\"Historical Climate\", \"SSP 3-7.0\"]\n",
    "\n",
    "# 1-in-X specifications -- modify for custom return periods\n",
    "return_periods = [10, 100]\n",
    "\n",
    "# GWL approach\n",
    "approach = \"Warming Level\"\n",
    "wls = [2.0, 2.5]\n",
    "\n",
    "# Time-based approach -- uncomment if desired \n",
    "# time_slice = (2000, 2029) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0047a522-0c75-41f7-9e65-0cc5165f773d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch-size set-up\n",
    "batch_size = 5  # Recommended batch_size = 5 on AE JupyterHub\n",
    "batches = np.array_split(locations, len(locations)//batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4d74dc",
   "metadata": {},
   "source": [
    "#### Step 5: Define submission jobs with your custom metric for multiple locations. \n",
    "\n",
    "The next cell defines two worker functions intended to be submitted to a ThreadPoolExecutor (parallelization). `process_batch_gwl()` and `process_batch_time()` fetch min/max temperature data for a batch of locations, compute the effective-temperature metric, run the batch 1‑in‑X calculation, export results, and return the Dataset. These functions are **intended to be submitted** using `executor.submit(process_batch_time, (idx, batch))` or `executor.submit(process_batch_gwl, (idx, batch))` in the next step -- select the appropriate submit job based on either a GWL-based approach or a time-based approach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba38b71-73ce-466b-8256-f3463284f659",
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "\n",
    "## GWL-based approach version\n",
    "def process_batch_gwl(args: tuple[int, pd.DataFrame]) -> pd.DataFrame | None:\n",
    "    \"\"\"\n",
    "    Process a batch of locations for global warming level (GWL) based data retrieval\n",
    "    and analysis.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    args : tuple\n",
    "        Tuple containing (idx, batch)\n",
    "        idx : int\n",
    "            Batch index used for logging and output file naming.\n",
    "        batch : pandas.DataFrame\n",
    "            DataFrame containing at least 'lat' and 'lon' columns that define the\n",
    "            set of locations to process.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame or None\n",
    "        The resulting DataFrame produced by calculate_1_in_x_custom for the batch,\n",
    "        or None if an exception occurred during processing.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    None\n",
    "        All exceptions are caught internally; details are printed and the function\n",
    "        returns None on error.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Uses get_data with `approach` and `warming_levels` (wls) parameters to fetch\n",
    "      min/max temperature data for the spatial bounding box of the batch.\n",
    "    - Computes teff and 1-in-x statistics via calculate_teff and\n",
    "      calculate_1_in_x_custom.\n",
    "    - Exports the result to a file named \"teff_batch_gwl_{idx}\".\n",
    "    \"\"\"\n",
    "    idx, batch = args\n",
    "    \n",
    "    print(f\"[Batch {idx}] Starting processing...\")\n",
    "    \n",
    "    try:\n",
    "        min_lat = min(batch['lat'].values.tolist())\n",
    "        max_lat = max(batch['lat'].values.tolist())\n",
    "        min_lon = min(batch['lon'].values.tolist())\n",
    "        max_lon = max(batch['lon'].values.tolist())\n",
    "\n",
    "        min_temp_df = get_data(\n",
    "            variable=min_temp_var,\n",
    "            latitude=(min_lat - space, max_lat + space),\n",
    "            longitude=(min_lon - space, max_lon + space),\n",
    "            downscaling_method=downscaling,\n",
    "            timescale=timescale,\n",
    "            scenario=scenario,\n",
    "            resolution=resolution,\n",
    "            units=units,\n",
    "            approach=approach,\n",
    "            warming_levels=wls\n",
    "        )\n",
    "        max_temp_df = get_data(\n",
    "            variable=max_temp_var,\n",
    "            latitude=(min_lat - space, max_lat + space),\n",
    "            longitude=(min_lon - space, max_lon + space),\n",
    "            downscaling_method=downscaling,\n",
    "            timescale=timescale,\n",
    "            scenario=scenario,\n",
    "            resolution=resolution,\n",
    "            units=units,\n",
    "            approach=approach,\n",
    "            warming_levels=wls\n",
    "        )\n",
    "\n",
    "        # CUSTOM METRIC CALCULATION\n",
    "        teff_batch = calculate_teff(min_temp_df, max_temp_df)\n",
    "        result = calculate_1_in_x_custom(\n",
    "            teff_batch,\n",
    "            batch,\n",
    "            return_periods=return_periods,\n",
    "        )\n",
    "\n",
    "        export(result, filename=f\"teff_batch_gwl_{idx}\")        \n",
    "        return result\n",
    "        \n",
    "    except Exception as exc:\n",
    "        print(f\"\\n[Batch {idx}] ❌❌❌ EXCEPTION OCCURRED:\")\n",
    "        print(f\"[Batch {idx}] Exception type: {type(exc).__name__}\")\n",
    "        print(f\"[Batch {idx}] Exception message: {exc}\")\n",
    "        print(f\"[Batch {idx}] Full traceback:\")\n",
    "        print(\"-\" * 60)\n",
    "        traceback.print_exc()\n",
    "        print(\"-\" * 60)\n",
    "        return None\n",
    "\n",
    "\n",
    "## Time-based approach version\n",
    "def process_batch_time(args: tuple[int, pd.DataFrame]) -> pd.DataFrame | None:\n",
    "    \"\"\"\n",
    "    Process a batch of locations for time-based data retrieval and analysis.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    args : tuple\n",
    "        Tuple containing (idx, batch)\n",
    "        - idx : int\n",
    "            Batch index used for logging and output file naming.\n",
    "        - batch : pandas.DataFrame\n",
    "            DataFrame containing at least 'lat' and 'lon' columns that define the\n",
    "            set of locations to process.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame or None\n",
    "        The resulting DataFrame produced by calculate_1_in_x_custom for the batch,\n",
    "        or None if an exception occurred during processing.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    None\n",
    "        All exceptions are caught internally; details are printed and the function\n",
    "        returns None on error.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The function:\n",
    "    - Determines the spatial bounding box from batch lat/lon values.\n",
    "    - Fetches min/max temperature data for that bounding box.\n",
    "    - Computes teff and 1-in-x statistics.\n",
    "    - Exports the result to a file named \"teff_batch_time_{idx}\".\n",
    "    \"\"\"\n",
    "    idx, batch = args\n",
    "    \n",
    "    print(f\"[Batch {idx}] Starting processing...\")\n",
    "    \n",
    "    try:\n",
    "        min_lat = min(batch['lat'].values.tolist())\n",
    "        max_lat = max(batch['lat'].values.tolist())\n",
    "        min_lon = min(batch['lon'].values.tolist())\n",
    "        max_lon = max(batch['lon'].values.tolist())\n",
    "        \n",
    "        min_temp_df = get_data(\n",
    "            variable=min_temp_var,\n",
    "            latitude=(min_lat - space, max_lat + space),\n",
    "            longitude=(min_lon - space, max_lon + space),\n",
    "            downscaling_method=downscaling,\n",
    "            timescale=timescale,\n",
    "            time_slice=time_slice,\n",
    "            scenario=scenario,\n",
    "            resolution=resolution,\n",
    "            units=units\n",
    "        )\n",
    "        max_temp_df = get_data(\n",
    "            variable=max_temp_var,\n",
    "            latitude=(min_lat - space, max_lat + space),\n",
    "            longitude=(min_lon - space, max_lon + space),\n",
    "            downscaling_method=downscaling,\n",
    "            timescale=timescale,\n",
    "            time_slice=time_slice,\n",
    "            scenario=scenario,\n",
    "            resolution=resolution,\n",
    "            units=units\n",
    "        )\n",
    "\n",
    "        # CUSTOM METRIC CALCULATION\n",
    "        teff_batch = calculate_teff(min_temp_df, max_temp_df)\n",
    "        result = calculate_1_in_x_custom(\n",
    "            teff_batch,\n",
    "            batch,\n",
    "            return_periods=return_periods,\n",
    "        )\n",
    "\n",
    "        export(result, filename=f\"teff_batch_time_{idx}\")        \n",
    "        return result\n",
    "        \n",
    "    except Exception as exc:\n",
    "        print(f\"\\n[Batch {idx}] ❌❌❌ EXCEPTION OCCURRED:\")\n",
    "        print(f\"[Batch {idx}] Exception type: {type(exc).__name__}\")\n",
    "        print(f\"[Batch {idx}] Exception message: {exc}\")\n",
    "        print(f\"[Batch {idx}] Full traceback:\")\n",
    "        print(\"-\" * 60)\n",
    "        traceback.print_exc()\n",
    "        print(\"-\" * 60)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80259a61",
   "metadata": {},
   "source": [
    "#### Step 6: Run batch processing workers in parallel to compute 1-in-X results\n",
    "\n",
    "The following cell is the **core working cell** now that we have set-up a custom metric calculation and multi-thread processing. There are two options, depending on your approach (GWL or time-based). **We recommend commenting out the approach method that you do not use.** If you are using both approaches, leave both as uncommented. This will increase the run time. \n",
    "\n",
    "> [!NOTE]\n",
    "> **Recommendations**:\n",
    "> - `n_workers` configures the thread pool size. We recommend keeping the value small. `n_workers` should be tuned for available memory and API limits; the DataInterface monkey‑patch (cell 3) should be applied if using multiple threads to avoid race conditions.\n",
    "> - Only the first two batches are processed because of `batches[:2]`; change that slice to process more.\n",
    "> - Each submitted job receives `(idx, batch)` (matching the worker function signatures).\n",
    "> - Exceptions inside worker functions are captured both inside the worker (they return `None`) and here when accessing `future.result()`; both paths are handled.\n",
    "> - Final `results` contains only the returned datasets (or `None` for failed batches), ordered by batch index.\n",
    "\n",
    "> [!NOTE]\n",
    "> **Reminder**: This cell performs all of the processing, and therefore will take a number of **hours**. In testing, each worker took 3.5 hours to finish a batch of **50 locations**. When selecting your batch size, be aware of the potential runtime. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffc6ca0-1a0b-4e85-8698-6676071c933b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "n_workers = 2  # Adjust based on your system capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6defe99f-4342-4562-a64b-571135da99d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GWL-based submission\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=n_workers) as executor:\n",
    "    # Submit all batches for parallel processing\n",
    "    future_to_batch = {executor.submit(process_batch_gwl, args): args[0] for args in enumerate(batches[:2])}\n",
    "    for future in concurrent.futures.as_completed(future_to_batch):\n",
    "        idx = future_to_batch[future]\n",
    "        try:\n",
    "            result = future.result()\n",
    "            results.append((idx, result))\n",
    "        except Exception as exc:\n",
    "            print(f\"[Batch {idx}] generated an exception: {exc}\")\n",
    "\n",
    "\n",
    "## Time-based submission -- uncomment if desired\n",
    "# with concurrent.futures.ThreadPoolExecutor(max_workers=n_workers) as executor:\n",
    "#     # Submit all batches for parallel processing\n",
    "#     future_to_batch = {executor.submit(process_batch_time, args): args[0] for args in enumerate(batches[:2])}\n",
    "#     for future in concurrent.futures.as_completed(future_to_batch):\n",
    "#         idx = future_to_batch[future]\n",
    "#         try:\n",
    "#             result = future.result()\n",
    "#             results.append((idx, result))\n",
    "#         except Exception as exc:\n",
    "#             print(f\"[Batch {idx}] generated an exception: {exc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e07255f-2ef1-43bc-8f08-de6aa57c49f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, sort results by batch index\n",
    "results.sort(key=lambda x: x[0])\n",
    "results = [r[1] for r in results]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0629440e-0624-455d-9816-6ad1ba3f15c2",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to efficiently: \n",
    "1. Define and calculate a custom metric for extreme value analysis\n",
    "2. Apply a multi-treading approach for speed-optimized results for multiple locations\n",
    "\n",
    "Without multi-threading, a batch size of **1600 locations** would take approximately **27 days to complete**. With multi-threading, the same batch size completes in **28 hours**.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
