{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e0361f0",
   "metadata": {},
   "source": [
    "# Drought Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64641b82",
   "metadata": {},
   "source": [
    "The goal of this notebook is to explore calculating two drought metrics (PDSI and EDDI) using WRF data in the AE catalog."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abad1d8",
   "metadata": {},
   "source": [
    "To start, we will calculate Potential Evapotranspiration, since that variable is needed for both PDSI and EDDI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98506237",
   "metadata": {},
   "source": [
    "**Intended Application:** FILL HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc35f46e",
   "metadata": {},
   "source": [
    "**Runtime:** With the default settings, this notebook takes approximately 25 minutes to run from start to finish. Modifications to selections may increase the runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ab2f9c",
   "metadata": {},
   "source": [
    "**Troubleshooting:** Getting an `IndexError: index 40 is out of bounds for axis 0 with size 40` when trying to run the PDSI calculation? Try changing your lat/lon or point of interest further away from the boundaries of our 3 km WRF domain (as seen in [<span style=\"color:blue\">this graphic</span>](https://analytics.cal-adapt.org/faq/#what-data-is-available))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5de287f",
   "metadata": {},
   "source": [
    "### Using the Penman-Monteith method (most physically accurate) to calculate Potential Evapotranspiration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c04c18f",
   "metadata": {},
   "source": [
    "**Variables needed:**\n",
    "- `tasmin`\n",
    "- `tasmax`\n",
    "- `relative humidity`\n",
    "- `radiation flux`\n",
    "    - rsds\n",
    "    - rsus\n",
    "    - rlds\n",
    "    - rlus\n",
    "- `wind speed (10m wind will be converted to 2m)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3babc840",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7e19917",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xclim\n",
    "import os\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot  as plt\n",
    "from pyproj import CRS\n",
    "from climakitae.core.data_interface import get_data\n",
    "from climakitae.core.data_load import load\n",
    "from climakitae.core.data_export import export\n",
    "from climakitae.util.utils import add_dummy_time_to_wl\n",
    "from climakitae.util.utils import reproject_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5626f6",
   "metadata": {},
   "source": [
    "### Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b31d378",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat = 37.787964\n",
    "lon = -122.065063\n",
    "\n",
    "variables_dict = {\n",
    "    \"tasmax\": \"Maximum air temperature at 2m\",\n",
    "    \"tasmin\": \"Minimum air temperature at 2m\",\n",
    "    \"hurs\": \"Relative humidity\",\n",
    "    \"rsds\": \"Instantaneous downwelling shortwave flux at bottom\",\n",
    "    \"rsus\": \"Instantaneous upwelling shortwave flux at bottom\",\n",
    "    \"rlds\": \"Instantaneous downwelling longwave flux at bottom\",\n",
    "    \"rlus\": \"Instantaneous upwelling longwave flux at bottom\",\n",
    "    \"wspd10mean\": \"Mean wind speed at 10m\",\n",
    "    \"precip\": \"Precipitation (total)\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a9173e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a `intermediate_data` folder to hold intermediate data variables needed for PET\n",
    "!mkdir $input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03173361",
   "metadata": {},
   "source": [
    "**IMPORTANT NOTE**: The following cell saves out the intermediate data that is required for PET into an `input_data` directory. If you change the lat/lon coordinate from above and DON'T delete or move the data already inside the `input_data` directory, it will just re-read the same data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bbd1dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Maximum air temperature at 2m\n",
      "WARNING FOR WARMING LEVELS APPROACH\n",
      "-----------------------------------\n",
      "There may be NaNs in your data for certain simulation/warming level combinations if the warming level is not reached for that particular simulation before the year 2100. \n",
      "\n",
      "This does not mean you have missing data, but rather a feature of how the data is combined in retrieval to return a single data object. \n",
      "\n",
      "If you want to remove these empty simulations, it is recommended to first subset the data object by each individual warming level and then dropping NaN values.\n",
      "Processing data to read 12.03 MB of data into memory... \n",
      "[########################################] | 100% Completed | 30.77 s\n",
      "Complete!\n",
      "Computing Minimum air temperature at 2m\n",
      "WARNING FOR WARMING LEVELS APPROACH\n",
      "-----------------------------------\n",
      "There may be NaNs in your data for certain simulation/warming level combinations if the warming level is not reached for that particular simulation before the year 2100. \n",
      "\n",
      "This does not mean you have missing data, but rather a feature of how the data is combined in retrieval to return a single data object. \n",
      "\n",
      "If you want to remove these empty simulations, it is recommended to first subset the data object by each individual warming level and then dropping NaN values.\n",
      "Processing data to read 12.03 MB of data into memory... \n",
      "[########################################] | 100% Completed | 30.85 s\n",
      "Complete!\n",
      "Computing Relative humidity\n",
      "WARNING FOR WARMING LEVELS APPROACH\n",
      "-----------------------------------\n",
      "There may be NaNs in your data for certain simulation/warming level combinations if the warming level is not reached for that particular simulation before the year 2100. \n",
      "\n",
      "This does not mean you have missing data, but rather a feature of how the data is combined in retrieval to return a single data object. \n",
      "\n",
      "If you want to remove these empty simulations, it is recommended to first subset the data object by each individual warming level and then dropping NaN values.\n",
      "Processing data to read 12.03 MB of data into memory... \n",
      "[########################################] | 100% Completed | 34.35 s\n",
      "Complete!\n",
      "Computing Instantaneous downwelling shortwave flux at bottom\n",
      "WARNING FOR WARMING LEVELS APPROACH\n",
      "-----------------------------------\n",
      "There may be NaNs in your data for certain simulation/warming level combinations if the warming level is not reached for that particular simulation before the year 2100. \n",
      "\n",
      "This does not mean you have missing data, but rather a feature of how the data is combined in retrieval to return a single data object. \n",
      "\n",
      "If you want to remove these empty simulations, it is recommended to first subset the data object by each individual warming level and then dropping NaN values.\n",
      "Processing data to read 12.03 MB of data into memory... \n",
      "[########################################] | 100% Completed | 26.80 s\n",
      "Complete!\n",
      "Computing Instantaneous upwelling shortwave flux at bottom\n",
      "WARNING FOR WARMING LEVELS APPROACH\n",
      "-----------------------------------\n",
      "There may be NaNs in your data for certain simulation/warming level combinations if the warming level is not reached for that particular simulation before the year 2100. \n",
      "\n",
      "This does not mean you have missing data, but rather a feature of how the data is combined in retrieval to return a single data object. \n",
      "\n",
      "If you want to remove these empty simulations, it is recommended to first subset the data object by each individual warming level and then dropping NaN values.\n",
      "Processing data to read 288.72 MB of data into memory... \n",
      "[########################################] | 100% Completed | 177.59 s\n",
      "Complete!\n",
      "Computing Instantaneous downwelling longwave flux at bottom\n",
      "WARNING FOR WARMING LEVELS APPROACH\n",
      "-----------------------------------\n",
      "There may be NaNs in your data for certain simulation/warming level combinations if the warming level is not reached for that particular simulation before the year 2100. \n",
      "\n",
      "This does not mean you have missing data, but rather a feature of how the data is combined in retrieval to return a single data object. \n",
      "\n",
      "If you want to remove these empty simulations, it is recommended to first subset the data object by each individual warming level and then dropping NaN values.\n",
      "Processing data to read 12.03 MB of data into memory... \n",
      "[########################################] | 100% Completed | 34.95 s\n",
      "Complete!\n",
      "Computing Instantaneous upwelling longwave flux at bottom\n",
      "WARNING FOR WARMING LEVELS APPROACH\n",
      "-----------------------------------\n",
      "There may be NaNs in your data for certain simulation/warming level combinations if the warming level is not reached for that particular simulation before the year 2100. \n",
      "\n",
      "This does not mean you have missing data, but rather a feature of how the data is combined in retrieval to return a single data object. \n",
      "\n",
      "If you want to remove these empty simulations, it is recommended to first subset the data object by each individual warming level and then dropping NaN values.\n",
      "Processing data to read 288.72 MB of data into memory... \n",
      "[########################################] | 100% Completed | 269.25 s\n",
      "Complete!\n",
      "Computing Mean wind speed at 10m\n",
      "WARNING FOR WARMING LEVELS APPROACH\n",
      "-----------------------------------\n",
      "There may be NaNs in your data for certain simulation/warming level combinations if the warming level is not reached for that particular simulation before the year 2100. \n",
      "\n",
      "This does not mean you have missing data, but rather a feature of how the data is combined in retrieval to return a single data object. \n",
      "\n",
      "If you want to remove these empty simulations, it is recommended to first subset the data object by each individual warming level and then dropping NaN values.\n",
      "Processing data to read 12.03 MB of data into memory... \n",
      "[########################################] | 100% Completed | 43.18 s\n",
      "Complete!\n",
      "Computing Precipitation (total)\n",
      "WARNING FOR WARMING LEVELS APPROACH\n",
      "-----------------------------------\n",
      "There may be NaNs in your data for certain simulation/warming level combinations if the warming level is not reached for that particular simulation before the year 2100. \n",
      "\n",
      "This does not mean you have missing data, but rather a feature of how the data is combined in retrieval to return a single data object. \n",
      "\n",
      "If you want to remove these empty simulations, it is recommended to first subset the data object by each individual warming level and then dropping NaN values.\n",
      "Processing data to read 12.03 MB of data into memory... \n",
      "[########################################] | 100% Completed | 26.95 s\n",
      "Complete!\n"
     ]
    }
   ],
   "source": [
    "### Retrieving the different variables needed for PET\n",
    "datas = []\n",
    "\n",
    "for _, (variable, var_long_name) in enumerate(variables_dict.items()):\n",
    "\n",
    "    file_path = f\"tmp_data/{variable}_daily.nc\"\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"Reading {variable} from file.\")\n",
    "        da = xr.open_dataarray(file_path)\n",
    "        \n",
    "    else:\n",
    "        # continue\n",
    "        print(f\"Computing {var_long_name}\")\n",
    "        ae_var_name = var_long_name\n",
    "        timescale = 'daily'\n",
    "        # if variable == 'tasmin':\n",
    "        #     ae_var_name = 'Air Temperature at 2m'\n",
    "        if variable == 'rlus' or variable == 'rsus':\n",
    "            timescale = 'hourly'\n",
    "        da = get_data(\n",
    "            variable=ae_var_name,\n",
    "            resolution='3 km',\n",
    "            timescale=timescale,\n",
    "            latitude=(lat - 0.1, lat + 0.1),\n",
    "            longitude=(lon - 0.1, lon + 0.1),\n",
    "            approach=\"Warming Level\",\n",
    "            warming_level=[0.8, 1.5, 2.0, 3.0],\n",
    "            # scenario='SSP 3-7.0',\n",
    "            # time_slice=(2030, 2060),\n",
    "            downscaling_method=\"Dynamical\"\n",
    "        )\n",
    "        da = load(add_dummy_time_to_wl(da), progress_bar=True)\n",
    "        if variable == 'tasmin':\n",
    "            agg_da = da.squeeze().resample(time='D').min()\n",
    "        elif variable == 'tasmax':\n",
    "            agg_da = da.squeeze().resample(time='D').max()\n",
    "        elif variable == 'precip':\n",
    "            agg_da = da.squeeze().resample(time='D').sum()\n",
    "        else:\n",
    "            agg_da = da.squeeze().resample(time='D').mean()\n",
    "        agg_da.to_netcdf(file_path)  # Save for reuse\n",
    "        da = agg_da\n",
    "\n",
    "    datas.append(da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df54b41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating daily variables for all hourly variables\n",
    "tasmin = datas[0]\n",
    "tasmax = datas[1]\n",
    "hurs = datas[2] / 100 # Convert from % to fraction\n",
    "new_hurs = hurs.assign_attrs(units='1')\n",
    "rsds = datas[3]\n",
    "rsus = datas[4]\n",
    "rlds = datas[5]\n",
    "rlus = datas[6]\n",
    "sfcWind = datas[7]\n",
    "precip = datas[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32f5bcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating PET from xclim\n",
    "pet_calc = xclim.indices.potential_evapotranspiration(\n",
    "    tasmin=tasmin,\n",
    "    tasmax=tasmax,\n",
    "    hurs=new_hurs,\n",
    "    rsds=rsds,\n",
    "    rsus=rsus,\n",
    "    rlds=rlds,\n",
    "    rlus=rlus,\n",
    "    sfcWind=sfcWind,\n",
    "    method=\"FAO_PM98\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58cbcd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preserving CRS and a spatial mask for later\n",
    "crs = CRS.from_cf(pet_calc['Lambert_Conformal'].attrs)\n",
    "spatial_mask = ~pet_calc.isel(warming_level=0, time=0, simulation=0).isnull()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d01841",
   "metadata": {},
   "source": [
    "# PDSI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415dcfc7",
   "metadata": {},
   "source": [
    "Here, we will use the PDSI function from the `climate_indices` library, which is also what drought.gov has referenced. However, we will install a specific commit of the package that is compatible with the AE hub environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "617da8ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/monocongo/climate_indices.git@43c5451\n",
      "  Cloning https://github.com/monocongo/climate_indices.git (to revision 43c5451) to /tmp/pip-req-build-_lj75s7h\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/monocongo/climate_indices.git /tmp/pip-req-build-_lj75s7h\n",
      "\u001b[33m  WARNING: Did not find branch or tag '43c5451', assuming revision or ref.\u001b[0m\u001b[33m\n",
      "\u001b[0m  Running command git checkout -q 43c5451\n",
      "  Resolved https://github.com/monocongo/climate_indices.git to commit 43c5451\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: cftime>=1.6.4 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from climate_indices==2.1.0) (1.6.4)\n",
      "Requirement already satisfied: dask>=2024.12.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from climate_indices==2.1.0) (2025.5.1)\n",
      "Requirement already satisfied: h5netcdf>=1.4.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from climate_indices==2.1.0) (1.6.1)\n",
      "Requirement already satisfied: scipy>=1.14.1 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from climate_indices==2.1.0) (1.15.2)\n",
      "Requirement already satisfied: xarray>=2024.11.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from climate_indices==2.1.0) (2025.4.0)\n",
      "Requirement already satisfied: numpy>=1.26.0b1 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from cftime>=1.6.4->climate_indices==2.1.0) (2.2.6)\n",
      "Requirement already satisfied: click>=8.1 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from dask>=2024.12.0->climate_indices==2.1.0) (8.2.1)\n",
      "Requirement already satisfied: cloudpickle>=3.0.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from dask>=2024.12.0->climate_indices==2.1.0) (3.1.1)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from dask>=2024.12.0->climate_indices==2.1.0) (2025.5.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from dask>=2024.12.0->climate_indices==2.1.0) (25.0)\n",
      "Requirement already satisfied: partd>=1.4.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from dask>=2024.12.0->climate_indices==2.1.0) (1.4.2)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from dask>=2024.12.0->climate_indices==2.1.0) (6.0.2)\n",
      "Requirement already satisfied: toolz>=0.10.0 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from dask>=2024.12.0->climate_indices==2.1.0) (1.0.0)\n",
      "Requirement already satisfied: h5py in /srv/conda/envs/notebook/lib/python3.12/site-packages (from h5netcdf>=1.4.0->climate_indices==2.1.0) (3.13.0)\n",
      "Requirement already satisfied: locket in /srv/conda/envs/notebook/lib/python3.12/site-packages (from partd>=1.4.0->dask>=2024.12.0->climate_indices==2.1.0) (1.0.0)\n",
      "Requirement already satisfied: pandas>=2.1 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from xarray>=2024.11.0->climate_indices==2.1.0) (2.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from pandas>=2.1->xarray>=2024.11.0->climate_indices==2.1.0) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from pandas>=2.1->xarray>=2024.11.0->climate_indices==2.1.0) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from pandas>=2.1->xarray>=2024.11.0->climate_indices==2.1.0) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /srv/conda/envs/notebook/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas>=2.1->xarray>=2024.11.0->climate_indices==2.1.0) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "# Pip install the specific commit of the package that supports AE package versions\n",
    "!pip install git+https://github.com/monocongo/climate_indices.git@43c5451"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0bc9fcf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Making imports from `climate_indices` package\n",
    "import climate_indices\n",
    "from climate_indices.palmer import pdsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "deb75906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resampling PET and precip to monthly since the function only takes monthly variables\n",
    "mon_pet = (pet_calc * 86400 / 25.4).resample(time='1ME').sum()\n",
    "mon_precip = (precip / 25.4).resample(time='1ME').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ffd18cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Combining WL objects together, historical WL as 2000-2030, future WL as 2030-2060\n",
    "def combine_wl_to_dummy_time(\n",
    "    da: xr.DataArray,\n",
    "    baseline_wl: float,\n",
    "    future_wls: list[float],\n",
    "    start_date: str = \"2000-01-31\",\n",
    ") -> xr.DataArray:\n",
    "    \"\"\"\n",
    "    Combine baseline warming level with multiple future warming levels into one\n",
    "    DataArray along a new 'combined_wl' dimension.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    da : xr.DataArray\n",
    "        Original data with dims including 'warming_level' and 'time'.\n",
    "    baseline_wl : float\n",
    "        The warming level used for the first time segment.\n",
    "    future_wls : list of float\n",
    "        Warming levels to concatenate after baseline.\n",
    "    start_date : str\n",
    "        Start date for the combined time series (monthly freq).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    xr.DataArray\n",
    "        Combined DataArray with new dimension 'combined_wl' and coordinate labels like \"0.8 to 1.5\".\n",
    "    \"\"\"\n",
    "    months_per_wl = da.sizes['time']\n",
    "    total_months = 2 * months_per_wl\n",
    "    new_time = pd.date_range(start_date, periods=total_months, freq='ME')\n",
    "\n",
    "    combined_list = []\n",
    "    combined_labels = []\n",
    "\n",
    "    for fw in future_wls:\n",
    "        da_base = da.sel(warming_level=baseline_wl)\n",
    "        da_future = da.sel(warming_level=fw)\n",
    "\n",
    "        combined = xr.concat([da_base, da_future], dim='time')\n",
    "        combined = combined.assign_coords(time=new_time)\n",
    "\n",
    "        wl_flag = np.array([baseline_wl] * months_per_wl + [fw] * months_per_wl)\n",
    "        combined = combined.assign_coords(warming_level_flag=('time', wl_flag))\n",
    "\n",
    "        combined_list.append(combined)\n",
    "        combined_labels.append(f\"{int(baseline_wl * 10):02d}_to_{int(fw * 10):02d}\")\n",
    "\n",
    "    combined_da = xr.concat(combined_list, dim='combined_wl')\n",
    "    combined_da = combined_da.assign_coords(combined_wl=combined_labels)\n",
    "\n",
    "    return combined_da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "119fa7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating one Dataset of PET and precip with WLs combined\n",
    "mon_pet_transform = combine_wl_to_dummy_time(mon_pet, baseline_wl=0.8, future_wls=[1.5,2.0,3.0])\n",
    "mon_precip_transform = combine_wl_to_dummy_time(mon_precip, baseline_wl=0.8, future_wls=[1.5,2.0,3.0])\n",
    "\n",
    "combined_ds = xr.Dataset({'precip': mon_precip_transform, 'pet': mon_pet_transform})\n",
    "\n",
    "# Applying spatial mask\n",
    "combined_ds = combined_ds.where(spatial_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6b5de48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to vectorize PDSI calculation across `combined_ds` dimensions.\n",
    "def calc_pdsi(timeseries: xr.Dataset):\n",
    "    \"\"\"\n",
    "    Compute the Palmer Drought Severity Index (PDSI) from a Dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    timeseries : xarray.Dataset\n",
    "        Dataset containing 'precip' and 'pet' variables with a time dimension.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    xarray.DataArray\n",
    "        PDSI values along the time dimension.\n",
    "    \"\"\"\n",
    "    # Extracting precip and PET by each timeseries and calculating PDSI\n",
    "    precip = timeseries['precip'].squeeze()\n",
    "    pet = timeseries['pet'].squeeze()\n",
    "    \n",
    "    pdsi_calc = pdsi(\n",
    "        precips=precip.values,\n",
    "        pet=pet.values,\n",
    "        awc=5,\n",
    "        data_start_year=2000,\n",
    "        calibration_year_initial=2000,\n",
    "        calibration_year_final=2030,\n",
    "    )\n",
    "    retval = xr.DataArray(pdsi_calc[0], coords={\"time\": precip.time.values}, dims=['time'])\n",
    "    \n",
    "    # Clipping PDSI to realistic values\n",
    "    return retval.clip(min=-10, max=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c625f3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applies the PDSI function across all dimensions so that a timeseries of PET/precip is always being passed into `pdsi`\n",
    "pdsi_da = combined_ds_extended.groupby([\n",
    "    'combined_wl',\n",
    "    'x',\n",
    "    'y',\n",
    "    'simulation'\n",
    "]).apply(\n",
    "    lambda timeseries: calc_pdsi(timeseries)\n",
    ")\n",
    "\n",
    "# Writing crs and reprojecting PDSI to lat/lon\n",
    "pdsi_da = pdsi_da.rio.write_crs(crs.to_wkt())\n",
    "pdsi_da = pdsi_da.transpose('time', 'combined_wl', 'simulation', 'y', 'x')\n",
    "pdsi_latlon = reproject_data(pdsi_da, 'EPSG:4326')\n",
    "del pdsi_latlon.attrs[\"_FillValue\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc3157f",
   "metadata": {},
   "source": [
    "### Exporting the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d745c44",
   "metadata": {},
   "source": [
    "The results will have the following dimensions:\n",
    "- time\n",
    "- wl (showing which WL PDSI was calibrated on, and then which WL PDSI was calculated on)\n",
    "- lat\n",
    "- lon\n",
    "- simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dc40f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning and labeling the data before exporting it in the next cell\n",
    "final_pdsi = pdsi_latlon.isel(time=slice(360, 720))\n",
    "final_pdsi = final_pdsi.rename({'combined_wl': 'wl'}).rename(\"pdsi\")\n",
    "final_pdsi = final_pdsi.assign_attrs({\n",
    "    \"long_name\": \"Palmer Drought Severity Index\",\n",
    "    \"units\": \"from -10 (dry) to +10 (wet)\",\n",
    "})\n",
    "pdsi_filename = f\"pdsi_wl_lat{str(lat).replace('.', '_')}_lon{str(lon).replace('.', '_')}.nc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6f3455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting the DataArray\n",
    "if os.path.exists(pdsi_filename):\n",
    "    raise Exception(\n",
    "        (\n",
    "            f\"File {pdsi_filename} exists. \"\n",
    "            \"Please either delete that file from the work space \"\n",
    "            \"or specify a new file name here.\"\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    final_pdsi.to_netcdf(pdsi_filename, encoding={\"pdsi\": {\"_FillValue\": -9999.0}})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc2e324",
   "metadata": {},
   "source": [
    "## EDDI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0645679",
   "metadata": {},
   "source": [
    "Now, we will calculate EDDI using PET."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38a1969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import `standardized_index` from xclim, which we will apply to our PET data object to generate EDDI\n",
    "from xclim.indices.stats import standardized_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3228b9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_eddi(timeseries: xr.DataArray):\n",
    "    \"\"\"\n",
    "    Compute the Evaporative Demand Drought Index (EDDI) for a time series.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    timeseries : xarray.DataArray\n",
    "        1D time series of ET₀. NaNs are skipped.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    xarray.DataArray\n",
    "        EDDI values. Positive = dry, negative = wet.\n",
    "    \"\"\"\n",
    "    eddi = standardized_index(\n",
    "        da=timeseries,\n",
    "        freq='MS',\n",
    "        window=1,\n",
    "        dist=\"gamma\",\n",
    "        method=\"ML\",\n",
    "        zero_inflated=False,\n",
    "        fitkwargs={},\n",
    "        cal_start=\"2000-01-31\",\n",
    "        cal_end=\"2029-12-31\"\n",
    "    )\n",
    "    # Clipping EDDI to realistic values\n",
    "    retval = eddi.clip(min=-2.5, max=2.5)\n",
    "    return retval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1240839c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applies the `calc_eddi` function across all dimensions so that a timeseries of PET is always being passed into `calc_eddi`\n",
    "eddi_da = combined_ds['pet'].groupby([\n",
    "    'combined_wl',\n",
    "    'x',\n",
    "    'y',\n",
    "    'simulation'\n",
    "]).apply(\n",
    "    lambda timeseries: calc_eddi(timeseries)\n",
    ")\n",
    "\n",
    "# Writing crs and reprojecting EDDI to lat/lon\n",
    "eddi_da = eddi_da.rio.write_crs(crs.to_wkt())\n",
    "eddi_da = eddi_da.transpose('time', 'combined_wl', 'simulation', 'y', 'x')\n",
    "eddi_da_latlon = reproject_data(eddi_da, 'EPSG:4326')\n",
    "del eddi_da_latlon.attrs[\"_FillValue\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45176f23",
   "metadata": {},
   "source": [
    "### Exporting the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59ee541",
   "metadata": {},
   "source": [
    "The results will have the following dimensions:\n",
    "- time\n",
    "- wl (showing which WL EDDI was calibrated on, and then which WL EDDI was calculated on)\n",
    "- lat\n",
    "- lon\n",
    "- simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86782f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving these results and cleaning the data\n",
    "final_eddi = eddi_da_latlon.isel(time=slice(360, 720))\n",
    "final_eddi = final_eddi.rename({'combined_wl': 'wl'}).rename(\"eddi\")\n",
    "final_eddi = final_eddi.assign_attrs({\n",
    "    \"long_name\": \"Evaporative Demand Drought Index\",\n",
    "    \"units\": \"from -2.5 (wet) to +2.5 (dry)\",\n",
    "})\n",
    "eddi_filename = f\"eddi_wl_lat{str(lat).replace('.', '_')}_lon{str(lon).replace('.', '_')}.nc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707286f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting the DataArray\n",
    "if os.path.exists(eddi_filename):\n",
    "    raise Exception(\n",
    "        (\n",
    "            f\"File {eddi_filename} exists. \"\n",
    "            \"Please either delete that file from the work space \"\n",
    "            \"or specify a new file name here.\"\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    final_eddi.to_netcdf(eddi_filename, encoding={\"eddi\": {\"_FillValue\": -9999.0}})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
