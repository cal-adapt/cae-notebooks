{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c64909f",
   "metadata": {},
   "source": [
    "# Accessing quality-controlled historical weather station data\n",
    "\n",
    "The <span style=\"color:#FF0000\">[Historical Observations Data Platform](https://eaglerockanalytics.com/project/historical-observations-data-platform/)</span> is a cloud-based historical weather observations database that enables access to high-quality, open climate and weather data. The Platform responds to a broad-scale need to understand weather and climate information including the severity, duration, frequency, and rate of change over time of extreme weather events, as well as supporting projections downscaling efforts. The Platform implements stringent, customized Quality Assurance / Quality Control (QA/QC) protocols in line with international conventions, with updates relevant to the energy sector to accurate capture extremes. The Platform has sourced publicly accessible weather observation stations from 27 networks throughout western North America, with a total of **14,927 quality-controlled and standardized stations** spanning 1980-2022. \n",
    "\n",
    "For more information on the QA/QC and standardization process, please check out the open-access methods and code at the <span style=\"color:#FF0000\">[Historical Observations Data Platform code repository](https://github.com/Eagle-Rock-Analytics/historical-obs-platform)</span>. A station list of all available quality-controlled and standardized stations is available in our <span style=\"color:#FF0000\">[data bucket](https://cadcat.s3.amazonaws.com/histwxstns/historical_wx_stations.csv)</span>.\n",
    "\n",
    "**Runtime**: < 1 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc34b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import intake \n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d4eb1c",
   "metadata": {},
   "source": [
    "First, open the catalog using `intake`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccad77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = intake.open_esm_datastore(\"https://cadcat.s3.amazonaws.com/histwxstns/era-hdp-collection.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989e64b5",
   "metadata": {},
   "source": [
    "Next, view the catalog in table format. You can inspect the first few rows by calling `.head()` on the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0389d0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access catalog as dataframe and inspect the first few rows\n",
    "cat_df = cat.df\n",
    "cat_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57cfbd5",
   "metadata": {},
   "source": [
    "View all the weather station networks by using the following code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c743e382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See all network options \n",
    "cat_df[\"network_id\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4864dd8c",
   "metadata": {},
   "source": [
    "You can also filter the catalog to see all stations within a network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687f661b",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_network = \"ASOSAWOS\"\n",
    "cat_df[cat_df[\"network_id\"] == my_network]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddfe314",
   "metadata": {},
   "source": [
    "You can subset the catalog and read in the cloud-optimized data as `xarray.Dataset` objects using the method shown below. To change the data downloaded, simply modify the inputs in the dictionary `query`. These inputs must correspond to valid options in the catalog. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a169749a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your query here \n",
    "query = {\n",
    "    \"network_id\": \"ASOSAWOS\",  # Name of the network\n",
    "    \"station_id\": [\"ASOSAWOS_72288023152\",\"ASOSAWOS_72389093193\",\"ASOSAWOS_72297603166\",\"ASOSAWOS_72493023230\"] # List of stations to get data for \n",
    "}\n",
    "\n",
    "# Subset catalog \n",
    "cat_subset = cat.search(**query)\n",
    "\n",
    "# View the data you've selected before downloading\n",
    "cat_subset.df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bc9a9b",
   "metadata": {},
   "source": [
    "Then, you can download all the files. The files will be downloaded as a dictionary, in which each key is a string description of the data, and the item is the data object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6eaf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataset dictionary \n",
    "dsets = cat_subset.to_dataset_dict(\n",
    "    xarray_open_kwargs={'consolidated':False},\n",
    "    storage_options={'anon':True}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21362aa5",
   "metadata": {},
   "source": [
    "To see all the string IDs for the Datasets in the dictionary, you can print them with the following code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da39ed95",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(dsets.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da72e866",
   "metadata": {},
   "source": [
    "You can easily access the files in the dictionary using the following format: \n",
    "```\n",
    "dsets[<string ID of data>]\n",
    "```\n",
    "The string ID of the data is constructed using both the network ID and the station ID for each individual weather station. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55e7d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve a single file\n",
    "ds = dsets[\"ASOSAWOS.ASOSAWOS_72389093193\"]\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69020ce",
   "metadata": {},
   "source": [
    "## Make a quick plot of the data \n",
    "`xarray` has some nice mapping features that enable you to quickly generate a plot for a single timestep. This lets you get a sense for the data you read in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313a99bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_to_plot = \"tas\"\n",
    "ds.squeeze()[variable_to_plot].plot(x=\"time\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfb215d-ab20-4bc5-bce8-4be8fc29c1f5",
   "metadata": {},
   "source": [
    "## Subset the historical weather stations for a region\n",
    "\n",
    "If you're interested in historical weather observation stations in a specific area, you can also subset the full archive of stations to identify those that you are interested in. We will read in the Historical Data Platform station list, which provides the coordinates, dates of coverage, source network, location information, and the *total number of observations* for each station. \n",
    "\n",
    "For this example, we will show you how to read a shapefile of historic wildfire boundaries in San Diego country using publically available data from the [San Diego Regional Data Warehouse](https://geo.sandag.org/portal/apps/experiencebuilder/experience/?id=fad9e9c038c84f799b5378e4cc3ed068#data_s=id%3AdataSource_1-0%3A202). Next, we'll subset the station list to get all the weather stations available in the fire zone for the [Cedar fire in 2003](https://www.sandiego.gov/fire/about/major-fires-incidents/2003-cedar-fire), which is one of the largest wildfires in California's history (and, as of 2025, the largest wildfire in San Diego county). Lastly, we'll export the subsetted station list and make a plot of the fire boundary with the station locations overlayed. \n",
    "\n",
    "You could easily modify this workflow for your own shapefiles, by using **web link to an open access shapefile** or **uploading your own shapefile** to your Hub instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400f678f-6b2c-4fe5-bf2d-b4c3894ade33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the Historical Data Platform station list\n",
    "hdp_stns = pd.read_csv(\"https://cadcat.s3.amazonaws.com/histwxstns/historical_wx_stations.csv\")\n",
    "\n",
    "# Convert to a GeoDataFrame so we can use the geometry column for subsetting\n",
    "hdp_stns = gpd.GeoDataFrame(hdp_stns, geometry=gpd.GeoSeries.from_wkt(hdp_stns[\"geometry\"]), crs=\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959c22c7",
   "metadata": {},
   "source": [
    "The open source Python library [geopandas](https://geopandas.org/en/stable/getting_started/introduction.html) makes working with shapefiles in python easy; you can read in a shapefile using one line of code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a0d6d0-7139-4836-94fb-80efa9efb323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Region of Interest shapefile\n",
    "roi = gpd.read_file(\"https://geo.sandag.org/server/rest/directories/downloads/Fire_Burn_History_shapefile.zip\") # Replace this with your own shapefile!\n",
    "\n",
    "# Convert to projection of HDP data\n",
    "roi = roi.to_crs(hdp_stns.crs) \n",
    "\n",
    "# Subset the shapefile to just get the Cedar fire of 2003 \n",
    "cedar_fire_2003 = roi[(roi[\"FIRE_NAME\"]==\"CEDAR\") & (roi[\"YEAR_\"] == 2003)]\n",
    "cedar_fire_2003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b564c79-28e1-4a2f-9d10-4e693d092de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clip the stationlist to subset within your area of interest\n",
    "stns_within_area = gpd.sjoin(hdp_stns, cedar_fire_2003, how='inner', predicate='intersects').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16c24b2-f50e-43f4-a7e5-f4615e967248",
   "metadata": {},
   "source": [
    "This subset are the stations within the designated area from your submitted shapefile! You can easily export this list now for your own information, and use it to look up specific stations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5b6ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the subset portion of station list\n",
    "stns_within_area.to_csv(\"subset_station_list.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586e884a",
   "metadata": {},
   "source": [
    "Let's also make a quick visualization of the data to better understand the subsetting process and the geographic boundaries of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31af2c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,10))\n",
    "cedar_fire_2003.plot(ax=ax, color='rosybrown')\n",
    "stns_within_area.plot(ax=ax, color='darkblue', markersize=12, label=\"station\")\n",
    "ax.legend()\n",
    "ax.set_title(\"Weather stations within the Cedar Fire boundary\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e7664d-cc10-4762-8205-d12371e6b626",
   "metadata": {},
   "source": [
    "Want a more interactive way to view the data? Use `stns.explore()`, a geopandas method that will generate an interactive map where you can zoom, pan, and click features to see their attributes. Note, this map may take some time to load. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b1f85c-015b-4d2b-9b13-aaf6fd65b077",
   "metadata": {},
   "outputs": [],
   "source": [
    "stns_within_area.explore()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (climakitae)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
