{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb166e85-ccb3-4754-8472-32f67be37d30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-15T19:24:55.032455Z",
     "iopub.status.busy": "2024-02-15T19:24:55.031803Z",
     "iopub.status.idle": "2024-02-15T19:25:09.887338Z",
     "shell.execute_reply": "2024-02-15T19:25:09.886541Z",
     "shell.execute_reply.started": "2024-02-15T19:24:55.032386Z"
    },
    "tags": []
   },
   "source": [
    "### **CPUC Data Request**: Localized air temperature and dewpoint temperature\n",
    "This notebook provides the full pre-processing and batch mode calculation for localized WRF data at 71 WECC weather station locations for both air temperature (degF) and dew-point temperature (degF). \n",
    "\n",
    "Produces:\n",
    "- A single netcdf file per station with air temperature and dewpoint temperature as separate variables\n",
    "- Summary statistics csv file\n",
    "    - Values represent the multi-model mean count of dewpoint temperature exceeding air temperature per month    \n",
    "\n",
    "<span style=\"color:#FF0000\">**Reference Notebook Only**</span>: The HadISD station data used to localize WRF data are in a private bucket location `wecc-hadisd` that requires access in order to run this notebook. **Therefore this notebook is provided as methodology process only, and cannot be run**. The sole reason for this is that the version of the HadISD station data that is \"hooked up\" to climakitae only has the air temperature variable at present. Replacing the publically available station data with the version with the second variable dew-point temperature would \"break\" the existing climakitae code. Once the climakitae backend code is updated, the version of data with both variables will replace the single-variable version. \n",
    "    \n",
    "### Step 0: Import libraries\n",
    "Import useful libraries for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c9f312-3ef9-4e0a-bb48-2ee8683b1fb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import climakitae as ck\n",
    "import climakitaegui as ckg\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from climakitae.util.utils import read_csv_file, get_closest_gridcell, convert_to_local_time\n",
    "\n",
    "from xclim.core.calendar import convert_calendar\n",
    "from xclim.core.units import convert_units_to\n",
    "from xclim.sdba.adjustment import QuantileDeltaMapping\n",
    "from xclim.sdba import Grouper\n",
    "\n",
    "from bokeh.models import HoverTool\n",
    "from timezonefinder import TimezoneFinder\n",
    "\n",
    "import s3fs\n",
    "import pyproj\n",
    "import itertools\n",
    "import panel as pn\n",
    "pn.extension()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5605272-173e-46d6-90e9-a35cacb87119",
   "metadata": {},
   "source": [
    "### Step 1: \"Helper\" functions for pre-processing\n",
    "Two functions are provided: \n",
    "- Quantile delta mapping function: `do_QDM`\n",
    "   - This is the same function that is provided in climakitae for localization \n",
    "- A version of climakitae function `get_closest_gridcell` that intentionally retrieves the closest gridcell:`get_closest_land_gridcell`\n",
    "   - This is the preliminary work for identifying the importance of land vs. water pixels for some near-shore HadISD station locations\n",
    "   - Currently set-up for Santa Barbara (KSBA) only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3044e5-f887-4eaa-b590-b9b7b92c0759",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "window = 90\n",
    "def do_QDM(obs, ds, nquantiles=20, \n",
    "           group='time.dayofyear', window=window, \n",
    "           kind=\"+\"):\n",
    "    \n",
    "    group = Grouper(group, window=window)\n",
    "\n",
    "    ds.attrs['variable'] = ds.name\n",
    "    ds.name = 'Raw' \n",
    "    \n",
    "    QDM = QuantileDeltaMapping.train(\n",
    "        obs, \n",
    "        ds.sel(\n",
    "            time=slice(str(obs.time.values[0]),\n",
    "                       str(obs.time.values[-1]))), \n",
    "        nquantiles=nquantiles, \n",
    "        group=group, \n",
    "        kind=kind)\n",
    "    \n",
    "    ds_adj = QDM.adjust(ds).compute()\n",
    "    \n",
    "    QDM_ds = QDM.ds.rename(dict(\n",
    "        dayofyear = 'Day of Year', \n",
    "        quantiles='Quantile'))    \n",
    "    \n",
    "    ds_adj.name = 'Adjusted' \n",
    "    ds_adj = xr.merge([ds, ds_adj])\n",
    "    \n",
    "    return QDM_ds,ds_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0dda9d-e66d-4bcc-8c33-7c625604f546",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_closest_land_gridcell(data, lat, lon, res='3km', print_coords=True):\n",
    "    \"\"\"From input gridded data, get the closest gridcell to a lat, lon coordinate pair.\n",
    "\n",
    "    This function first transforms the lat,lon coords to the gridded data’s projection.\n",
    "    Then, it uses xarray’s built in method .sel to get the nearest gridcell.\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    data: xr.DataArray or xr.Dataset\n",
    "        Gridded data\n",
    "    lat: float\n",
    "        Latitude of coordinate pair\n",
    "    lon: float\n",
    "        Longitude of coordinate pair\n",
    "    res: str\n",
    "        Spatial resolution for Santa Barbara station\n",
    "    print_coords: bool, optional\n",
    "        Print closest coordinates?\n",
    "        Default to True. Set to False for backend use.\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    xr.DataArray\n",
    "        Grid cell closest to input lat,lon coordinate pair\n",
    "\n",
    "    See also\n",
    "    --------\n",
    "    xarray.DataArray.sel\n",
    "    \"\"\"\n",
    "    # Make Transformer object\n",
    "    lat_lon_to_model_projection = pyproj.Transformer.from_crs(\n",
    "        crs_from=\"epsg:4326\",  # Lat/lon\n",
    "        crs_to=data.rio.crs,  # Model projection\n",
    "        always_xy=True,\n",
    "    )\n",
    "\n",
    "    # Hard-coding for Santa Barbara, forces land pixel selection over water pixel\n",
    "    # Note fine-tuning produces different results for different spatial resolutions, specify which on call\n",
    "    if lat==34.424 and lon==-119.842: # station coordinates for SB (HadISD_72392523190)\n",
    "        print('Selecting closest land pixel for Santa Barbara...')\n",
    "        if res == '9km':\n",
    "            lat = 34.43\n",
    "            lon = -119.83\n",
    "        elif res == '3km':\n",
    "            lat = 34.43\n",
    "            lon = -119.845\n",
    "\n",
    "    # Convert coordinates to x,y\n",
    "    x, y = lat_lon_to_model_projection.transform(lon, lat)\n",
    "\n",
    "    # Get closest gridcell\n",
    "    closest_gridcell = data.sel(x=x, y=y, method=\"nearest\")\n",
    "\n",
    "    # Output information\n",
    "    if print_coords:\n",
    "        print(\n",
    "            \"Input coordinates: (%.4f, %.4f)\" % (lat, lon)\n",
    "            + \"\\nNearest grid cell coordinates: (%.4f, %.4f)\"\n",
    "            % (closest_gridcell.lat.values.item(), closest_gridcell.lon.values.item())\n",
    "        )\n",
    "    return closest_gridcell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc0d1c2-4baf-4f9b-958c-6ae8675af10f",
   "metadata": {},
   "source": [
    "### Step 2: Run full batch mode for all available stations\n",
    "The next code is the full batch calculation to generate localized data for all 71 locations for both air temperature and dewpoint temperature. \n",
    "Requested data notes:\n",
    "- Data is `Historical Climate` + `SSP 3-7.0 -- Business as Usual`\n",
    "- Data covers 1981 - 2100, at hourly timesteps\n",
    "- For 67 of the 71 stations, the 9km spatial resolution is used as many stations are located outside of CA\n",
    "   - 3 stations are outside of the WECC domain and were run with the 45 km data (KFSD, KMCI, KSGF)\n",
    "   - 1 station was run with 3 km, using the closest land gridcell (KSBA)\n",
    "\n",
    "<span style=\"color:#FF0000\">**Warning**</span>: Each station takes approximately **25 minutes to run**. All 71 stations will take approximately **30 hours of continuous run time**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a189157-5aec-4e89-8486-838cc6405e9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read stations df\n",
    "stations = \"/home/jovyan/cae-notebooks/collaborative/DFU/wecc-station-data.csv\"\n",
    "stations_df = read_csv_file(stations)\n",
    "\n",
    "# prep summary stats df\n",
    "stats_list = []\n",
    "\n",
    "# initialize selections\n",
    "selections = ckg.Select()\n",
    "\n",
    "def tas_dew_localize(stations_df, correct_tz=False):\n",
    "    '''\n",
    "    Performs the localization procedure, where each variable is processed **individually**\n",
    "    Multi-variate localization improvements are forthcoming. \n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    stations_df [pd.DataFrame]: station list of lat-lon locations to run\n",
    "    correct_tz [Boolean]: flag to run timezone correction to PST, default is False (UTC)\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    stats_df [pd.DataFrame]: df of computed diff of tdps > tas counts\n",
    "    '''\n",
    "    \n",
    "    for my_station in stations_df['icao']:\n",
    "    # for my_station in ['KFSD', 'KMCI', 'KSGF']:    # outside of wecc, run with 45km\n",
    "    # for my_station in ['KSBA']:                    # SB station - water pixel check, run with 3km\n",
    "\n",
    "        ## STEP 1 =================================================================================================\n",
    "        # Grab observations data\n",
    "        station_id = str(stations_df[stations_df['icao'] == my_station]['station id'].values[0]).replace('-', '')\n",
    "        print('Running tas+dpts localization on: HadISD_{} ({})'.format(station_id, my_station))\n",
    "\n",
    "        s3 = s3fs.S3FileSystem(anon=False, key=AWS_ACCESS_KEY_ID, secret=AWS_SECRET_ACCESS_KEY)\n",
    "        aws_path = \"s3://wecc-hadisd/02_tmp_tas_dpt/\"\n",
    "        filepath_zarr = aws_path + \"HadISD_{}.zarr\".format(station_id)\n",
    "        print('Opening: {}'.format(filepath_zarr))\n",
    "        store = s3fs.S3Map(root=filepath_zarr, s3=s3, check=False)\n",
    "        ds = xr.open_zarr(store=store, consolidated=True) # observation data retrieved\n",
    "\n",
    "        ## STEP 2 =================================================================================================\n",
    "        # Subset and run per variable\n",
    "        vars_to_run = ['tas', 'dpts']\n",
    "        for var in vars_to_run: # run one variable at a time\n",
    "            print('\\nStart processing on {}'.format(var))\n",
    "            obs_ds = ds[var] # Subset for variable\n",
    "            obs_ds = convert_units_to(obs_ds, \"degF\") # Convert units from K to degF\n",
    "            obs_ds = obs_ds.chunk(dict(time=-1)).compute()\n",
    "\n",
    "            # extract coordinates\n",
    "            lat0 = obs_ds.latitude.values\n",
    "            lon0 = obs_ds.longitude.values\n",
    "            print('Obs lat-lon coords: ', lat0, lon0)\n",
    "            \n",
    "            if correct_tz == True: # station timezone conversion\n",
    "                # cannot use the convert_to_local_time function as we are not pulling bias-corrected model data for a station \n",
    "                # but modifying the existing raw station data itself\n",
    "                # 1981 - 2014 is the baseline period\n",
    "                print('Starting timezone conversion...')\n",
    "                obs_ds_data = obs_ds.loc[(obs_ds.time.dt.year >= 1981) & (obs_ds.time.dt.year <= 2014)]\n",
    "                obs_ds_data.time # to keep\n",
    "\n",
    "                # need to retrieve 2015 to grab the last ~8 hours of \"2014\" from 2015 (in UTC) to do timezone conversion\n",
    "                obs_ds_tzslice = obs_ds.loc[obs_ds.time.dt.year == 2015]\n",
    "\n",
    "                # combine and convert for timezone correction\n",
    "                obs_ds_total = xr.concat([obs_ds_data, obs_ds_tzslice], dim='time') # 1981-2015\n",
    "                tf = TimezoneFinder()\n",
    "                local_tz = tf.timezone_at(lng=float(lon0), lat=float(lat0))\n",
    "                new_time = (pd.DatetimeIndex(obs_ds_total.time)\n",
    "                            .tz_localize(\"UTC\")\n",
    "                            .tz_convert(local_tz)\n",
    "                            .tz_localize(None)\n",
    "                            .astype(\"datetime64[ns]\"))\n",
    "                obs_ds_total['time'] = new_time\n",
    "\n",
    "                # subset by initial time\n",
    "                start = obs_ds_data.time[0]\n",
    "                end = obs_ds_data.time[-1]\n",
    "                obs_ds_local = obs_ds_total.sel(time=slice(start, end))\n",
    "                print('Obs timezone correction complete!')\n",
    "\n",
    "            # retrieve WRF data\n",
    "            print('Retrieve WRF data...')\n",
    "            selections.scenario_historical=['Historical Climate']\n",
    "            selections.scenario_ssp=['SSP 3-7.0 -- Business as Usual']\n",
    "            selections.append_historical = True\n",
    "            selections.area_average = 'No'\n",
    "            selections.time_slice = (1981, 2100)\n",
    "            selections.timescale = 'hourly'\n",
    "            if var == 'tas':\n",
    "                selections.variable = 'Air Temperature at 2m'\n",
    "            elif var == 'dpts':\n",
    "                selections.variable = 'Dew point temperature'\n",
    "            selections.units = 'degF'\n",
    "            selections.area_subset = 'lat/lon'\n",
    "            selections.cached_area = ['coordinate selection']\n",
    "            # depending on which station is being run, different spatial resolutions are required\n",
    "            if my_station in ['KFSD', 'KMCI', 'KSGF']: # outside of WECC stations\n",
    "                selections.resolution = '45 km' \n",
    "                selections.latitude = (lat0-.5, lat0+.5) \n",
    "                selections.longitude = (lon0-.5, lon0+.5)\n",
    "            elif my_station in ['KSBA']: # Santa Barbara station run at closest gridcell, 3km\n",
    "                selections.resolution = '3 km'\n",
    "                selections.latitude = (lat0-.1, lat0+.1)\n",
    "                selections.longitude = (lon0-.1, lon0+.1)\n",
    "            else: # all other stations\n",
    "                selections.resolution = '9 km' # can only use 9km, not 3km for outside of CA regions\n",
    "                selections.latitude = (lat0-.2, lat0+.2) \n",
    "                selections.longitude = (lon0-.2, lon0+.2) \n",
    "            wrf_ds = selections.retrieve()\n",
    "            print('Retrieving WRF lat-lon: ', selections.latitude, selections.longitude)\n",
    "            # spacing on latlon kept larger because some station locations are tricky\n",
    "            # keeping large spacing and reduce to single grid cell\n",
    "            \n",
    "            if correct_tz == True: # WRF timezone conversion\n",
    "                wrf_ds = convert_to_local_tie(wrf_ds, selections)\n",
    "             \n",
    "            # reduce to only closest grid cell\n",
    "            if my_station in ['KSBA']: \n",
    "                # for Santa Barbara, we have tested using \"get_closest_land_gridcell\" defined above\n",
    "                print('Retrieving closest LAND gridcell...')\n",
    "                wrf_ds = get_closest_land_gridcell(wrf_ds, lat0, lon0, res='3km', print_coords=True) ## test run on land pixel selection for KSBA\n",
    "            else: \n",
    "                print('Retrieving closest gridcell...')\n",
    "                wrf_ds = get_closest_gridcell(wrf_ds, lat0, lon0, print_coords=True)\n",
    "\n",
    "            # need to unchunk for bias correction\n",
    "            wrf_ds = wrf_ds.chunk(dict(time=-1)).compute()\n",
    "            # do some renaming for plotting ease later\n",
    "            wrf_ds.attrs['physical_variable'] = wrf_ds.name\n",
    "            wrf_ds.name = 'Raw'\n",
    "            \n",
    "            # dropping duplicate time indexes and resetting calendar\n",
    "            print('Dropping duplicates in time dimension')\n",
    "            wrf_ds = wrf_ds.drop_duplicates(dim='time', keep='first')\n",
    "            obs_ds = obs_ds.drop_duplicates(dim='time', keep='first')\n",
    "            \n",
    "            print('Converting to no leap day calendar, best practice')\n",
    "            wrf_ds = convert_calendar(wrf_ds, \"noleap\")\n",
    "            obs_ds = convert_calendar(obs_ds, \"noleap\")\n",
    "\n",
    "            print('{} data now processed to go into localization process'.format(var))\n",
    "            if var == 'tas':\n",
    "                adj_factors1, adj_ds1 = do_QDM(obs_ds, wrf_ds) # run QDM                \n",
    "                # drop raw data variable (and unnecessary coordinates), and rename adjusted back to air temperature\n",
    "                adj_ds1 = adj_ds1['Adjusted']\n",
    "                adj_ds1.name = 'Adjusted Air Temperature at 2m'\n",
    "                adj_ds1 = adj_ds1.squeeze()\n",
    "                adj_ds1 = adj_ds1.reset_coords(names=['Lambert_Conformal','x','y','lakemask','landmask','lat','lon'], drop='True')\n",
    "                print('QDM complete on {}'.format(var))\n",
    "\n",
    "            elif var == 'dpts':\n",
    "                adj_factors2, adj_ds2 = do_QDM(obs_ds, wrf_ds) # run QDM\n",
    "                # drop raw data variable (and unnecessary coordinates), and rename adjusted back to air temperature\n",
    "                adj_ds2 = adj_ds2['Adjusted']\n",
    "                adj_ds2.name = 'Adjusted Dewpoint Temperature'\n",
    "                adj_ds2 = adj_ds2.squeeze()\n",
    "                adj_ds2 = adj_ds2.reset_coords(names=['Lambert_Conformal','x','y','lakemask','landmask','lat','lon'], drop='True')\n",
    "                print('QDM complete on {}'.format(var))\n",
    "\n",
    "        ## STEP 3 =================================================================================================\n",
    "        # Merge individual variable arrays into one dataset\n",
    "        merged_ds = xr.merge([adj_ds1, adj_ds2], compat='override')\n",
    "        # merged_ds = merged_ds.sel(time=slice('1981-01-01', '2100-12-31')) # ensuring right length\n",
    "        merged_ds.attrs['localization_version'] = 'v2_utc' # add a \"localization version\" attribute to tag the data\n",
    "        print('\\nds created')\n",
    "        \n",
    "        ## STEP 4 =================================================================================================\n",
    "        # summary stats\n",
    "        td_exceed_tas = merged_ds['Adjusted Dewpoint Temperature'] > merged_ds['Adjusted Air Temperature at 2m'] # number of instances where Td > T\n",
    "        counts_per_month_sim = td_exceed_tas.groupby('time.month').sum().mean(dim='simulation')\n",
    "        counts_per_month_sim_percent = (counts_per_month_sim.values / len(merged_ds['time'])) *100\n",
    "        stats_list.append(counts_per_month_sim_percent)\n",
    "        \n",
    "        ## STEP 5 =================================================================================================\n",
    "        # export data\n",
    "        filename = 'bc_tas_dpts_HadISD_{}_UTC.nc'.format(station_id)\n",
    "        ck.export(merged_ds, filename, 'NetCDF')\n",
    "\n",
    "        ## STEP 6 =================================================================================================\n",
    "        # close dataset to save memory\n",
    "        wrf_ds.close()\n",
    "        obs_ds.close()\n",
    "        merged_ds.close()\n",
    "        print('All files closed ----------------------------------------------------------------------\\n')\n",
    "        \n",
    "    stats_df = pd.DataFrame(stats_list)    \n",
    "    return stats_df\n",
    "        \n",
    "## Approximately takes 25 minutes per station\n",
    "## total time to run for 70+ stations: ~30 hours\n",
    "stats_df = tas_dew_localize(stations_df, correct_tz=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12075b00-9a44-4182-b680-f34aa65e54d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# process and export summary stats dataframe\n",
    "stats_df.to_csv('dewpt_exceed_tas_counts_station_localization.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6c95f8-320e-45d9-a260-49df547e7e53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-13T19:02:48.519351Z",
     "iopub.status.busy": "2024-06-13T19:02:48.518959Z",
     "iopub.status.idle": "2024-06-13T19:02:52.372339Z",
     "shell.execute_reply": "2024-06-13T19:02:52.371579Z",
     "shell.execute_reply.started": "2024-06-13T19:02:48.519326Z"
    },
    "tags": []
   },
   "source": [
    "**Optional**: Timezone check, ensuring that UTC timestamped data is consistently 1 hour apart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4859cbee-96fc-4a00-979d-ffc17b4d4541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in a successfully run station\n",
    "ds = xr.open_dataset('FILENAME.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5370fb35-df65-471b-a87d-6a4e0a9d47fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_freq(df):\n",
    "    print('len of df: {}'.format(len(df)))\n",
    "    df['t_delta'] = df['time'].diff().fillna(pd.Timedelta(0))\n",
    "    df['hours_diff'] = df['t_delta']/np.timedelta64(1, 'h')\n",
    "    \n",
    "    return df['hours_diff'].value_counts()\n",
    "\n",
    "df = ds.to_dataframe().reset_index()\n",
    "t_freq(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:notebook] *",
   "language": "python",
   "name": "conda-env-notebook-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
