{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc908fc1-557b-49ad-b515-70e91ed630c8",
   "metadata": {},
   "source": [
    "# Using the Analytics Engine (AE) to produce heating and cooling degree days\n",
    "This notebook aims to reproduce the workflow CEC's Demand Forecast Unit takes to generate weather and climate information for the annual consumption model. Here the existing workflow is replicated, but connecting with new data from California's Fifth Climate Change Assessment.\n",
    "\n",
    "To execute a given 'cell' of this notebook, place the cursor in the cell and press the 'play' icon, or simply press shift+enter together. Some cells will take longer to run, and you will see a [$\\ast$] to the left of the cell while AE is still working.\n",
    "\n",
    "**Intended Application**: As a user, I want to **<span style=\"color:#FF0000\">generate heating and cooling degree days</span>** as input for an annual energy consumption model by:\n",
    "1. Understand and visualize the difference between data at the weather station, and aggregated across the demand zone\n",
    "2. Compute and visualize trends in heating and cooling degree days with a flexible threshold\n",
    "3. Compute and visualize trends in heating and cooling degree hours with a flexible threshold\n",
    "\n",
    "**Runtime**: With the default settings, this notebook takes approximately **11 minutes** to run from start to finish. Modifications to selections may increase the runtime. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ab2392-ae18-4ce8-baff-3aa87c58dd96",
   "metadata": {},
   "source": [
    "## Step 0: Setup\n",
    "First, we'll import any general python libraries required to run the notebook. We'll also import the python library climakitae, our AE toolkit for climate data analysis, along with specific functions that we'll use within this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8703a0d0-82dc-41d1-8482-e0c7e824f5f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import hvplot.pandas\n",
    "\n",
    "import climakitae as ck\n",
    "import climakitaegui as ckg\n",
    "from climakitae.core.data_interface import get_data\n",
    "from climakitae.util.utils import (compute_annual_aggreggate, trendline, \n",
    "                                   compute_multimodel_stats, combine_hdd_cdd)\n",
    "from climakitae.tools.derived_variables import compute_hdd_cdd, compute_hdh_cdh\n",
    "from climakitaegui.util.utils import (hdd_cdd_lineplot, hdh_cdh_lineplot)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a2c2ee-7b86-4d5f-aeb8-010cd8b29537",
   "metadata": {},
   "source": [
    "## Step 1: Get data from the closest grid cell to the weather station\n",
    "As an example - to replicate the historical observations at Sacramento Executive Airport, grab the grid cell from the model nearest to the airport. It is **critical** to note that the station-based grid cell data we are retrieving is **bias-adjusted**. In later steps, the gridded data that we will retrieve is **not bias-adjusted**, and therefore should be carefully considered."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7c8f9c-685e-4405-bf6b-2ddab808f748",
   "metadata": {},
   "source": [
    "### 1a) Read in the data \n",
    "Here, we'll use the `get_data` function to load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85d4be9-1fab-4d38-a850-972a2e4259dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_at_station = get_data(\n",
    "    variable = \"Air Temperature at 2m\", \n",
    "    resolution = \"3 km\",\n",
    "    timescale = \"hourly\",\n",
    "    data_type = \"Stations\", # Retrieve the single grid cell closest to the weather station \n",
    "    stations = \"Sacramento Executive Airport (KSAC)\",\n",
    "    units = \"degF\", \n",
    "    time_slice = (2005, 2025), \n",
    "    scenario = [\"Historical Climate\", \"SSP 3-7.0\"]\n",
    ")\n",
    "data_at_station"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c93201-5a51-44c4-aaa4-ed422adb1f7e",
   "metadata": {},
   "source": [
    "Becasuse the dynamically downscaled WRF data in the Cal-Adapt: Analytics Engine is in UTC time, we'll convert to the timezome of the station we've selected. This is particularly important for determining the timing of the daily maximum and minimum temperatures. For a station located in Pacific Time (US), UTC time places the daily minimum \"in\" the day prior because UTC is 8 hours ahead of Pacific! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ceda482-9303-4f98-ade3-65f164e40560",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_at_station[\"time\"] = data_at_station[\"time\"] - pd.Timedelta(hours=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfac1c91-7e8a-4c9a-8df7-a28efce720c1",
   "metadata": {},
   "source": [
    "### 1b) Load the data into memory\n",
    "This may take some time, because the data has to be loaded into memory and then subsetted to get the closest grid cell. All computations we've done before this step are actually computed in this step; before, we just see a preview of the data. Because of this, **we recommend running this notebook in the Analytics Engine's Jupyter Hub, which provides additional computational resources that greatly speed up this step.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c96c9d-14cd-425e-8d9d-ad1b178e05c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_at_station = ck.load(data_at_station)\n",
    "data_at_station"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae4c3c1-2892-4338-a55b-bb5202302cc3",
   "metadata": {},
   "source": [
    "### 1c) Read in a csv file of the station coordinates\n",
    "We'll use the Sacramento Executive Airport here as an example. Make sure the filepath to the csv file matches the correct location on your computer. This file will be read into the notebook as a pandas DataFrame object. We'll use it in plotting below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9404f52-2978-435d-9175-4423fc79e53c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stations_df = pd.read_csv(\"data/CEC_Forecast_Weather_Stations_California.csv\", index_col=\"STATION\")\n",
    "stations_df.head(5) # Display the first 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1d23ca-b482-42a4-b2cb-be4c890266ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "station_name = \"SACRAMENTO METROPOLITAN AP\"\n",
    "one_station = stations_df.loc[station_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308fcd1a-f637-4525-bebb-bfe20e93ec0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_station"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68132d7-e91c-4555-ab0d-13af7d61e1b7",
   "metadata": {},
   "source": [
    "### 1d) Output final data product as a csv file\n",
    "We'll drop all unneeded coordinates and convert our xarray Dataset to a pandas Dataframe, allowing us to easily output the final data product to a csv file. In the output table, the first column is the time in units of UTC, and the second column are the various global climate models (which can be filtered in excel or in python code in the notebook). The other columns are the variables selected at the beginning of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc75bc8-1041-4df0-87c4-c2742ebbc623",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_at_station_df = data_at_station.isel(scenario=0).drop([\"scenario\"]).to_dataframe()\n",
    "data_at_station_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752d3406-911f-4bfa-80aa-34bc53820930",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"hourly_data_at_station_{0}.csv\".format(station_name.replace(\" \", \"_\")).lower()\n",
    "data_at_station_df.to_csv(filename, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c772f774-9a2b-4565-ab70-3a44851f8d2e",
   "metadata": {},
   "source": [
    "## Step 2: Get data from across the demand forecast zone\n",
    "\n",
    "As an alternative to a single point, we can instead consider weather conditions across an entire forecast zone. In this example, we calculate the median of all conditions across the Sacramento Municipal Utility District. We have pre-loaded data selections for gridded air temperature within the Sacramento Municipal Utility District for 2005-2025. Feel free to make modifications for your own workflows. <br>\n",
    "**Reminder**: The gridded data we will be retrieving in this step and using throughout this notebook is not bias-corrected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e84f21-69df-48e4-9b10-c07236b582e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dfz = get_data(\n",
    "    variable = \"Air Temperature at 2m\", \n",
    "    resolution = \"3 km\",\n",
    "    timescale = \"hourly\",\n",
    "    data_type = \"Gridded\", \n",
    "    cached_area = \"SMUD Service Territory\", # Retrieve all cells over this region \n",
    "    units = \"degF\", \n",
    "    time_slice = (2005, 2025), \n",
    "    scenario = [\"Historical Climate\", \"SSP 3-7.0\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b681a024-b44f-4e25-a579-ab7a8826ab81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dfz[\"time\"] = data_dfz[\"time\"] - pd.Timedelta(hours=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769cdbba-bc22-46ec-b610-e07aed14e3ec",
   "metadata": {},
   "source": [
    "## Step 3: Compute the median value of the grid cells in station's corresponding forecast zone\n",
    "\n",
    "In this example, we will visualize the data across the Demand Forecast Zone for the Sacramento Municipal Utility District, and then calculate the median of all conditions across the Sacramento Municipal Utility District."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bffd2d1-104a-4dbd-a308-1ed4efdee246",
   "metadata": {},
   "source": [
    "### 3a) Visualize both the Demand Forecast Zone and the weather station on the same map "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47d48af-1060-40c1-a273-03b56b0de5d1",
   "metadata": {},
   "source": [
    "For simplicity's sake, we'll show just the first 12 hours of data. In the outputted map, you can see that our data contains multiple simulation options as well, which you can toggle between in the map's dropdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc711be3-937b-4832-a4d5-b56696c55eac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Used to add weather station as star to map \n",
    "point_df = pd.DataFrame({\n",
    "    \"longitude (degrees_east)\":[one_station.LON_X],\n",
    "    \"latitude (degrees_north)\":[one_station.LAT_Y],\n",
    "    \"weather station\": station_name\n",
    "})\n",
    "\n",
    "# Grab subset of data and load into memory \n",
    "to_plot = data_dfz.isel(time = np.arange(0,13))\n",
    "to_plot = ck.load(to_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f142cf6-c160-495b-b701-9c3e17b45302",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ckg.view(to_plot) * point_df.hvplot.points(\n",
    "    hover_cols = [\"weather station\"], \n",
    "    marker = \"star\", size = 300, color = \"black\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d59289-4f6b-4683-8c56-9a4b25c3e672",
   "metadata": {},
   "source": [
    "### 3b) Aggregate values across grid cells in the forecast zone \n",
    "**Chose your aggregation: median, mean, min, or max.** All can be easily computed with just one line of code, thanks to xarray. You could also write your own code to compute a weighted mean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9aa428-48d4-4944-b653-39a142041d70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dfz_aggregated = data_dfz.median(dim=[\"x\",\"y\"])\n",
    "# data_dfz_aggregated = data_dfz.mean(dim=[\"x\",\"y\"])\n",
    "# data_dfz_aggregated = data_dfz.min(dim=[\"x\",\"y\"])\n",
    "# data_dfz_aggregated = data_dfz.max(dim=[\"x\",\"y\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98739e89-6c7e-416d-88c2-9246cb6a68f3",
   "metadata": {},
   "source": [
    "Finally, let's load this final data product into memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c2715d-658b-431f-880e-453203450556",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dfz_aggregated = ck.load(data_dfz_aggregated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d9b3f8-2676-4a16-b21d-3b75f1fb3fea",
   "metadata": {},
   "source": [
    "### 3c) Output final data product as a csv file\n",
    "We'll drop all unneeded coordinates and convert our xarray Dataset to a pandas Dataframe, allowing us to easily output the final data product to a csv file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e5c476-9a73-4da4-8b20-c95e400eaede",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dfz_aggregated_df = data_dfz_aggregated.isel(scenario=0).drop(\n",
    "    [\"scenario\",\"Lambert_Conformal\"]).to_dataframe()\n",
    "dfz_aggregated_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1ab9af-4c01-4814-8513-2bd0e8ab9bb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filename = \"dfz_aggregated_{0}.csv\".format(station_name.replace(\" \", \"_\").lower())\n",
    "dfz_aggregated_df.to_csv(filename, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36d58c7-5f96-4749-add4-f09d0558d107",
   "metadata": {},
   "source": [
    "## Step 4: Compute heating degree days and cooling degree days\n",
    "Degree days are [based on the assumption](https://www.weather.gov/key/climate_heat_cool) that when the outside temperature is 65°F, we don't need heating or cooling to be comfortable. However, you may wish to use a different threshold than 65°F: for example, you might want to assume that temperatures between 60-70°F won't require heating or cooling, and use degrees below 60°F for HDD and degrees above 70°F for CDD. <br><br> In the code below, a heating degree day (HDD) is calculated by computing how many degrees Fahrenheit **colder** the daily temperature is from a specified temperature threshold. A cooling degree day (CDD) is calculated by computing how many degrees **warmer** the daily temperature is from a specified temperature threshold. In the computation below, you can provide different thresholds for HDD and CDD based on your needs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f1f8c9-8fc2-41aa-8d9d-0ec6614920d2",
   "metadata": {},
   "source": [
    "### 4a) Decide which input data you want to use \n",
    "\n",
    "You can use the data within the demand forecast zone, which we retrieved in **step 2b**. Or, you can use the closest grid cell to the weather station, which we computed in **step 1b**. We do not recommend using the aggregated DFZ data calculated in step 3b, as aggregating the data prior to computing HDD and CDD may remove some critical information about the weather extremes. You can comment out whichever method you don't want to use. We've chosen to show the analysis with the DFZ data, but if you want to use the closest grid cell data, just comment out the DFZ cells and uncomment the closest grid cells. Note, the closest grid cell resampling will take a few minutes! <br><br>Depending on the input data, we will also set a new variable defining the number of grid cells. This will be just 1 for the closest grid cell method; for the DFZ data, however, this value will change depending on the size of the DFZ before aggregating. This information is used to compute the annual aggregate HDD and CDD in step 4c. Lastly, we provide 2 options for determining the [daily mean](https://www.weather.gov/key/climate_heat_cool) against which we calculate degree days. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b4413e-e085-409c-a603-7232a3bd5781",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ALL DATA WITHIN DFZ ZONE\n",
    "data_to_use = data_dfz\n",
    "num_grid_cells = data_dfz.x.size * data_dfz.y.size # Number of grid cells within the demand forecast region\n",
    "\n",
    "## CLOSEST GRID CELL \n",
    "# data_to_use = data_at_station.to_array(name='Air Temperature at 2m').squeeze()\n",
    "# num_grid_cells = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226432f9-ab77-437a-9cb8-0fc5b193d510",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# METHOD 1: Using daily max/min difference\n",
    "daily_max = data_to_use.resample(time='1D').max()\n",
    "daily_min = data_to_use.resample(time='1D').min()\n",
    "data_to_use = (daily_max + daily_min)/2\n",
    "\n",
    "## METHOD 2: Using daily mean\n",
    "# data_to_use = data_to_use.resample(time='1D').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410b69ba-7697-483f-828d-634bb6b5200f",
   "metadata": {},
   "source": [
    "### 4b) Compute HDD and CDD \n",
    "We'll use the climakitae helper function `compute_hdd_cdd` to compute both heating and cooling degree days, which uses the function arguments `hdd_threshold` and `cdd_threshold` to represent any threshold of your choosing. In the example below, we will calculate HDD with a threshold of 60 degF and CDD with a threshold of 70 degF. The function performs the following calculations:<br><br>\n",
    "**HDD = threshold - temperature<br>\n",
    "CDD = (-1)\\*(threshold - temperature)**<br><br>\n",
    "For HDD, we can just subtract the 2m temperature from the selected threshold, then set any negative value to 0. For CDD, we will do the same, but will then multiply by -1 to turn negative values to positive, then set negative values to 0. We need to multiply by -1 for CDD to avoid having all negative values; for example, on a day of 80F and a cdd_threshold of 70F, CDD = 70 - 80 = -10, but the CDD value is +10. Multiplying -10 by -1 will give us the true value of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43ab84e-93c5-4d88-8585-9a2e25a12d31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#help(compute_hdd_cdd) # See information about the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf8afd0-3889-4fb2-8e0c-4fc4ba85975b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hdd, cdd = compute_hdd_cdd(data_to_use, hdd_threshold=65, cdd_threshold=65) # Set for all data within selected DFZ zone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9250344-7103-40e3-8c03-7da17d080cc7",
   "metadata": {},
   "source": [
    "Now that we have computed the HDD and CDD, we can then aggregate the results across grid cells in the forecast zone like we did previously above. We will need to do this for both the HDD and CDD variables. If you would like to change the aggregation method, you can easily modify between **median, mean, min, or max**, or write your own code to compute a weighted mean here too. We will use the *median* as an example here. Note, because we are aggregating here, the number of grid cells is reduced to represent the aggregation method to 1. \n",
    "\n",
    "Please note, that this next step is **not required** if you selected the closest grid cell to the station instead of all data across the DFZ. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a798614-e387-4a96-a7a4-de11bdefd668",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# only for all data within DFZ zone, not station data\n",
    "hdd = hdd.median(dim=[\"x\",\"y\"])\n",
    "cdd = cdd.median(dim=[\"x\",\"y\"])\n",
    "num_grid_cells = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269feeb8-5af2-43cf-9da3-e2233f967d54",
   "metadata": {},
   "source": [
    "### 4c) Aggregate annually to find HDD and CDD per year\n",
    "To do this, we will first group the data by year and compute a sum across space and time. Then, we will divide the annual aggregated data by the number of grid cells over which the sum was computed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d1af18-bb1f-43d7-9143-97c13d0eb383",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hdd_annual = compute_annual_aggreggate(\n",
    "    data=hdd, \n",
    "    name=\"Annual Heating Degree Days (HDD)\", \n",
    "    num_grid_cells=num_grid_cells\n",
    ")\n",
    "cdd_annual = compute_annual_aggreggate(\n",
    "    data=cdd, \n",
    "    name=\"Annual Cooling Degree Days (CDD)\", \n",
    "    num_grid_cells=num_grid_cells\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71416d6-d849-4f61-8908-fda24f8a4da5",
   "metadata": {},
   "source": [
    "### 4d) Compute the multimodel mean, min, and max. \n",
    "We'll add these statistics to our main datasets, `hdd_annual` and `cdd_annual`, so they can be easily accessed for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e136ea87-b766-4b47-9783-db9668d618e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hdd_annual = compute_multimodel_stats(hdd_annual)\n",
    "cdd_annual = compute_multimodel_stats(cdd_annual)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ded86c-3e65-4bf2-a724-f807e8afbf86",
   "metadata": {},
   "source": [
    "### 4e) Compute a trendline using the mean of all simulations\n",
    "We'll find the coefficients for a first degree (linear) polynomial using [numpy's `polyfit` function](https://numpy.org/doc/stable/reference/generated/numpy.polyfit.html). The returned coefficients (**m** and **b** in the code below) will allow us to compute the trendline using the linear polynomial y = mx + b, where **y** is the trendline and **x** is the years. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e81e5d-1277-4218-8e95-7e138c1793fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hdd_trendline = trendline(hdd_annual, kind='mean') \n",
    "cdd_trendline = trendline(cdd_annual, kind='mean') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267d8368-c7ee-4a12-bef8-54c84138d25e",
   "metadata": {},
   "source": [
    "### 4f) Visualize the results\n",
    "Using the python package *hvplot*, we can easily make a line plot of the annual aggregated data. To do this, we'll plot the annual HDD, then add the trendline on top. The code to generate the plot is contained in a function `hdd_cdd_lineplot`. \n",
    "\n",
    "Please note, the gridded data is not currently bias-corrected. As a result of this, the minimum or maximum timeseries could reflect a single simulation that is biased high or low compared to others. You can toggle lines on and off in the plots below by clicking on the name in the legend. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e094aa-b9b7-4cd1-9f1a-8f2ac7adea4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hdd_cdd_lineplot(\n",
    "    annual_data = hdd_annual, \n",
    "    trendline = hdd_trendline, \n",
    "    title = \"Annual Aggregate Heating Degree Days\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dab899-3053-4e62-8d54-59ec867ab30c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hdd_cdd_lineplot(\n",
    "    annual_data = cdd_annual, \n",
    "    trendline = cdd_trendline, \n",
    "    title = \"Annual Aggregate Cooling Degree Days\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fbb5b8-8d4c-4a7c-9eb9-a0d51a4e1daa",
   "metadata": {},
   "source": [
    "### 4g) Output data as csv files\n",
    "We'll drop all unneeded coordinates and convert our xarray Dataset to a pandas Dataframe, allowing us to easily output the final data product to a csv file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf13ea2a-fb98-4438-ba98-fb7b1039d579",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Merge and simplify data \n",
    "hdd_cdd_combined = xr.merge([combine_hdd_cdd(hdd_annual), combine_hdd_cdd(cdd_annual)])\n",
    "hdd_cdd_combined = ck.load(hdd_cdd_combined)\n",
    "\n",
    "# Convert to pandas dataframe \n",
    "hdd_cdd_df = hdd_cdd_combined.to_dataframe()\n",
    "hdd_cdd_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e3937b-1e36-4164-b96a-55b097d56c49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filename = \"annual_hdd_cdd_{0}.csv\".format(station_name.replace(\" \", \"_\").lower())\n",
    "hdd_cdd_df.to_csv(filename, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459602b9-5898-41c2-99ed-f6d2202afa03",
   "metadata": {},
   "source": [
    "## Step 5: Compute heating degree hours and cooling degree hours\n",
    "Alternatively, you may be interested in the number of hours in each day that a designated heating or cooling threshold crosses. For Cooling Degree Hours (CDH), this is the number of hours in which the hourly temperature exceeds the cooling degree threshold. Likewise, Heating Degree Hours (HDH) is the number of hours in which the hourly temperature is below the heating degree threshold. We'll use the helper function `compute_hdh_cdh` to calculate HDH and CDH:<br><br>\n",
    "**CDH = num of hours where (temperature $>$ threshold)<br>\n",
    "HDH = num of hours where (temperature $<$ threshold)**<br><br>\n",
    "We will display the results to see how trends change throughout the year. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748022b0-f25a-44aa-9070-a4e4a74d2e9c",
   "metadata": {},
   "source": [
    "### 5a) Compute HDH and CDH\n",
    "Like the CDD and HDD examples above, we'll use all of the data for our selected DFZ zone to calculate CDH and HDH. Note that we've added an attribute to the data to retain the threshold used to compute the data here. If you forget, look at the attributes of CDH or HDH. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e347130a-7cff-4817-9bd1-451bd6a45712",
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(compute_hdh_cdh) # See information about the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51af09c3-cfd6-404a-8cbc-71c49deb59a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_to_use = data_dfz # reset to hourly data\n",
    "hdh, cdh = compute_hdh_cdh(data_to_use, hdh_threshold=60, cdh_threshold=70) # Set for all data within selected DFZ zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b64873-50d7-4134-973b-de80f237198d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# only for all data within DFZ zone, not station data\n",
    "hdh = hdh.median(dim=[\"x\",\"y\"])\n",
    "cdh = cdh.median(dim=[\"x\",\"y\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c55c615-93e0-4710-8501-bf168979e33d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 5b) Display a month of CDH and HDH\n",
    "Next, we'll plot specific months of the overall timeseries produced by the CDH and HDH calculation to see the trend in degree hours. We'll use a helper plotting function, and input  a month of interest. For example, we'll look at June of 2011, but you can input any date of interest; we provide examples for plotting a specific month and a specific year below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124475ad-df98-4404-ba90-fa54be2880b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_one_month = cdh.sel(time=\"June 2011\")\n",
    "hdh_cdh_lineplot(data_one_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207ab41e-7eea-42b7-b7bf-4b690157d52e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_one_month = hdh.sel(time=\"June 2011\")\n",
    "hdh_cdh_lineplot(data_one_month)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d424d2cb-260e-4e2c-beb6-2932fbe366b5",
   "metadata": {},
   "source": [
    "Alternatively, it may be useful to visualize a specific year to see the trends over time. We'll do this for 2021 as an example below with Cooling Degree Hours. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0658136c-a412-4ef6-99fa-1eac0f586997",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_one_year = cdh.sel(time=\"2021\")\n",
    "hdh_cdh_lineplot(data_one_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d604ae3-40f6-4b16-be36-2d7e0c705293",
   "metadata": {},
   "source": [
    "### 5c) Output data as csv files\n",
    "We'll drop all unneeded coordinates and convert our xarray Dataset to a pandas Dataframe, allowing us to easily output the final data product to a csv file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f1b639-6caf-48a3-b477-6beac0ee9fd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Merge and simplify data \n",
    "hdh_cdh_combined = xr.merge([combine_hdd_cdd(hdh), combine_hdd_cdd(cdh)])\n",
    "hdh_cdh_combined = ck.load(hdh_cdh_combined) \n",
    "\n",
    "# Convert to pandas dataframe \n",
    "hdh_cdh_df = hdh_cdh_combined.to_dataframe()\n",
    "hdh_cdh_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a7544e-2a5d-4f3c-b848-c8e2588011e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filename = \"daily_hdh_cdh_{0}.csv\".format(station_name.replace(\" \", \"_\").lower())\n",
    "hdh_cdh_df.to_csv(filename, index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
