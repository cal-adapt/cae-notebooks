{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c34af87a-a2b7-4e93-9db9-d013d9a1fbdf",
   "metadata": {},
   "source": [
    "# Hourly climate projections bias-corrected to the location of a weather station\n",
    "\n",
    "**Intended Application**: As a user, I want to **<span style=\"color:#FF0000\">generate annual hourly profiles of localized data, or an 8760</span>** by:\n",
    "1. Accessing and exporting hourly data at station locations (netCDF or csv)\n",
    "2. Examine the “8760’s” and find the one with the smallest distance from the median.\n",
    "\n",
    "**Runtime**: With the default settings, this notebook takes approximately **6 minutes** to run from start to finish. Modifications to selections may increase the runtime.\n",
    "\n",
    "### Step 0: Import libaries\n",
    "First, import our python package and any other necessary libraries for analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4536d890-1b3b-4db5-9f45-509858a0e6d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import climakitae as ck\n",
    "import climakitaegui as ckg\n",
    "import panel as pn\n",
    "pn.extension()\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2845243-040c-4c83-ac5d-e8f70c8991aa",
   "metadata": {},
   "source": [
    "### Step 1: Access\n",
    "#### 1a) Select data of interest\n",
    "For the rest of the examples in this notebook, you will want to select `station` for `Data type`. Make any other choices that fit your needs. For the default, we recommend starting with a single station selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48d916d-6cf4-4a38-bf7a-711d07ff4fca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "selections = ckg.Select()\n",
    "selections.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb2fe31-d427-4fc4-8260-6b2e7b784e81",
   "metadata": {},
   "source": [
    "#### 1b) Now retrieve the data, storing it in a variable called 'data'\n",
    "The data that will be retrieved is being bias-corrected using hourly historical observations at each station selected. The historical observations come from [HadISD](https://catalogue.ceda.ac.uk/uuid/f579035b3c954475922e4b13705a7669). \n",
    "\n",
    "The bias-correction procedure happening behind-the-scenes is called Quantile Delta Mapping ([QDM](https://journals.ametsoc.org/view/journals/clim/28/17/jcli-d-14-00754.1.xml)), which is designed to preserve trends while allowing for different biases across quantiles. For more information, `localization_methodology.ipynb` steps through the bias-correction procedure in more detail. \n",
    "\n",
    "You will see a warning regarding \"non-standard calendar\" -- don't worry about this! This just means that the data specifically had leap days removed in order to downscale. Removing leap days in bias correction is the standard best practice for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c191bee8-4daa-420a-9ae3-084244deeedb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "data = selections.retrieve()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f174c317-54df-424b-9517-aff3b0cef3e0",
   "metadata": {},
   "source": [
    "#### 1c) At this point you may wish to load the data \n",
    "Then you can examine it further, and save time later when you go to export it. When you explicitly load the data, only then does it actually perform all of the computations to bias-correct the gridded data to your chosen stations. If you skip this step it will defer this computation until you do some other step that *uses* the data, whether that is to graph it or export it. \n",
    "\n",
    "Depending on the number of stations you selected above, the time that this step takes to complete will scale linearly with the number of stations selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e938933d-6e84-48a5-b05a-cef46b4f5869",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "data = ck.load(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fad4df5-c4c7-49fc-a420-318a45d44188",
   "metadata": {},
   "source": [
    "A quick preview of the data structure can be seen by executing a cell with the data variable as the final line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda83ec9-cadb-4300-8b2d-9062566aa520",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d1abf5-1991-4fd9-8344-af5d7925e3ab",
   "metadata": {},
   "source": [
    "Now that we have hourly data to work with, we can create and sample from a pool of 8760's which have consistent and realistic time series properties, improving on stochastic sampling methods that had otherwise been in use.\n",
    "\n",
    "### Step 2: Sample the \"8760\" closest to the median\n",
    "\n",
    "#### 2a) Calculate 8760s\n",
    "\n",
    "One way that you may wish to use this data is to examine \"8760's\", or entire years of hourly data. If you chose a wide range of years previously, you might want to now sub-select, for example, only a historical reference period.\n",
    "\n",
    "Two main reasons to choose only a historical period:\n",
    "- the following steps do not correct for the trends in the data due to climate change, so the median will reflect when the median amount of climate change has happened, instead of a median year solely with respect to year-to-year variability\n",
    "- if you max out on the number of years, subsequent steps may execute more slowly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cebe21-8afb-4dd3-8e4c-80f2d1e357a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sel(time=slice('19810101','20101231'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b352904-8ab5-4767-9d04-52fab3b7b1fb",
   "metadata": {},
   "source": [
    "Beware that if you selected `Historical Climate`, and don't do something like the step above you will have partial years at the beginning and end, since the simulations start in Sept and run through Aug of the following calendar year. You could alternatively deal with that this way:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4f70f7e5-2525-4009-8cd1-dc045c10eaf9",
   "metadata": {},
   "source": [
    "min_year = data['time.year'].min().values\n",
    "max_year = data['time.year'].max().values\n",
    "data = data.sel(time=slice(str(min_year+1)+'0101',str(max_year-1)+'1231'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8fd82f-8b82-4394-883a-e909025145bf",
   "metadata": {},
   "source": [
    "How many 8760s are there in this dataset for each station location?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2c91a8-4a0d-4a33-a88d-ec02241921d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_years = (data['time.year'].max() - data['time.year'].min()).values\n",
    "unique_8760s = number_of_years * len(data.simulation) \n",
    "unique_8760s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc351b96-1ce8-4338-a66b-5c33d71aa2d1",
   "metadata": {},
   "source": [
    "For each station, let's find the \"8760\" that is closest to the median (the euclidean distance between that year and the median across all days and hours is smallest).\n",
    "\n",
    "Next, we'll prepare some helper functions. You could also loop over days and times, but that would take much longer to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bfdd3b-9fa3-46fe-b134-0ba4361ee826",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_difference_hour(y): \n",
    "    #for this hour, of this day of the year, the median across all 8760s:\n",
    "    median = y.quantile(q=0.5)\n",
    "    #return the difference from that median for each 8760:\n",
    "    return np.abs(y - median)\n",
    "\n",
    "def get_difference(y): \n",
    "    return y.groupby('time.hour').apply(get_difference_hour) \n",
    "        \n",
    "def euclidean_distance(y):\n",
    "    return xr.DataArray(np.sqrt(np.sum(y**2)))\n",
    "    \n",
    "def get_distance(y):\n",
    "    return y.groupby('simulation').apply(euclidean_distance)\n",
    "\n",
    "def get_median_8760(y):\n",
    "    difference_from_median = y.squeeze().groupby('time.dayofyear').apply(get_difference) \n",
    "    euclidean_distance = difference_from_median.groupby('time.year').apply(get_distance)\n",
    "    final = euclidean_distance.stack(all8760s=['year','simulation'])\n",
    "    which_8760 = final.isel(all8760s=final.argmin())\n",
    "    to_return = y.sel(time=str(which_8760.year.values),simulation=which_8760.simulation.values)\n",
    "    # set time to the same generic calendar year regardless:\n",
    "    to_return['time'] = np.arange(1,8761)\n",
    "    # drop the simulation coord \n",
    "    to_return = to_return.drop(\"simulation\")\n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89537493-42ba-450c-bece-3dce51be677c",
   "metadata": {},
   "source": [
    "Note that you could replace the 0.5 quantile (or median) with some other quantile.\n",
    "\n",
    "Apply the above functions across all the stations in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee0dcb2-e140-4bfe-8ebd-c34852a2cd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "median_year = data.squeeze().map(get_median_8760)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a161ed78-8f03-41d7-a83e-6f7ea8111066",
   "metadata": {},
   "source": [
    "It doesn't matter *which* year this is in one of the simulations, because the phasing and timing of interannual variability is random, and not meant to match how this actually unfolded historically (unless you've selected \"Historical Reconstruction\").\n",
    "\n",
    "Now we can see what that year looks like as a time-series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d356ce4-e07e-4a0a-933a-f2401e64c420",
   "metadata": {},
   "outputs": [],
   "source": [
    "median_year.drop(['scenario']).hvplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22d6e66-7216-4f57-8a18-6ccd9f9cc9aa",
   "metadata": {},
   "source": [
    "#### 2b) Compare with other 8760s\n",
    "\n",
    "We will compare this 8760 (closest to the median) with the statistical median itself, and the range of other 8760s, looking one station at a time. You'll see that we set the time dimension to a length of 8784 - this is to accommodate leap days, but don't worry, they are dropped in according to scientific best practices for bias-correction. \n",
    "\n",
    "First, a little work gathering and reshaping the data to plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e79dde7-21fc-49d8-bb12-ac50c680dc1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def median_by_hour(y):\n",
    "    by_hour = y.groupby('time.hour').quantile(q=0.5)\n",
    "    generic_year_day = pd.to_datetime('1981'+str(y['time.month'].values[0]).zfill(2)+\n",
    "                                      str(y['time.day'].values[0]).zfill(2))\n",
    "    by_hour['time'] = pd.date_range(generic_year_day,periods=24,freq='H')\n",
    "    by_hour = by_hour.drop('hour')\n",
    "    return by_hour\n",
    "\n",
    "stat_median = data.groupby('time.dayofyear').apply(median_by_hour)\n",
    "stat_median['time'] = np.arange(1,8785) # handle for leap days (8760 + 24)\n",
    "stat_median = stat_median.to_array('station')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6981cfee-64ce-4329-8084-45661f257d67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "median_year_by_station = median_year.to_array('station')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db850a8-64f3-402c-a72e-6876afc8373c",
   "metadata": {},
   "source": [
    "Some rearrangement of the initial data, by year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6bd773-3f16-4a0e-8048-5e0f19d7cf4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all8760s = data.to_array('station')\n",
    "\n",
    "def number_years(y):\n",
    "    y['year'] = y['time.year'].values[0]\n",
    "    y['time'] = np.arange(1,8761)\n",
    "    return y\n",
    "\n",
    "all8760s = all8760s.groupby('time.year').apply(number_years)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24d4452-8952-44cc-b5be-e3814c6bf261",
   "metadata": {},
   "source": [
    "Now finally plot it all together. (Note that this plot is interactive and you can zoom in and out.) Initial plotting (and each time you switch station) may take a moment.\n",
    "\n",
    "Samples from all 8760's are plotted in light grey, the statistical medians for each of the 8760 hours of the year are in black, and the single 8760 with the smallest euclidean distance from that statistical median is shown on top in the thin line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91beea08-7b59-4bb5-bbaa-2ba55629d409",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all8760s.hvplot(x='time', groupby='station', kind='scatter', color='grey', alpha=0.2) * \\\n",
    "    stat_median.hvplot(groupby='station', kind='scatter', c='k') * \\\n",
    "    median_year_by_station.hvplot(groupby='station', kind='line', line_width=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d536520-d38a-47c2-8e0e-209dd1fd0f9b",
   "metadata": {},
   "source": [
    "#### 2c) Typical year heatmap\n",
    "Turning back to that closest-to-the-median year, with all of its variability, let's view it another way.\n",
    "\n",
    "You may want to rearrange the timeseries to view as a heatmap over day-of-year vs hour-of-day. There may be other ways to do this, but here we will add dimensions for day and hour, and then convert into a pandas dataframe, which we can more easily pivot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25a02b0-b5ab-403d-9351-999d6d5601cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "median_year_array = deepcopy(median_year)\n",
    "median_year_array['time'] = pd.date_range('19810101',periods=8760,freq='H')\n",
    "median_year_array['hour'] = median_year_array['time.hour']\n",
    "median_year_array['dayofyear'] = median_year_array['time.dayofyear']\n",
    "\n",
    "median_year_df = median_year_array.to_pandas().drop(['scenario'],axis=1) \n",
    "median_year_df = median_year_df.pivot(index=\"dayofyear\", columns=\"hour\")\n",
    "median_year_df.columns.names = ['station', 'hour']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa3971e-800b-4865-92bc-d5b12101e312",
   "metadata": {},
   "source": [
    "Then put it back into an xarray datastructure again for better plotting of heatmaps, and examine the new data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863d6e21-37f1-4e5b-9d60-933c20a33eb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "median_year_array = median_year_df.unstack().to_xarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0588f7c4-7f86-4500-a251-b627e7313e49",
   "metadata": {},
   "source": [
    "With just a little attention to formatting in local time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b1de4c-c923-4f37-b1a6-c647ca5c00d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def to_PST(hour):\n",
    "    #take hour 0-23, assumed UTC\n",
    "    #return formatted Pacific Standard Time string\n",
    "    return pd.to_datetime(hour*60, origin=pd.Timestamp('1981-01-01'),unit='m', utc=True).tz_convert('US/Pacific').strftime('%r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab21ce6-148a-4408-9d41-2f3e7727a136",
   "metadata": {},
   "source": [
    "Finally, the heatmap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ea17fd-bba4-4a31-95a0-014065b4b7f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "median_year_array.hvplot(\n",
    "    x='dayofyear',\n",
    "    y='hour',\n",
    "    yticks=[(int(one_hour),to_PST(one_hour)) for one_hour in median_year_array.hour.values],\n",
    "    groupby='station',\n",
    "    cmap='Reds',\n",
    "    clabel=selections.variable +' ('+ selections.units +')',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c0141a-cd82-4031-a681-fec622cb79e1",
   "metadata": {},
   "source": [
    "### Step 3: Export the data\n",
    "We'll show exporting of the data as originally accessed, but you could also export the processed data in a similar fashion.\n",
    "#### 3a) Option 1: netCDF\n",
    "The simplest way to export the data is to use the netCDF format, which this data structure already mirrors in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb009cff-70bd-4645-b6f5-ddaa4490c051",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.to_netcdf('test_output.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94aead2e-a2ed-47f1-bf3a-449aa8103395",
   "metadata": {},
   "source": [
    "The file should appear in the file list to the left, after the cell is done executing $[*]$.\n",
    "\n",
    "For reference, to read this back in using python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90dfe46-c5a1-4f74-a76f-0c5422876edc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_back = xr.open_dataset('test_output.nc')\n",
    "data_back"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14de0ed8-6709-44ca-96b5-76b293a0f6f5",
   "metadata": {},
   "source": [
    "#### 3b) Option 2: csv\n",
    "If for some reason you prefer a .csv file, first convert the data structure to a table compatible with export to csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de49cd3-47b5-4021-a062-d6a330978a91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_table = data.to_dataframe(dim_order=['scenario','simulation','time'])\n",
    "data_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc0d82b-9a01-47c2-b975-6e5638f7ca83",
   "metadata": {},
   "source": [
    "You can save the above out to csv to work with in R, but it will be too many rows for e.g. Excel to read in.\n",
    "\n",
    "Before saving out the csv, you may want to first remove any spaces in the column headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19961b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_table_for_R = data_table\n",
    "data_table_for_R.columns = data_table_for_R.columns.str.replace(' ', '_')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0c27c7",
   "metadata": {},
   "source": [
    "We can also rearrange the data so it's distributed over more columns instead, although it may take a moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e1c694-2c40-4db7-9fba-8219306ae21a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_table = data_table.unstack().unstack().stack('time')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443f5f09-1632-42eb-8433-d0ec1999ba80",
   "metadata": {},
   "source": [
    "Examine it first to make sure you like the arrangement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b753ac-a91a-46ed-b4ee-ddaef48778af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cf0ebb-60dc-4b8e-a441-1f96e2231fb5",
   "metadata": {},
   "source": [
    "And export is a simple matter. As above, this file will appear in the file list to the left when the cell is done executing $[*]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1a26b1-5823-4a5d-be5e-2e490bbeaa10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_table.to_csv('test_output.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369a643a-de1b-4238-8be9-e99b0a17c4c1",
   "metadata": {},
   "source": [
    "Or if you wanted the R-friendly version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2ab15d-31d5-4be8-9d0d-97f3048cd32c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_table_for_R.to_csv('test_output_for_R.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d34a2f-1b0c-4df9-8f3e-b23d25877f94",
   "metadata": {},
   "source": [
    "For reference, here would be how to read this back in using python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a49999-e0f7-49d1-ba16-4d5630045382",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "table_back = pd.read_csv('test_output.csv',index_col=[0,1], header=[0,1])\n",
    "table_back"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
