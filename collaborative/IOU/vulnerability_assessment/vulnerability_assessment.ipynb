{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90ef497a-dcb7-4b23-aa33-2b7690294144",
   "metadata": {},
   "source": [
    "# Vulnerability Assessment Pilot\n",
    "This notebook demonstrates on-going development of climate adaptation vulnerability assessment (CAVA) support using climate data in the Analytics Engine. \n",
    "\n",
    "To execute a given 'cell' of this notebook, place the cursor in the cell and press the 'play' icon, or simply press shift+enter together. Some cells will take longer to run, and you will see a [$\\ast$] to the left of the cell while AE is still working.\n",
    "\n",
    "**Intended Application**: As a user, I want to **<span style=\"color:#FF0000\">access climate projections data for my vulnerability assessment report</span>** by:\n",
    "1. Retrieve data metrics required for planning needs\n",
    "\n",
    "**Runtime**: With the default settings, this notebook takes approximately **several hours** to run from start to finish, depending on the metric choice. Modifications to selections may increase the runtime. \n",
    "\n",
    "### Step 0: Set-up\n",
    "\n",
    "First, we'll import the Python library [climakitae](https://github.com/cal-adapt/climakitae), our AE toolkit for climate data analysis, along with this specific functions from that library that we'll use in this notebook, as well as any other necessary Python libraries to aid in analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3d6530-a94d-416e-81a3-2fac5e79bf72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from climakitae.explore.vulnerability import cava_data\n",
    "from climakitae.explore.vulnerability_table import create_vul_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153b50c3-5e8c-4f4c-b7e2-1839ae0c462f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 1: Import locations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c435c50f-dec5-4440-9bd6-e5207a317f92",
   "metadata": {},
   "source": [
    "To import your own custom locations, we recommend putting your csv file in the same folder as this notebook for ease:\n",
    "1. Drag and drop a csv file into the file tree on the left hand side; or\n",
    "2. Use the `upload` button (the \"up arrow\" symbol next to the large blue plus symbol above the file tree). \n",
    "\n",
    "<span style=\"color:#FF0000\">**Formatting note**</span>: For the code cells below to work, there must be **2 columns labeled `lat` and `lon`**. Functionality to accept different labeling is forthcoming!\n",
    "\n",
    "In the cell below, we read the csv file in. We use the HadISD station list as an example here -- you may want to replace with your own locations file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a7366d-562b-4921-8c70-ae76a91a206c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read in dummy locations from `stations_csv` file\n",
    "from climakitae.core.paths import stations_csv_path\n",
    "from climakitae.util.utils import read_csv_file\n",
    "example_locs = read_csv_file(stations_csv_path, index_col=0)[['LAT_Y', 'LON_X']].rename(columns={'LAT_Y': 'lat', 'LON_X': 'lon'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bafbc5-2695-422e-a6b7-4795aeb418ad",
   "metadata": {},
   "source": [
    "### Step 2: Retrieve metric data\n",
    "\n",
    "The `cava_data` funciontality is designed to provide flexibility over customizable metric calculation. There are 4 customizable metrics that can be built with this functionality:\n",
    "1. Likely seasonal event occurence (e.g., \"likely summer night low temperature\")\n",
    "2. 1-in-X temperature events (e.g., \"1-in-10 year maximum temperature\")\n",
    "3. High/Extreme Heat Index events (e.g., \"how many days per year does the Heat Index exceed 90°F\")\n",
    "4. 1-in-X precipitation events (e.g., \"1-in-100 year, 24 hour precipitation\")\n",
    "\n",
    "Below is a table outlining all avaialble arguments to the `cava_data` function. The \"Required\" flag notes whether the argument must be passed to start generating data. Input options for each argument is provided, as well as whether a setting is required for any of the required selections. We provide multiple examples of working with the `cava_data` function with multiple configurations.\n",
    "\n",
    "| Argument | Options | Argument required for | Notes |\n",
    "|----------|---------|-----------------------|-------|\n",
    "|input_locations | Pass a location via csv. | All | Option to run either a single location, or multiple when **batch_mode=True**.|\n",
    "|variable | \"Air Temperature at 2m\", \"NOAA Heat Index\", \"Precipitation (total)\"| All | |\n",
    "|approach | \"Time\", \"Warming Level\" | All | |\n",
    "|downscaling_method | \"Dynamical\", \"Statistical\"| All | |\n",
    "|time_start_year | Numerical (min is 1981) | Required for **approach=Time**.| |\n",
    "|time_end_year | Numerical (max is 2100) | Required for **approach=Time**.| |\n",
    "|historical_data| \"Historical Climate\", \"Historical Reconstruction\" | Required for **approach=Time**| **Historical Climate** ranges from 1980-2015 for WRF and 1950-2015 for LOCA2-Hybrid. **Historical Reconstruction** ranges from 1950-2022. Historical Reconstruction data cannot be combined with SSP data.|\n",
    "|ssp_data | \"[SSP 2-4.5]\", \"[SSP 3-7.0]\", \"[SSP 5-8.5]\" | Required for **approach=Time** | Dynamical only has SSP 3-7.0, Statisical has all 3 SSP options.|\n",
    "|warming_level| 0.8, 1.2, 1.5, 2.0, 2.5, 3.0 | Required for **approach=Warming Level**| Historical/Current period GWLs: 0.8, 1.2. Future GWLs: 1.5, 2.0, 2.5, 3.0. |\n",
    "|metric_calc| \"max\", \"min\" | Required for 1-in-X events, Heat Index, likely seasonal event | |\n",
    "|heat_idx_threshold | Numerical | Required for Heat Index | Heat Index can only be calculated with **downscaling_method=\"Dynamical\"**.|\n",
    "|one_in_x | np.array of Numerical x values | Required for 1-in-X events| |\n",
    "|event_duration| Numerical + \"day\"/\"hour\"| Optional for 1-in-X events| Must adhere to following structure: ({number}, \"{temporal frequency}\"). Temporal frequency options: \"day\", \"hour\". Default is (1, \"day\").|\n",
    "|distr| \"gev\", \"genpareto\", \"gumbel\", \"wibull\", \"pearson3\", \"gamma\"| Optional for 1-in-X events |Default set to \"gev\".|\n",
    "|percentile | Numerical (0-100) | Required for likely seasonal event |   |\n",
    "|season| \"summer\", \"winter\", \"all\" | Required for likely seasonal event| Default set to \"all\".|\n",
    "|units | Temp/Heat Index: \"degF\", \"degC\", \"K\". Precip: \"mm\", \"inches\" | Optional | Default for temp/Heat Index is DegF. Default for precip is mm.|\n",
    "|wrf_bias_adjust| True, False | Optional| Option to return only the 5 bias-adjusted WRF models. Only applicable for **downscaling_method=\"Dynamical\"**. Default set to True.|\n",
    "|export_method| \"raw\", \"calculate\", \"both\" | Optional | Default set to \"both\".|\n",
    "|file_format | \"NetCDF\", \"csv\" | Optional | Default set to \"NetCDF\".|\n",
    "|separate_files | True, False | Optional | Option to export separate files if multiple points are passed. Default set to True.|\n",
    "|batch_mode | True, False | Optional, but recommmended for multiple locations. | Option to efficiently run multiple points. Separate files for export is turned off in batch mode. Default set to False.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1718991-771d-4f53-873c-3a774644b772",
   "metadata": {},
   "source": [
    "The following cells illustrate several examples of how to retrieve and calculate various configurations of the `cava_data` function. Below is a list of the examples; and more are coming soon as more functionality is built in!\n",
    "1. Likely seasonal event, single location, time approach, all WRF data, with custom percentile\n",
    "2. Likely seasonal event, batch mode for multiple locations, time approach, all WRF data, with custom percentile\n",
    "3. 1-in-X temperature event, single location, bias-adjusted WRF data only, time approach, with custom event frequency and distribution\n",
    "4. Heat index event, single location, time approach, with custom threshold\n",
    "5. Likely seasonal event, single location, warming level approach, all WRF data, wtih custom percentile\n",
    "6. Likely seasonal event, single location, warming level approach, all LOCA2 data, with custom percentile\n",
    "7. Heat index event, batch mode for multiple locations, time approach, Historical Reconstruction data, with custom threshold\n",
    "8. Likely seasonal event, batch mode for multiple locations, warming levels approach, all LOCA2 data, with custom percentile\n",
    "9. 1-in-X precipitation event, single location, all LOCA2 data, time approach, with custom return period\n",
    "10. 1-in-X precipitation event, single location, all WRF data, time approach, with custom return period, event duration, and distribution\n",
    "11. Example of reading the calculated metric data via xarray for easy viewing within this notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f0c405-6996-4823-8af4-f2bda7b0b980",
   "metadata": {},
   "source": [
    "#### Example: Likely seasonal event\n",
    "Example scenario: I want to calculate \"likely summer day high temperature for 2030-2050, where likely is the 75th percentile, in Celsius, using all available WRF data (bias-adjusted and non-bias-adjusted), and export only the calculated metric data\" for a **single location**. I would input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379508eb-526b-4008-9e5b-94f417f20c40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = cava_data(\n",
    "    ## Set-up\n",
    "    example_locs[:1], # select a single location        \n",
    "    approach=\"Time\",  \n",
    "    time_start_year=2030, \n",
    "    time_end_year=2050,\n",
    "    downscaling_method=\"Dynamical\",  # WRF data\n",
    "    wrf_bias_adjust=False, # return all WRF models\n",
    "    \n",
    "    ## Likely seaonal event specific arguments\n",
    "    variable=\"Air Temperature at 2m\", \n",
    "    metric_calc=\"max\", # daily high temperature\n",
    "    percentile=75, # likeliness percentile\n",
    "    season=\"summer\", # season\n",
    "    units=\"degC\", # change units\n",
    "    \n",
    "    ## Export\n",
    "    export_method=\"calculate\",  # export only calculated metric data\n",
    "    file_format=\"NetCDF\",\n",
    "    batch_mode=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431806d8-c66d-4464-ab83-c02cb0c91ec8",
   "metadata": {},
   "source": [
    "#### Example: Likely seasonal event, in batch mode for multiple locations\n",
    "Example scenario: I want to calculate \"likely summer day high temperature for 2030-2050, where likely is the 75th percentile, in Celsius, using all available WRF data (bias-adjusted and non-bias-adjusted) for many locations, and export only the calculated metric data\" for **multiple locations**. I would input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedeb1ca-c401-46ce-ae60-943cae75126d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = cava_data(\n",
    "    ## Set-up\n",
    "    example_locs, # no subsetting for a single location from input list\n",
    "    time_start_year=2020, \n",
    "    time_end_year=2050,\n",
    "    downscaling_method=\"Dynamical\",  # WRF data\n",
    "    approach=\"Time\",  \n",
    "    wrf_bias_adjust=False, # return all WRF models\n",
    "    \n",
    "    ## Likely seaonal event specific arguments\n",
    "    variable=\"Air Temperature at 2m\", \n",
    "    metric_calc=\"max\", # daily high temperature\n",
    "    percentile=75, # likeliness percentile\n",
    "    season=\"summer\", # season\n",
    "    units=\"degC\", # change units\n",
    "    \n",
    "    ## Export\n",
    "    export_method=\"calculate\",  # export only calculated metric data\n",
    "    file_format=\"NetCDF\",\n",
    "    batch_mode=True, # batch mode - optimized for multiple locations\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f215a2e-5098-455d-8e29-3a9e82fc695b",
   "metadata": {},
   "source": [
    "#### Example: 1-in-X temperature event\n",
    "Example scenario: I want to calculate \"1-in-10 AND 1-in-100 year maximum temperature using the GEV distribution, in Fahrenheit, for 2070-2090, using only the bias-adjusted WRF data, and export both the raw and calculated metric data.\" I would input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb333d9-23db-44c1-a664-985f9e9a79be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = cava_data(\n",
    "    ## Set-up\n",
    "    example_locs.iloc[:1], # select a single location\n",
    "    time_start_year=2070,\n",
    "    time_end_year=2090,\n",
    "    downscaling_method=\"Dynamical\",  # WRF data \n",
    "    approach=\"Time\",  \n",
    "    wrf_bias_adjust=True, # return bias adjusted WRF models\n",
    "    \n",
    "    ## 1-in-X event specific arguments\n",
    "    variable=\"Air Temperature at 2m\",\n",
    "    metric_calc=\"max\", # daily maximum temperature\n",
    "    one_in_x=np.array([10]), # One-in-X\n",
    "    distr=\"gev\", # change distribution\n",
    "    units=\"degF\", # change units\n",
    "    \n",
    "    ## Export\n",
    "    export_method=\"both\",\n",
    "    file_format=\"NetCDF\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437119b2-9c7d-423e-b597-c389fe52cec2",
   "metadata": {},
   "source": [
    "#### Example: Heat Index\n",
    "Example scenario: I want to calculate \"the number of days per year that the Heat Index exceeds 90°F between 2030-2060, and export only the raw data\". I would input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b3e5f8-1e2b-43a2-b7c0-5129af840331",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = cava_data(\n",
    "    ## Set-up\n",
    "    example_locs.iloc[:1], # select a single location\n",
    "    time_start_year=2030,\n",
    "    time_end_year=2060,\n",
    "    downscaling_method=\"Dynamical\",  # WRF data\n",
    "    approach=\"Time\",  \n",
    "    \n",
    "    ## Heat Index specific arguments\n",
    "    variable=\"NOAA Heat Index\", \n",
    "    metric_calc=\"max\", # daily maximum\n",
    "    heat_idx_threshold=90, # Heat Index Threshold\n",
    "    units=\"degF\", # change units\n",
    "    \n",
    "    ## Export\n",
    "    export_method=\"raw\",\n",
    "    file_format=\"csv\",\n",
    "    batch_mode=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2078977e-8c4f-477e-978a-a4198f56e5ce",
   "metadata": {},
   "source": [
    "#### Example: Global Warming Level approach with WRF\n",
    "Example scenario: I want to calculate \"likely summer day high temperature with a 2°C warming level, where likely is the 50th percentile, in Celsius, using all available WRF data (bias-adjusted and non-bias-adjusted), and export both the raw and calculated metric data\". I would input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfe8ee8-d252-47ed-9b7e-37befd903723",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = cava_data(\n",
    "    ## Set-up\n",
    "    example_locs.iloc[:1], # select a single location\n",
    "    downscaling_method=\"Dynamical\",  # WRF data\n",
    "    approach=\"Warming Level\",\n",
    "    warming_level=2.0,\n",
    "    wrf_bias_adjust=False, # return all WRF models\n",
    "    \n",
    "    ## Likely seasonal event specific arguments\n",
    "    variable=\"Air Temperature at 2m\", \n",
    "    metric_calc=\"max\", # daily high temperature\n",
    "    percentile=50, # likeliness percentile\n",
    "    season=\"summer\", # season\n",
    "    units=\"degC\", # change units\n",
    "    \n",
    "    ## Export\n",
    "    export_method=\"both\",\n",
    "    file_format=\"NetCDF\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47f0510-668a-4891-a263-4ed004831c32",
   "metadata": {},
   "source": [
    "#### Example: Global Warming Level approach with LOCA2\n",
    "Example scenario: I want to calculate \"likely winter day high temperature with a 2°C warming level, where likely is the 60th percentile, in Celsius, using LOCA2 data, and export both the raw and calculated metric data\". I would input: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf3efb8-1831-4deb-a9f4-f0964c9e9588",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = cava_data(\n",
    "    ## Set-up\n",
    "    example_locs.iloc[:1], # select a single location\n",
    "    downscaling_method=\"Statistical\",  # LOCA2 data\n",
    "    approach=\"Warming Level\",\n",
    "    warming_level=2.0,\n",
    "    \n",
    "    ## Likely seasonal event specific arguments\n",
    "    variable=\"Air Temperature at 2m\", \n",
    "    metric_calc=\"max\", # daily high temperature\n",
    "    percentile=60, # likeliness percentile\n",
    "    season=\"winter\", # season\n",
    "    units=\"degC\", # change units\n",
    "    \n",
    "    ## Export\n",
    "    export_method=\"both\",\n",
    "    file_format=\"NetCDF\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc50edf-5811-4481-a5b9-4a509fe45e65",
   "metadata": {},
   "source": [
    "#### Example: Heat Index, in batch mode for multiple locations\n",
    "Example scenario: I want to calculate \"the number of days per year that the Heat Index exceeds 104°F between 1990-2010 in the Historical Reconstruction data for many locations, and export only the calculated metric data\". I would input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b19c2e-1afc-49ef-a6a2-78cac70529a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = cava_data(\n",
    "    ## Set-up\n",
    "    example_locs, # no subsetting for a single location from input list\n",
    "    time_start_year=1990,\n",
    "    time_end_year=2010,\n",
    "    historical_data=\"Historical Reconstruction\", # selecting reconstruction data\n",
    "    approach=\"Time\",  \n",
    "    \n",
    "    ## Heat Index specific arguments\n",
    "    variable=\"NOAA Heat Index\", \n",
    "    metric_calc=\"max\", # daily maximum\n",
    "    heat_idx_threshold=104, # Heat Index Threshold\n",
    "    units=\"degF\", # change units\n",
    "    \n",
    "    ## Export\n",
    "    export_method=\"calculate\",\n",
    "    file_format=\"NetCDF\",\n",
    "    batch_mode=True, # batch mode - optimized for multiple locations\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55380761-2cb8-458c-a5c8-b709f6f7edb5",
   "metadata": {},
   "source": [
    "#### Example: Likely seasonal event, in batch mode for multiple locations with LOCA2\n",
    "Example scenario: I want to calculate \"likely summer day high temperature for 1.5°C warming level, where likely is the 70th percentile, in Celsius, using all available LOCA2 data, and export only the calculated metric data\" for **multiple locations**. I would input:\n",
    "\n",
    "**Note:** Batch mode for LOCA2 data using the warming levels approach resets to `batch_mode = False` regardless of your setting here due to optimization constraints, but will compute all desired metrics. This will take quite some time to run (**2 locations** with warming levels with LOCA2 data takes **approx. 1 hour to run**) -- hang tight! Improvements in this space is forthcoming!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75871a13-f4f8-4fd8-ba20-075dc0669304",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = cava_data(\n",
    "    ## Set-up\n",
    "    example_locs, # select multiple locations\n",
    "    downscaling_method=\"Statistical\",  # LOCA data \n",
    "    approach=\"Warming Level\",  \n",
    "    warming_level=1.5, \n",
    "    \n",
    "    ## Likely seasonal event specific arguments\n",
    "    variable=\"Air Temperature at 2m\",\n",
    "    metric_calc=\"max\", # daily maximum temperature\n",
    "    season='summer', # change season\n",
    "    percentile=70, # change percentile\n",
    "    units=\"degC\", # change units\n",
    "\n",
    "    ## Export\n",
    "    export_method=\"calculate\",\n",
    "    file_format=\"NetCDF\",\n",
    "    batch_mode=False  # batch mode - optimized for multiple locations\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34612770-de3d-4f79-83cb-2dd2ab8608ed",
   "metadata": {},
   "source": [
    "#### Example: 1-in-X precipitation event\n",
    "Example scenario: I want to calculate \"1-in-10 year precipitation event using the GEV distribution, in inches, for 2070-2090 with SSP 3-7.0, using LOCA2 data, and export both the raw and calculated metric data.\" I would input:\n",
    "\n",
    "**Notes**: \n",
    "- For daily precipitation 1-in-X events (i.e., 24-hour), we recommend the use of LOCA2 data instead of WRF data. If looking for a non-24-hour event, WRF data must be used, as the LOCA2 data is not available at hourly time steps. \n",
    "- The goodness of fit on the distribution is provided for 1-in-X precipitation events. The p-value of the distribution fit to the data is provided during the calculation step and as an attribute in the final data object. For 1-in-X precipitation events, **we recommend the use of \"gev\" as the distribution** for the first distribution test. GEV allows for a continuous range of different shapes, and will reduce to either Gumbel, Weibull, or Generalized Pareto distributions under different conditions. GEV is typically a better fit than the 3 individaul distributions, and is a common distribution in hydrological applications. If the **p-value is less than 0.05**, this indicates that the **selected distribution is not a good fit for the data**, and we recommend choosing a different distribution and re-running the `cava_data` function. \n",
    "- In certain geographic regions, the selection of a high return period event (e.g., 1-in-1000) may produce unrealistically high precipitation values. This is primarily a limitation of the data sample size beyond what the data can reasonably estimate, where a small change in noise will produce a large change in the distribution tails. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2411ac-1bf3-40ea-89dd-938fb1b70e0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = cava_data(\n",
    "    ## Set-up\n",
    "    example_locs.iloc[:1], # select a single location\n",
    "    time_start_year=2070,\n",
    "    time_end_year=2090,\n",
    "    downscaling_method=\"Statistical\",  # LOCA2 data \n",
    "    approach=\"Time\",  \n",
    "    ssp_data=[\"SSP 3-7.0\"], # ssp selection\n",
    "    \n",
    "    ## 1-in-X event specific arguments\n",
    "    variable=\"Precipitation (total)\",\n",
    "    metric_calc=\"max\", # daily maximum precipitation\n",
    "    one_in_x=np.array([10]), # One-in-X\n",
    "    distr=\"gev\", # change distribution\n",
    "    units=\"inches\", # change units\n",
    "    \n",
    "    ## Export\n",
    "    export_method=\"both\",\n",
    "    file_format=\"NetCDF\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19fca7d-e2a0-4f99-a7ff-b74d5bf30ad5",
   "metadata": {},
   "source": [
    "#### Example: 1-in-X precipitation event with custom event duration\n",
    "Example scenario: I want to calculate \"1-in-100 year precipitation event for a 3-hour event using the GEV distribution, in inches, for 2070-2090 with SSP3-7.0, using only the bias-adjusted WRF data, and export both the raw and calculated metric data.\" I would input:\n",
    "\n",
    "**Notes**: \n",
    "- For daily precipitation 1-in-X events (i.e., 24-hour), we recommend the use of LOCA2 data instead of WRF data. If looking for a non-24-hour event, WRF data must be used, as the LOCA2 data is not available at hourly time steps. \n",
    "- The goodness of fit on the distribution is provided for 1-in-X precipitation events. The p-value of the distribution fit to the data is provided during the calculation step and as an attribute in the final data object. For 1-in-X precipitation events, **we recommend the use of \"gev\" as the distribution** for the first distribution test. GEV allows for a continuous range of different shapes, and will reduce to either Gumbel, Weibull, or Generalized Pareto distributions under different conditions. GEV is typically a better fit than the 3 individaul distributions, and is a common distribution in hydrological applications. If the **p-value is less than 0.05**, this indicates that the selected distribution is not a good fit for the data, and we recommend choosing a different distribution and re-running the `cava_data` function. \n",
    "- In certain geographic regions, the selection of a high return period event (e.g., 1-in-1000) may produce unrealistically high precipitation values. This is primarily a limitation of the data sample size beyond what the data can reasonably estimate, where a small change in noise will produce a large change in the distribution tails. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f6ec20-4e25-4ccc-923b-0dc616e39d33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = cava_data(\n",
    "    ## Set-up\n",
    "    example_locs.iloc[:1], # select a single location\n",
    "    downscaling_method=\"Dynamical\",  # WRF data \n",
    "    approach=\"Time\",\n",
    "    time_start_year=2070,\n",
    "    time_end_year=2090,\n",
    "    wrf_bias_adjust=True, # return bias-adjusted WRF models\n",
    "    \n",
    "    ## 1-in-X event specific arguments\n",
    "    variable=\"Precipitation (total)\",\n",
    "    metric_calc=\"max\", # daily maximum precipitation\n",
    "    one_in_x=np.array([100]), # One-in-X\n",
    "    distr=\"gev\", # change distribution\n",
    "    event_duration = (3, 'hour'), # change event duration\n",
    "    units=\"inches\", # change units\n",
    "    \n",
    "    ## Export\n",
    "    export_method=\"both\",\n",
    "    file_format=\"NetCDF\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d856628-522d-4375-926a-744fc9df86cb",
   "metadata": {},
   "source": [
    "#### Example: Looking at the `cava_data` output\n",
    "It may be useful to look at the `cava_data` output within this notebook to assess the results and make any changes to the data request. After running the `cava_data` function, in a new cell you can type `data` to view the xarray data object. Depending on your export setting (\"raw\", \"calculate\", \"both\"), you can also view the data object in a more user-friendly xarray view. We provide the code to do so in the next cell -- select which option matches your `cava_data` run and the export option you would like to view! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0f7725-79f3-4cd2-9063-a6b6b81faf91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data # looking at the full xarray data object; will be a dictionary of data arrays!\n",
    "# data['calc_data'] # looking at just the calculated data metric\n",
    "# data['raw'] # looking at just the raw input data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befaee12-c451-4827-a783-8bf6ca9257bc",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e6b5b8-ce77-4202-8f8f-73a6b2debc10",
   "metadata": {},
   "source": [
    "### Appendix: Table Generation Sample Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c558433-b9c2-4b3a-b001-cce58c9067e0",
   "metadata": {},
   "source": [
    "Below, you'll find code that generates a table with different climate data metrics used in a CAVA Report. Feel free to run it and check it out! It is still very much in progress. **This will take 30+ min. to run.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c35d388-1989-4716-8c9f-d78f435c9bb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "percentile = 50\n",
    "heat_idx_threshold = 80\n",
    "one_in_x = np.array([10])\n",
    "df = create_vul_table(example_locs.iloc[[10]], percentile, heat_idx_threshold, one_in_x)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
